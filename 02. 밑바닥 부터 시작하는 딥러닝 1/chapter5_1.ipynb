{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter5_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaWkA1pYNkH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79f7a38-8db3-4ab6-c5c1-9ee3b779b0dc"
      },
      "source": [
        "!git clone https://github.com/WegraLee/deep-learning-from-scratch.git\n",
        "\n",
        "\n",
        "%cd deep-learning-from-scratch"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'deep-learning-from-scratch' already exists and is not an empty directory.\n",
            "/content/deep-learning-from-scratch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r7QsmfYm73w"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iuCt6qbnNOk"
      },
      "source": [
        "class TwoLayerNet:\n",
        "    def __init__(self, input_size, hidden_size,output_size, weight_init_std =0.01):\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "    def predict(self,x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layers.forward(X)\n",
        "        return x\n",
        "    \n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y,t)\n",
        "    \n",
        "    def accuracy(self,x , t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis = 1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y ==t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "    def gradient(self,x ,t):\n",
        "        self.loss(x,t) # 순전파\n",
        "\n",
        "        # 역전파\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "        \n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV4O5QzWpq_a",
        "outputId": "dad458f0-292d-4d9a-ac61-4ed79a2e41a1"
      },
      "source": [
        "ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mch01\u001b[0m/  \u001b[01;34mch04\u001b[0m/  \u001b[01;34mch07\u001b[0m/    cover_image.jpg            \u001b[01;32mLICENSE.md\u001b[0m*\n",
            "\u001b[01;34mch02\u001b[0m/  \u001b[01;34mch05\u001b[0m/  \u001b[01;34mch08\u001b[0m/    \u001b[01;34mdataset\u001b[0m/                   map.png\n",
            "\u001b[01;34mch03\u001b[0m/  \u001b[01;34mch06\u001b[0m/  \u001b[01;34mcommon\u001b[0m/  equations_and_figures.zip  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGnG-d8coepJ",
        "outputId": "9968c85e-c76e-4fc4-97dc-c50c1df1da66"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from ch05.two_layer_net import TwoLayerNet\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size= 50, output_size=10)\n",
        "\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "for key in grad_numerical.keys():\n",
        "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
        "    print(key + \":\" + str(diff))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n",
            "W1:4.6413066312990484e-10\n",
            "b1:2.981441390680073e-09\n",
            "W2:6.907865677758007e-09\n",
            "b2:1.3990650467587963e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OusD5u_RqmUP",
        "outputId": "5f20a8ea-bcd8-42d8-c714-187e7223865e"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from ch05.two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09516666666666666 0.1009\n",
            "0.9039166666666667 0.9043\n",
            "0.9230833333333334 0.9236\n",
            "0.93365 0.9328\n",
            "0.94345 0.9404\n",
            "0.9515 0.9477\n",
            "0.9547833333333333 0.9518\n",
            "0.95935 0.9541\n",
            "0.9634 0.9572\n",
            "0.9654833333333334 0.9602\n",
            "0.9683166666666667 0.9624\n",
            "0.9696333333333333 0.9628\n",
            "0.9703666666666667 0.9638\n",
            "0.9744666666666667 0.9677\n",
            "0.9748666666666667 0.9675\n",
            "0.9771 0.9681\n",
            "0.97865 0.9688\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}