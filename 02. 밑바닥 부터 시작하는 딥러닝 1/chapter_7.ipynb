{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter 7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAyAEW_SWbv8"
      },
      "source": [
        "# 7.4.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfVQI_3IWX5S",
        "outputId": "dcbb05d0-f563-4f97-a240-958cb1b4ee3c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.random.rand(10,1,28,28)\n",
        "x.shape"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pCtXO3kWogh",
        "outputId": "9fa2c01b-14e4-4196-c28d-bf34cd6c15eb"
      },
      "source": [
        "x[0].shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4wyw-5uWqQZ",
        "outputId": "5bccde72-2933-4e1c-e68a-659647f78d42"
      },
      "source": [
        "x[1].shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCYS-Q0rZd7a",
        "outputId": "eff5d74e-8f41-4c33-d7e3-f7893e97ee91"
      },
      "source": [
        "!git clone https://github.com/WegraLee/deep-learning-from-scratch.git\n",
        "\n",
        "\n",
        "%cd deep-learning-from-scratch"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch'...\n",
            "remote: Enumerating objects: 826, done.\u001b[K\n",
            "remote: Total 826 (delta 0), reused 0 (delta 0), pack-reused 826\u001b[K\n",
            "Receiving objects: 100% (826/826), 52.21 MiB | 29.50 MiB/s, done.\n",
            "Resolving deltas: 100% (477/477), done.\n",
            "/content/deep-learning-from-scratch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1QtcLCNZrGN"
      },
      "source": [
        "import sys, os\n",
        "from common.util import im2col"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FawVRngMZv7S",
        "outputId": "1c59b469-5ab2-4c5b-c45d-27a99493c5a3"
      },
      "source": [
        "x1= np.random.rand(1,3,7,7)\n",
        "col1 = im2col(x1, 5,5, stride=1, pad=0)\n",
        "col1.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 75)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3RC4ASlZ5Dg"
      },
      "source": [
        "class Convolution:\n",
        "\n",
        "    def __init__(self, W, b, stride = 1, pad = 0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN , C , FH , FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + H+ 2 * self.pad -FH / self.stride)\n",
        "        out_w = int(1 + W+ 2 * self.pad -FW / self.stride)\n",
        "        \n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        \n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0,3,1,2)\n",
        "        \n",
        "        return out\n",
        "        \n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF9ysADnf9uh"
      },
      "source": [
        "from collections import OrderedDict\n",
        "class SimpleConvNet:\n",
        "\n",
        "    def __init__(self, input_dim=(1, 28, 28), \n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1tmiJVhgleX",
        "outputId": "b85bb8cb-6bc5-4a58-8893-bb2b9c364c00"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.trainer import Trainer\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n",
            "train loss:2.299335488560212\n",
            "=== epoch:1, train acc:0.178, test acc:0.187 ===\n",
            "train loss:2.298721514090408\n",
            "train loss:2.2949970844873477\n",
            "train loss:2.2885743052555223\n",
            "train loss:2.282808402819954\n",
            "train loss:2.2692652345304807\n",
            "train loss:2.247711046363409\n",
            "train loss:2.222993362485441\n",
            "train loss:2.2330468132843113\n",
            "train loss:2.2187767134053855\n",
            "train loss:2.182153704783177\n",
            "train loss:2.153790667117984\n",
            "train loss:2.1505155010346098\n",
            "train loss:2.1011324573039905\n",
            "train loss:1.9917574404689764\n",
            "train loss:2.005717553788888\n",
            "train loss:1.966244748161865\n",
            "train loss:1.89130834831011\n",
            "train loss:1.8343914913249209\n",
            "train loss:1.6285371083360554\n",
            "train loss:1.7534914103092123\n",
            "train loss:1.6345071004368743\n",
            "train loss:1.5769182729939368\n",
            "train loss:1.4682576221623238\n",
            "train loss:1.3377945289254407\n",
            "train loss:1.4006722821294075\n",
            "train loss:1.292009043150225\n",
            "train loss:1.1226659621582202\n",
            "train loss:1.1012148071875805\n",
            "train loss:1.1237014265891352\n",
            "train loss:0.8506274319020342\n",
            "train loss:0.9465549568118833\n",
            "train loss:0.9558950421533315\n",
            "train loss:0.8343508650287472\n",
            "train loss:0.7527451659188966\n",
            "train loss:0.8410561921629045\n",
            "train loss:0.7736692615091768\n",
            "train loss:0.7057670248471578\n",
            "train loss:0.6748765373511679\n",
            "train loss:0.6735353962502372\n",
            "train loss:0.7290454746246293\n",
            "train loss:0.4398350514375128\n",
            "train loss:0.8311680689326024\n",
            "train loss:0.5962178717467218\n",
            "train loss:0.6058102246363687\n",
            "train loss:0.6048026399176916\n",
            "train loss:0.6804614991907336\n",
            "train loss:0.6230264162647903\n",
            "train loss:0.5787820107845095\n",
            "train loss:0.5473072582366251\n",
            "train loss:0.6417364766727942\n",
            "train loss:0.6684204904191379\n",
            "train loss:0.4802767113734824\n",
            "train loss:0.4788753801158656\n",
            "train loss:0.5856036211338399\n",
            "train loss:0.7116896378971082\n",
            "train loss:0.4913230311855194\n",
            "train loss:0.6022233307385424\n",
            "train loss:0.5405713487572761\n",
            "train loss:0.4468220045617408\n",
            "train loss:0.37067293608832874\n",
            "train loss:0.6996816186402797\n",
            "train loss:0.21285726132216531\n",
            "train loss:0.6898314822246574\n",
            "train loss:0.5612247564105112\n",
            "train loss:0.6246478575313732\n",
            "train loss:0.38512890269216693\n",
            "train loss:0.4863598172130141\n",
            "train loss:0.39160959559646036\n",
            "train loss:0.6252709933638386\n",
            "train loss:0.4123348124162976\n",
            "train loss:0.43443475408800825\n",
            "train loss:0.621581289431692\n",
            "train loss:0.4425866607162412\n",
            "train loss:0.45643136009002616\n",
            "train loss:0.42412580285500545\n",
            "train loss:0.5152733449789139\n",
            "train loss:0.3514084768886174\n",
            "train loss:0.42402318056344307\n",
            "train loss:0.44669988988263964\n",
            "train loss:0.6187268259595793\n",
            "train loss:0.3022218584068815\n",
            "train loss:0.23207652537957837\n",
            "train loss:0.275484625656864\n",
            "train loss:0.3966292025695779\n",
            "train loss:0.320169975910436\n",
            "train loss:0.44849793417709755\n",
            "train loss:0.41612734810572116\n",
            "train loss:0.6397842572980053\n",
            "train loss:0.3548901000863983\n",
            "train loss:0.4191949825343311\n",
            "train loss:0.4764741238571367\n",
            "train loss:0.42361767936011946\n",
            "train loss:0.4853122028838014\n",
            "train loss:0.4566513148498117\n",
            "train loss:0.44153221217693855\n",
            "train loss:0.4611760395482733\n",
            "train loss:0.5633608574307302\n",
            "train loss:0.36855939537425814\n",
            "train loss:0.5013194421583278\n",
            "train loss:0.3909212746825173\n",
            "train loss:0.4516445252685337\n",
            "train loss:0.4702140897979269\n",
            "train loss:0.498860869109815\n",
            "train loss:0.5343540605416752\n",
            "train loss:0.427477850236364\n",
            "train loss:0.4542485047537469\n",
            "train loss:0.36308430603695574\n",
            "train loss:0.450945401604307\n",
            "train loss:0.39372399845096384\n",
            "train loss:0.2923734970303743\n",
            "train loss:0.31052769886107817\n",
            "train loss:0.4426789563285918\n",
            "train loss:0.4075262359128819\n",
            "train loss:0.40931830067478214\n",
            "train loss:0.36186575761038886\n",
            "train loss:0.3450572375811997\n",
            "train loss:0.4279589425949984\n",
            "train loss:0.2833289004347546\n",
            "train loss:0.2879111383463731\n",
            "train loss:0.338829395965815\n",
            "train loss:0.435858426824267\n",
            "train loss:0.2526772415921906\n",
            "train loss:0.29259809703513573\n",
            "train loss:0.4337299871544104\n",
            "train loss:0.29502382439359714\n",
            "train loss:0.4312185405797089\n",
            "train loss:0.3135822010842628\n",
            "train loss:0.503407826187714\n",
            "train loss:0.2844582624159112\n",
            "train loss:0.3448096793796559\n",
            "train loss:0.261934081646031\n",
            "train loss:0.2652086479981038\n",
            "train loss:0.2085821934322328\n",
            "train loss:0.24002255793690414\n",
            "train loss:0.41745300533133206\n",
            "train loss:0.32826969984857546\n",
            "train loss:0.30197790106508526\n",
            "train loss:0.2513912610660664\n",
            "train loss:0.2774189938635296\n",
            "train loss:0.447019614892205\n",
            "train loss:0.31007911710475805\n",
            "train loss:0.3522679230301308\n",
            "train loss:0.3237188244314522\n",
            "train loss:0.2746578461177345\n",
            "train loss:0.2654534630477487\n",
            "train loss:0.25848633940585364\n",
            "train loss:0.7779050485202121\n",
            "train loss:0.38310413713159663\n",
            "train loss:0.268313038349563\n",
            "train loss:0.28833990417316335\n",
            "train loss:0.2619174200615737\n",
            "train loss:0.5151471388033199\n",
            "train loss:0.2894682767575483\n",
            "train loss:0.2239887616298754\n",
            "train loss:0.4917352559680765\n",
            "train loss:0.27078059364572865\n",
            "train loss:0.22893697650757117\n",
            "train loss:0.3301364577802559\n",
            "train loss:0.3805331794834715\n",
            "train loss:0.3626653790563563\n",
            "train loss:0.2657966805249762\n",
            "train loss:0.2904639418102396\n",
            "train loss:0.5329233331992718\n",
            "train loss:0.3167354794838566\n",
            "train loss:0.3109180462312661\n",
            "train loss:0.2713754434690984\n",
            "train loss:0.3134528162481576\n",
            "train loss:0.2819421809698111\n",
            "train loss:0.4946401790395508\n",
            "train loss:0.22378073956431352\n",
            "train loss:0.2619239907501407\n",
            "train loss:0.23970706619528187\n",
            "train loss:0.2626246264713772\n",
            "train loss:0.30250305434289304\n",
            "train loss:0.19218465168953738\n",
            "train loss:0.2392472454307688\n",
            "train loss:0.2663964185225918\n",
            "train loss:0.3697385067614508\n",
            "train loss:0.39137689859184954\n",
            "train loss:0.33932435581525444\n",
            "train loss:0.24921469192267803\n",
            "train loss:0.23407837534363007\n",
            "train loss:0.23939546035821663\n",
            "train loss:0.3484833749779401\n",
            "train loss:0.27688459160140316\n",
            "train loss:0.4315576213348753\n",
            "train loss:0.22091656146711014\n",
            "train loss:0.24776625234379382\n",
            "train loss:0.21619921885778845\n",
            "train loss:0.31626919236866996\n",
            "train loss:0.27543682199843256\n",
            "train loss:0.20389550698885042\n",
            "train loss:0.3659346709841688\n",
            "train loss:0.26174896119511964\n",
            "train loss:0.33835931766627636\n",
            "train loss:0.2081624009916424\n",
            "train loss:0.3664969342930015\n",
            "train loss:0.3338164304296805\n",
            "train loss:0.16667421014326023\n",
            "train loss:0.2951193049687906\n",
            "train loss:0.20861956862698136\n",
            "train loss:0.17980675451258205\n",
            "train loss:0.22430764146312712\n",
            "train loss:0.19795963777685752\n",
            "train loss:0.153629040256567\n",
            "train loss:0.26515743642907713\n",
            "train loss:0.2176841768001075\n",
            "train loss:0.1983734851890171\n",
            "train loss:0.2660849298977268\n",
            "train loss:0.2129282101759029\n",
            "train loss:0.31396403287777175\n",
            "train loss:0.20092943747055153\n",
            "train loss:0.21588565581024025\n",
            "train loss:0.284877064488731\n",
            "train loss:0.3075994821614952\n",
            "train loss:0.48103540823429664\n",
            "train loss:0.127942494769904\n",
            "train loss:0.3369833413449828\n",
            "train loss:0.23291769641948104\n",
            "train loss:0.21341043064294213\n",
            "train loss:0.38387180445405705\n",
            "train loss:0.20593125316109748\n",
            "train loss:0.3365672056243625\n",
            "train loss:0.2938887459139712\n",
            "train loss:0.23395905594997846\n",
            "train loss:0.3070515912125355\n",
            "train loss:0.18930751626144196\n",
            "train loss:0.21169212191237086\n",
            "train loss:0.26279559190358853\n",
            "train loss:0.2793873632155683\n",
            "train loss:0.24429085295110423\n",
            "train loss:0.21880180620685494\n",
            "train loss:0.17425175675757423\n",
            "train loss:0.25545431838751226\n",
            "train loss:0.22481381913257895\n",
            "train loss:0.2847493757092556\n",
            "train loss:0.20292812183367775\n",
            "train loss:0.34568061878017536\n",
            "train loss:0.19666865232990735\n",
            "train loss:0.23539479870734742\n",
            "train loss:0.2764132959703611\n",
            "train loss:0.21625364011492082\n",
            "train loss:0.15101119368511712\n",
            "train loss:0.1568286959974824\n",
            "train loss:0.1159865336573897\n",
            "train loss:0.348059476598093\n",
            "train loss:0.21879646714899337\n",
            "train loss:0.30862084511052645\n",
            "train loss:0.21831820540283645\n",
            "train loss:0.13495117021330263\n",
            "train loss:0.18207267367987715\n",
            "train loss:0.16944755654407803\n",
            "train loss:0.32240227029991475\n",
            "train loss:0.20754869311704774\n",
            "train loss:0.3069704158442367\n",
            "train loss:0.25875126348959865\n",
            "train loss:0.34137516678805113\n",
            "train loss:0.3093610848838189\n",
            "train loss:0.346679786754353\n",
            "train loss:0.20199748216846156\n",
            "train loss:0.3848708146914143\n",
            "train loss:0.1621787898039144\n",
            "train loss:0.3527891292473679\n",
            "train loss:0.2165132600788807\n",
            "train loss:0.18855819158480777\n",
            "train loss:0.2589010339011348\n",
            "train loss:0.23716108334453537\n",
            "train loss:0.2208219557952219\n",
            "train loss:0.3030760568613977\n",
            "train loss:0.2585995085421093\n",
            "train loss:0.4243606163400871\n",
            "train loss:0.18759391299662895\n",
            "train loss:0.1685355381706303\n",
            "train loss:0.24181391688188325\n",
            "train loss:0.18892299323040496\n",
            "train loss:0.19859190119422135\n",
            "train loss:0.15075966004882466\n",
            "train loss:0.3269083356421034\n",
            "train loss:0.3109548987905741\n",
            "train loss:0.1938241383221405\n",
            "train loss:0.38789170872176354\n",
            "train loss:0.15862691732496703\n",
            "train loss:0.18704684289862938\n",
            "train loss:0.2919877645572198\n",
            "train loss:0.16104475213943611\n",
            "train loss:0.1965751745211982\n",
            "train loss:0.18003848642602757\n",
            "train loss:0.3649987428861423\n",
            "train loss:0.16950813832950637\n",
            "train loss:0.2811681687648731\n",
            "train loss:0.18258488371996023\n",
            "train loss:0.34857895987033105\n",
            "train loss:0.2078695544815658\n",
            "train loss:0.20744544402880216\n",
            "train loss:0.2983353956817534\n",
            "train loss:0.17008890487799558\n",
            "train loss:0.18211326477710366\n",
            "train loss:0.2411456986007581\n",
            "train loss:0.1590294924618709\n",
            "train loss:0.23694288487515527\n",
            "train loss:0.10717073022465345\n",
            "train loss:0.19027250033076915\n",
            "train loss:0.2617615752297863\n",
            "train loss:0.3004478075240278\n",
            "train loss:0.19852614637714208\n",
            "train loss:0.2202091975665997\n",
            "train loss:0.3134832007005745\n",
            "train loss:0.19732046378911192\n",
            "train loss:0.22843245926572034\n",
            "train loss:0.2801019167719369\n",
            "train loss:0.2364183205652953\n",
            "train loss:0.19201588972262956\n",
            "train loss:0.2046252470862715\n",
            "train loss:0.21925948755908845\n",
            "train loss:0.24268697822809293\n",
            "train loss:0.19103394382805905\n",
            "train loss:0.13104841950442273\n",
            "train loss:0.23031853207353803\n",
            "train loss:0.1913645170376381\n",
            "train loss:0.295319622360515\n",
            "train loss:0.25481434479558945\n",
            "train loss:0.1744032523197883\n",
            "train loss:0.1233004262970891\n",
            "train loss:0.340741960001763\n",
            "train loss:0.2763363234926244\n",
            "train loss:0.23063420418118274\n",
            "train loss:0.26471226693722366\n",
            "train loss:0.20655238985004587\n",
            "train loss:0.17903027120721973\n",
            "train loss:0.10831619387645594\n",
            "train loss:0.38589170956462987\n",
            "train loss:0.2887065141021492\n",
            "train loss:0.2428586323312043\n",
            "train loss:0.15166769183168666\n",
            "train loss:0.19187638407430396\n",
            "train loss:0.09499421603748257\n",
            "train loss:0.08467630974439089\n",
            "train loss:0.16055587990416648\n",
            "train loss:0.17974783419602552\n",
            "train loss:0.28539269521552363\n",
            "train loss:0.13592280223332165\n",
            "train loss:0.15170207681762707\n",
            "train loss:0.12146109724934771\n",
            "train loss:0.19551534265373982\n",
            "train loss:0.16768613786428083\n",
            "train loss:0.14638045328222204\n",
            "train loss:0.21887796297860562\n",
            "train loss:0.2333566340892582\n",
            "train loss:0.09452698320348386\n",
            "train loss:0.24437392734595423\n",
            "train loss:0.18045052121388663\n",
            "train loss:0.1268782303233055\n",
            "train loss:0.17405674600024215\n",
            "train loss:0.10614494834477622\n",
            "train loss:0.1901743396715447\n",
            "train loss:0.35800854588035813\n",
            "train loss:0.10400870388860983\n",
            "train loss:0.23666521500945198\n",
            "train loss:0.09637085621813579\n",
            "train loss:0.24027190754373226\n",
            "train loss:0.24980652729706945\n",
            "train loss:0.13235067145232282\n",
            "train loss:0.21997729354448495\n",
            "train loss:0.24277127008713972\n",
            "train loss:0.17241270064173858\n",
            "train loss:0.17272577002548103\n",
            "train loss:0.12259535670349213\n",
            "train loss:0.2262498577653701\n",
            "train loss:0.14879564737182233\n",
            "train loss:0.1409675276315177\n",
            "train loss:0.17600518503594398\n",
            "train loss:0.14097565857220823\n",
            "train loss:0.19385810172081044\n",
            "train loss:0.177842669604484\n",
            "train loss:0.1389520413001629\n",
            "train loss:0.3136172004255085\n",
            "train loss:0.20266594764400295\n",
            "train loss:0.21048725703492566\n",
            "train loss:0.1983333023704564\n",
            "train loss:0.32715720269588017\n",
            "train loss:0.1946643607944244\n",
            "train loss:0.14520983665626974\n",
            "train loss:0.20301887018699574\n",
            "train loss:0.14516613553452373\n",
            "train loss:0.1596178057887624\n",
            "train loss:0.18662449521541646\n",
            "train loss:0.20300839618567187\n",
            "train loss:0.16533167429893555\n",
            "train loss:0.15728400934979817\n",
            "train loss:0.14436711290849913\n",
            "train loss:0.20416782203332834\n",
            "train loss:0.17379045982771266\n",
            "train loss:0.1311723310859538\n",
            "train loss:0.2080621947840398\n",
            "train loss:0.23975883519772082\n",
            "train loss:0.16770197416989457\n",
            "train loss:0.10922470960501815\n",
            "train loss:0.2584942826668459\n",
            "train loss:0.14791457639105132\n",
            "train loss:0.14503540362868308\n",
            "train loss:0.17182514513319558\n",
            "train loss:0.29818524618425146\n",
            "train loss:0.12648875410578775\n",
            "train loss:0.29358692087987087\n",
            "train loss:0.14141905183583284\n",
            "train loss:0.1851362858641803\n",
            "train loss:0.1089387756933061\n",
            "train loss:0.17462848792758837\n",
            "train loss:0.10109665045773446\n",
            "train loss:0.1388066911700631\n",
            "train loss:0.1645142117444074\n",
            "train loss:0.1234426972969592\n",
            "train loss:0.11900325112177831\n",
            "train loss:0.165710279592856\n",
            "train loss:0.2635492001744914\n",
            "train loss:0.14107704591539277\n",
            "train loss:0.08757545639962089\n",
            "train loss:0.16217008648182948\n",
            "train loss:0.17203791363827026\n",
            "train loss:0.1747865094328334\n",
            "train loss:0.09683360679148668\n",
            "train loss:0.24175280026994372\n",
            "train loss:0.052835874663008146\n",
            "train loss:0.19647269015766114\n",
            "train loss:0.16171568440291914\n",
            "train loss:0.153123219645243\n",
            "train loss:0.09497998261369078\n",
            "train loss:0.2559239954044771\n",
            "train loss:0.15300282389888212\n",
            "train loss:0.20225666171551268\n",
            "train loss:0.25263034859111216\n",
            "train loss:0.05560489291213181\n",
            "train loss:0.14967980418595325\n",
            "train loss:0.07869955148656778\n",
            "train loss:0.19048866116727525\n",
            "train loss:0.1682510699404286\n",
            "train loss:0.147908021471666\n",
            "train loss:0.18117423244790978\n",
            "train loss:0.09724021429279327\n",
            "train loss:0.08861709303603087\n",
            "train loss:0.10595541247655652\n",
            "train loss:0.15104232225112693\n",
            "train loss:0.15994922112409082\n",
            "train loss:0.14019553297969428\n",
            "train loss:0.1544451792696273\n",
            "train loss:0.10212863596254593\n",
            "train loss:0.1895968355697296\n",
            "train loss:0.19195098774081346\n",
            "train loss:0.17733695396999397\n",
            "train loss:0.14608246134013678\n",
            "train loss:0.16187442291938103\n",
            "train loss:0.22211499440175203\n",
            "train loss:0.08485058616626623\n",
            "train loss:0.1379321415143437\n",
            "train loss:0.1946933045867379\n",
            "train loss:0.309577350873353\n",
            "train loss:0.13306973400862712\n",
            "train loss:0.14115644402025873\n",
            "train loss:0.13028342027939094\n",
            "train loss:0.10042634990871781\n",
            "train loss:0.18144007346029717\n",
            "train loss:0.2946835600081261\n",
            "train loss:0.1448400421233712\n",
            "train loss:0.14551597277592596\n",
            "train loss:0.10597690186169251\n",
            "train loss:0.1726555102256484\n",
            "train loss:0.21635218675814002\n",
            "train loss:0.16052142061443062\n",
            "train loss:0.1383525973482509\n",
            "train loss:0.11315018020746423\n",
            "train loss:0.18263624357167355\n",
            "train loss:0.26179715092824074\n",
            "train loss:0.16066050811686128\n",
            "train loss:0.13455875283738114\n",
            "train loss:0.1454908883070366\n",
            "train loss:0.14180889600332486\n",
            "train loss:0.2100332037639331\n",
            "train loss:0.19030527781449058\n",
            "train loss:0.15374418978225474\n",
            "train loss:0.1316678380858395\n",
            "train loss:0.20965324983805567\n",
            "train loss:0.14391389761689194\n",
            "train loss:0.12619714823466457\n",
            "train loss:0.12752273251490817\n",
            "train loss:0.07988656066116584\n",
            "train loss:0.11979287715854209\n",
            "train loss:0.13385145147649116\n",
            "train loss:0.09865392087993376\n",
            "train loss:0.19573989319703375\n",
            "train loss:0.09882369528832656\n",
            "train loss:0.1170595804498892\n",
            "train loss:0.10273937765792646\n",
            "train loss:0.1605907299518841\n",
            "train loss:0.10643964830156491\n",
            "train loss:0.2591796430336505\n",
            "train loss:0.06283590188749534\n",
            "train loss:0.09451987416441585\n",
            "train loss:0.13362326577802713\n",
            "train loss:0.31966066262221615\n",
            "train loss:0.11181107253987604\n",
            "train loss:0.09772735344221647\n",
            "train loss:0.07813305722869979\n",
            "train loss:0.10728474739745449\n",
            "train loss:0.20207281878719546\n",
            "train loss:0.13680790445683708\n",
            "train loss:0.1403489663754198\n",
            "train loss:0.15040425159935528\n",
            "train loss:0.13289096156792218\n",
            "train loss:0.14466811041835254\n",
            "train loss:0.13132124081124663\n",
            "train loss:0.07073369156820478\n",
            "train loss:0.1347686802876162\n",
            "train loss:0.1244348894621548\n",
            "train loss:0.16332711772964895\n",
            "train loss:0.04487660622271533\n",
            "train loss:0.09037716859839201\n",
            "train loss:0.08632128565436947\n",
            "train loss:0.20080830728552285\n",
            "train loss:0.12540296548303206\n",
            "train loss:0.1559621255348005\n",
            "train loss:0.18446329815901066\n",
            "train loss:0.14142760901849322\n",
            "train loss:0.1028486229305886\n",
            "train loss:0.2056190376806751\n",
            "train loss:0.0747633013002563\n",
            "train loss:0.10386996808568391\n",
            "train loss:0.15256688240367144\n",
            "train loss:0.0847892104528838\n",
            "train loss:0.22930216511515733\n",
            "train loss:0.12805873651330207\n",
            "train loss:0.0659623493613605\n",
            "train loss:0.0714204941538348\n",
            "train loss:0.1188467107549663\n",
            "train loss:0.07888026637350738\n",
            "train loss:0.1636423483743179\n",
            "train loss:0.14912005462056538\n",
            "train loss:0.10174226481010823\n",
            "train loss:0.10699363834747518\n",
            "train loss:0.10742091596757693\n",
            "train loss:0.17432669646309246\n",
            "train loss:0.08449821644492435\n",
            "train loss:0.1309392261656444\n",
            "train loss:0.06756009140309778\n",
            "train loss:0.08865028911418003\n",
            "train loss:0.11514164616148033\n",
            "train loss:0.15364136809705214\n",
            "train loss:0.1687747086127018\n",
            "train loss:0.08987651353566405\n",
            "train loss:0.1968982906933834\n",
            "train loss:0.08031619139332007\n",
            "train loss:0.12142661341322344\n",
            "train loss:0.11670017306936786\n",
            "train loss:0.09257143272959822\n",
            "train loss:0.09836019937379276\n",
            "train loss:0.17376590659542313\n",
            "train loss:0.2955262135707548\n",
            "train loss:0.05036638651568541\n",
            "train loss:0.11788197028156394\n",
            "train loss:0.10752214525145938\n",
            "train loss:0.2242995541968504\n",
            "train loss:0.10371011237958559\n",
            "train loss:0.07852916766099116\n",
            "train loss:0.14503031365102267\n",
            "train loss:0.15514758193611658\n",
            "train loss:0.14876285749165943\n",
            "train loss:0.0475688918321231\n",
            "train loss:0.08617643054428069\n",
            "train loss:0.13816801907959952\n",
            "train loss:0.09279414808541492\n",
            "train loss:0.07051538836138431\n",
            "train loss:0.08020857635122415\n",
            "train loss:0.11614656277548828\n",
            "train loss:0.17635685689755284\n",
            "train loss:0.15241987603071303\n",
            "train loss:0.07517965675567068\n",
            "train loss:0.17671097712078537\n",
            "train loss:0.09001818066578426\n",
            "train loss:0.08340923484695613\n",
            "train loss:0.1429896940266306\n",
            "train loss:0.20344157909646016\n",
            "train loss:0.13456979192709595\n",
            "train loss:0.13860202585148948\n",
            "train loss:0.17021802149610252\n",
            "train loss:0.12378535714446101\n",
            "train loss:0.10062294656591345\n",
            "train loss:0.07328040529578943\n",
            "train loss:0.14653602583228703\n",
            "train loss:0.1193263148629345\n",
            "train loss:0.11665935851126984\n",
            "train loss:0.11507125952468973\n",
            "train loss:0.13372716449421493\n",
            "train loss:0.06477847902200663\n",
            "train loss:0.1624261410546662\n",
            "train loss:0.07008596737945384\n",
            "train loss:0.10365044221802097\n",
            "train loss:0.1130126414780814\n",
            "train loss:0.17152479302755708\n",
            "train loss:0.18779196184757588\n",
            "train loss:0.08570066545739408\n",
            "train loss:0.06063174917347933\n",
            "=== epoch:2, train acc:0.962, test acc:0.965 ===\n",
            "train loss:0.16929196929025842\n",
            "train loss:0.05630643211786653\n",
            "train loss:0.11105857896033934\n",
            "train loss:0.22829626984416\n",
            "train loss:0.0670424431141006\n",
            "train loss:0.17038514998249635\n",
            "train loss:0.15163353628539725\n",
            "train loss:0.12183654027970184\n",
            "train loss:0.1900598897924996\n",
            "train loss:0.15927295753466258\n",
            "train loss:0.10349794585251676\n",
            "train loss:0.11049610025104771\n",
            "train loss:0.10797514987441337\n",
            "train loss:0.11380895946093889\n",
            "train loss:0.11055528805997619\n",
            "train loss:0.14416879923301387\n",
            "train loss:0.05733664956494959\n",
            "train loss:0.13319193744663962\n",
            "train loss:0.10232587783767194\n",
            "train loss:0.09973170266772316\n",
            "train loss:0.10181980478191982\n",
            "train loss:0.0747305322675851\n",
            "train loss:0.05560025166835581\n",
            "train loss:0.05535629571514088\n",
            "train loss:0.058046699735384365\n",
            "train loss:0.06096422717072819\n",
            "train loss:0.10286383781233617\n",
            "train loss:0.10466022763671404\n",
            "train loss:0.1081834774780274\n",
            "train loss:0.07383255879978448\n",
            "train loss:0.10282543272011425\n",
            "train loss:0.08847720063825117\n",
            "train loss:0.04423358703658419\n",
            "train loss:0.0646679488268815\n",
            "train loss:0.09016909176577315\n",
            "train loss:0.07312684017375773\n",
            "train loss:0.22955974686667285\n",
            "train loss:0.11303301117920282\n",
            "train loss:0.07413582251610273\n",
            "train loss:0.08678368567755776\n",
            "train loss:0.08150635785457806\n",
            "train loss:0.07175268880962787\n",
            "train loss:0.09597916123706453\n",
            "train loss:0.08056730910091989\n",
            "train loss:0.06790410630091716\n",
            "train loss:0.10195642784529449\n",
            "train loss:0.24966127455682582\n",
            "train loss:0.03397244860428631\n",
            "train loss:0.11665759326853438\n",
            "train loss:0.058679976609509854\n",
            "train loss:0.11216176022723845\n",
            "train loss:0.12386078273237704\n",
            "train loss:0.1212596522913217\n",
            "train loss:0.024571021614851748\n",
            "train loss:0.08106162918203486\n",
            "train loss:0.030198936090157807\n",
            "train loss:0.17090590430971772\n",
            "train loss:0.12124164478585793\n",
            "train loss:0.09682795472278873\n",
            "train loss:0.08765502633109605\n",
            "train loss:0.07019674351801103\n",
            "train loss:0.10916862000891575\n",
            "train loss:0.086240651230856\n",
            "train loss:0.144605718643843\n",
            "train loss:0.06932540834410111\n",
            "train loss:0.20794823424935896\n",
            "train loss:0.0784166660955594\n",
            "train loss:0.1448426137452315\n",
            "train loss:0.18425975529408467\n",
            "train loss:0.05820812512830496\n",
            "train loss:0.08934297069034866\n",
            "train loss:0.09940064080632788\n",
            "train loss:0.13786594603632113\n",
            "train loss:0.07702154594296277\n",
            "train loss:0.1667010312744056\n",
            "train loss:0.15244844485030687\n",
            "train loss:0.19020899276414396\n",
            "train loss:0.11710652172447893\n",
            "train loss:0.15061700625109675\n",
            "train loss:0.15871683161813432\n",
            "train loss:0.06400467280739744\n",
            "train loss:0.08495547916043894\n",
            "train loss:0.046775311146000684\n",
            "train loss:0.07476024724098072\n",
            "train loss:0.210455976951026\n",
            "train loss:0.04646902646974932\n",
            "train loss:0.09279634398207832\n",
            "train loss:0.2219195406372233\n",
            "train loss:0.11928578142482742\n",
            "train loss:0.08094477289079471\n",
            "train loss:0.06489790745630715\n",
            "train loss:0.04779398785980358\n",
            "train loss:0.08387520547555391\n",
            "train loss:0.05072666082190387\n",
            "train loss:0.08259434782962413\n",
            "train loss:0.11008079828365183\n",
            "train loss:0.14827511419307604\n",
            "train loss:0.11512014343927614\n",
            "train loss:0.12279088326669277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cWT2mBWgnar"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    \"\"\"\n",
        "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
        "    \"\"\"\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "network = SimpleConvNet()\n",
        "# 무작위(랜덤) 초기화 후의 가중치\n",
        "filter_show(network.params['W1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmz-K9YFhREO"
      },
      "source": [
        "\n",
        "# 학습된 가중치\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQbJhld1hUzr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}