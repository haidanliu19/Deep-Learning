{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "chapter 7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAyAEW_SWbv8"
      },
      "source": [
        "# 7.4.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfVQI_3IWX5S",
        "outputId": "92d11d22-1bd3-478e-b90b-c79b9aa76d12"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.random.rand(10,1,28,28)\n",
        "x.shape"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pCtXO3kWogh",
        "outputId": "23c3ec71-9681-45a5-f492-b6ae8c2daa2a"
      },
      "source": [
        "x[0].shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4wyw-5uWqQZ",
        "outputId": "a95ebe3f-c117-4fc3-8341-590b9efa0b96"
      },
      "source": [
        "x[1].shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCYS-Q0rZd7a",
        "outputId": "6a5cb04c-0ede-4a29-f4a5-c57650e8dda2"
      },
      "source": [
        "!git clone https://github.com/WegraLee/deep-learning-from-scratch.git\n",
        "\n",
        "\n",
        "%cd deep-learning-from-scratch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'deep-learning-from-scratch' already exists and is not an empty directory.\n",
            "/content/deep-learning-from-scratch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1QtcLCNZrGN"
      },
      "source": [
        "import sys, os\n",
        "from common.util import im2col\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.trainer import Trainer\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FawVRngMZv7S",
        "outputId": "761c19fe-a1cd-43bd-f35c-f74813e8ce07"
      },
      "source": [
        "x1= np.random.rand(1,3,7,7)\n",
        "col1 = im2col(x1, 5,5, stride=1, pad=0)\n",
        "col1.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 75)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3RC4ASlZ5Dg"
      },
      "source": [
        "class Convolution:\n",
        "\n",
        "    def __init__(self, W, b, stride = 1, pad = 0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN , C , FH , FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + H+ 2 * self.pad -FH / self.stride)\n",
        "        out_w = int(1 + W+ 2 * self.pad -FW / self.stride)\n",
        "        \n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        \n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0,3,1,2)\n",
        "        \n",
        "        return out\n",
        "        \n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF9ysADnf9uh"
      },
      "source": [
        "\n",
        "class SimpleConvNet:\n",
        "\n",
        "    def __init__(self, input_dim=(1, 28, 28), \n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g1tmiJVhgleX",
        "outputId": "aae7d946-4ac8-4595-a261-c82a55cff01a"
      },
      "source": [
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "train loss:0.005687039013253744\n",
            "train loss:0.0001615737867232268\n",
            "train loss:0.008907551209307285\n",
            "train loss:0.006094189900936183\n",
            "train loss:0.006704305880001224\n",
            "train loss:0.0030015917361765625\n",
            "train loss:0.01592024717414336\n",
            "train loss:0.0006956399800254893\n",
            "train loss:0.003114935607634038\n",
            "train loss:0.0009378438152399526\n",
            "train loss:0.0030957448734903153\n",
            "train loss:0.004469928344228304\n",
            "train loss:0.000474365972984457\n",
            "train loss:0.009974845031667918\n",
            "train loss:0.005390231774552897\n",
            "train loss:0.0016550683644952957\n",
            "train loss:0.0036627511370985327\n",
            "train loss:0.0006700717043996688\n",
            "train loss:0.009490795695830756\n",
            "train loss:0.0010885865952441275\n",
            "train loss:0.0044013793067919065\n",
            "train loss:0.009113241360884988\n",
            "train loss:0.0015224550463287271\n",
            "train loss:0.0024239959933629335\n",
            "train loss:0.0010623392248643468\n",
            "train loss:0.005203172026327715\n",
            "train loss:0.015430962011362659\n",
            "train loss:0.0008811773550405745\n",
            "train loss:0.0023348145751065186\n",
            "train loss:0.0007483555589675376\n",
            "train loss:0.0018438283312426106\n",
            "train loss:0.0018815771541132278\n",
            "train loss:0.0041592920648382\n",
            "train loss:0.003296321637190192\n",
            "train loss:0.02314860390530768\n",
            "train loss:0.022825851374837253\n",
            "train loss:0.0014784716611098922\n",
            "train loss:0.002318624644237285\n",
            "train loss:0.0004059759554210373\n",
            "train loss:0.010555413347348646\n",
            "train loss:0.009207338431523665\n",
            "train loss:0.006362396796053077\n",
            "train loss:0.004208629465787187\n",
            "train loss:0.002784250632514624\n",
            "train loss:0.004617077827482849\n",
            "train loss:0.0005557326326415534\n",
            "train loss:0.003895067860987739\n",
            "train loss:0.0013518324367422468\n",
            "train loss:0.0013683838785922015\n",
            "train loss:0.0028574916826278243\n",
            "train loss:0.000951631048693634\n",
            "train loss:0.0024823274648955876\n",
            "train loss:0.006148779648426848\n",
            "train loss:0.004906312943165539\n",
            "train loss:0.0006349363180220948\n",
            "train loss:0.0011924112146915735\n",
            "train loss:0.0004602561599049543\n",
            "train loss:0.0010919230248014331\n",
            "train loss:0.0006195559983773619\n",
            "train loss:0.0031777397461350853\n",
            "train loss:0.0032028975105878278\n",
            "train loss:0.0022189256578949348\n",
            "train loss:0.011504897629205335\n",
            "train loss:0.013171464614943774\n",
            "train loss:0.005450258120504314\n",
            "train loss:0.0006015515954411497\n",
            "train loss:0.0016397358289871212\n",
            "train loss:0.00537380220661819\n",
            "train loss:0.0031667853127431158\n",
            "train loss:0.0014428939763965845\n",
            "train loss:0.0018263081955466691\n",
            "train loss:0.0019236706602239234\n",
            "train loss:0.0005961135036051201\n",
            "train loss:0.0046292582445630805\n",
            "train loss:0.0010885448616763127\n",
            "train loss:0.0034629974832551037\n",
            "train loss:0.0008309847545280659\n",
            "train loss:0.0018646996115881554\n",
            "train loss:0.013792113515545672\n",
            "train loss:0.0037564486009081834\n",
            "train loss:0.00568535633377541\n",
            "train loss:0.0015652859856144367\n",
            "train loss:0.003744772621757218\n",
            "train loss:0.0019608077869458485\n",
            "train loss:0.00541679496237501\n",
            "train loss:0.0011479654559875606\n",
            "train loss:0.0011332756552683283\n",
            "train loss:0.005281569909826473\n",
            "train loss:0.009471980756861596\n",
            "train loss:0.0008939207421431277\n",
            "train loss:0.0036643043944662034\n",
            "train loss:0.0019506527719686912\n",
            "train loss:0.0058831567635849345\n",
            "train loss:0.0018783763696356831\n",
            "train loss:0.0007722684561937955\n",
            "train loss:0.0050317871826407535\n",
            "train loss:0.005250365946842279\n",
            "train loss:0.007003364437653354\n",
            "train loss:0.0643517221928864\n",
            "train loss:0.00047651825134941767\n",
            "train loss:0.011752999423346835\n",
            "train loss:0.008561400694067231\n",
            "train loss:0.00036547334609167035\n",
            "train loss:0.0005163463925477157\n",
            "train loss:0.004253927763086595\n",
            "train loss:0.01262300165218014\n",
            "train loss:0.0001353968137122996\n",
            "train loss:0.0002258631522457008\n",
            "train loss:0.0009037410903550919\n",
            "train loss:0.001292825136993185\n",
            "train loss:9.709524235262171e-05\n",
            "train loss:0.011195995537665584\n",
            "train loss:0.0008366153871913958\n",
            "train loss:0.0006410063895907804\n",
            "train loss:0.0020705662646009052\n",
            "train loss:0.0008202354654221844\n",
            "train loss:0.0008843112647874025\n",
            "train loss:0.001437635159171825\n",
            "train loss:0.0030708951510076267\n",
            "train loss:0.0017894792054234136\n",
            "train loss:0.006145725354170497\n",
            "train loss:0.0037618595411336563\n",
            "train loss:0.0003606788922588262\n",
            "train loss:0.007165533346592193\n",
            "train loss:0.007737503566768555\n",
            "train loss:0.0007768079239252702\n",
            "train loss:0.00031223573824284664\n",
            "train loss:0.0005493599180568791\n",
            "train loss:0.011231657733728157\n",
            "train loss:0.0015507161495270838\n",
            "train loss:0.0012562411266068105\n",
            "train loss:0.0007717853979770566\n",
            "train loss:0.007859449983893757\n",
            "train loss:0.01773606185916855\n",
            "train loss:0.0037913946974220914\n",
            "train loss:0.006333065456416419\n",
            "train loss:0.0032325094317625057\n",
            "train loss:0.00041618365936329816\n",
            "train loss:0.00545120183133026\n",
            "train loss:0.0218657238892103\n",
            "train loss:0.006751361729888956\n",
            "train loss:0.00034443436695429\n",
            "train loss:0.008025394028332892\n",
            "train loss:0.00549210269356388\n",
            "train loss:0.001616347962612698\n",
            "train loss:0.017985494406643116\n",
            "train loss:0.005202471249144996\n",
            "train loss:0.0029023453230676683\n",
            "train loss:0.008150578340164054\n",
            "train loss:0.005787440987142818\n",
            "train loss:0.0008129917584480188\n",
            "train loss:0.02647219424857583\n",
            "train loss:0.002598705928397673\n",
            "train loss:0.018542512283834297\n",
            "train loss:0.003434331708603306\n",
            "train loss:0.003013277589188714\n",
            "train loss:0.0018055131581247064\n",
            "train loss:0.0017084746096749162\n",
            "train loss:0.008131468474233168\n",
            "train loss:0.0017223946706788939\n",
            "train loss:0.000414562809315111\n",
            "train loss:0.0012586016664612642\n",
            "train loss:0.0011801187860611352\n",
            "train loss:0.056727902713044065\n",
            "train loss:0.004979365271077731\n",
            "train loss:0.0018025508532425972\n",
            "train loss:0.009267898585118357\n",
            "train loss:0.0032732242334260974\n",
            "train loss:0.0021710308487552037\n",
            "train loss:0.0014580683640672632\n",
            "train loss:0.007548629616376951\n",
            "train loss:0.10115456579493896\n",
            "train loss:0.005589669337814402\n",
            "train loss:0.0036129583994371937\n",
            "train loss:0.00331938150424919\n",
            "train loss:0.0006852432022822066\n",
            "train loss:0.01028336648988481\n",
            "train loss:0.01100086470116932\n",
            "train loss:0.001828608998224996\n",
            "train loss:0.0036323942088395274\n",
            "train loss:0.009980206354963965\n",
            "train loss:0.005334926200943141\n",
            "train loss:0.002225966689780257\n",
            "train loss:0.0025353920162954247\n",
            "train loss:0.0019299256235431276\n",
            "train loss:0.011936401747630766\n",
            "train loss:0.0016903298933488831\n",
            "train loss:0.0023878066945007197\n",
            "train loss:0.006789243998950408\n",
            "train loss:0.00011465517334058862\n",
            "train loss:0.005562281220801576\n",
            "=== epoch:13, train acc:0.997, test acc:0.989 ===\n",
            "train loss:0.004046024075381231\n",
            "train loss:0.00616433002382049\n",
            "train loss:0.004421736600301866\n",
            "train loss:0.013651706948959818\n",
            "train loss:0.001572000749753327\n",
            "train loss:0.016199994653517333\n",
            "train loss:0.0003661188677649785\n",
            "train loss:0.009910045031905502\n",
            "train loss:0.002642926549284735\n",
            "train loss:0.013688875718423558\n",
            "train loss:0.0025046336759986434\n",
            "train loss:0.012446036691520078\n",
            "train loss:0.0003590822412377688\n",
            "train loss:0.0015304176455822088\n",
            "train loss:7.129881809524885e-05\n",
            "train loss:0.005015353312573898\n",
            "train loss:0.002976483292560369\n",
            "train loss:0.000393261041944775\n",
            "train loss:0.00038145177258245856\n",
            "train loss:0.0006298464453805253\n",
            "train loss:0.002470911408488368\n",
            "train loss:0.0011681379608013776\n",
            "train loss:0.0007644892099824556\n",
            "train loss:0.0005869936059411661\n",
            "train loss:0.0044893737166206045\n",
            "train loss:0.0029535504039102955\n",
            "train loss:0.0016980065347142546\n",
            "train loss:0.009784134582853375\n",
            "train loss:0.0009718955310215271\n",
            "train loss:0.0006302870172990762\n",
            "train loss:0.07301211951582669\n",
            "train loss:0.0009637748639133964\n",
            "train loss:0.002368383830168686\n",
            "train loss:0.0006203649914543644\n",
            "train loss:0.0006461131691267678\n",
            "train loss:0.008367002340488152\n",
            "train loss:0.03361858539945351\n",
            "train loss:0.0029966456475689195\n",
            "train loss:0.004403980436844885\n",
            "train loss:0.009483834934659534\n",
            "train loss:0.0025103071345026406\n",
            "train loss:0.020686173975088522\n",
            "train loss:0.02950776190832428\n",
            "train loss:0.0038528468278082872\n",
            "train loss:0.002485532222012378\n",
            "train loss:0.0007877749112076658\n",
            "train loss:0.007126506006026449\n",
            "train loss:0.006163484466797\n",
            "train loss:0.001788986730062325\n",
            "train loss:0.0078020518448202285\n",
            "train loss:0.004963614050828142\n",
            "train loss:0.0037295217266435365\n",
            "train loss:0.0010385188255873038\n",
            "train loss:0.012178624458252199\n",
            "train loss:0.001221859257141962\n",
            "train loss:0.0020232118019647876\n",
            "train loss:0.009207302568707165\n",
            "train loss:0.0015867543358159461\n",
            "train loss:0.008913152102522835\n",
            "train loss:0.0017187392204602336\n",
            "train loss:0.008595289405497326\n",
            "train loss:0.0020679808635486224\n",
            "train loss:0.006880797464750699\n",
            "train loss:0.01360464347872419\n",
            "train loss:0.002726170540240188\n",
            "train loss:0.008862717492274508\n",
            "train loss:0.009297399919216322\n",
            "train loss:0.005859022228544698\n",
            "train loss:0.007495671310332452\n",
            "train loss:0.028841328895102908\n",
            "train loss:0.002733435580266649\n",
            "train loss:0.005886021389447349\n",
            "train loss:0.004227973064966611\n",
            "train loss:0.0064314363010150154\n",
            "train loss:0.008481050515708214\n",
            "train loss:0.009393350605504877\n",
            "train loss:0.0020853072669509043\n",
            "train loss:0.003291839645658174\n",
            "train loss:0.016187156892169843\n",
            "train loss:0.002495936287319121\n",
            "train loss:0.012212501857807884\n",
            "train loss:0.0008656586887250808\n",
            "train loss:0.00734677620984302\n",
            "train loss:0.009138731790589745\n",
            "train loss:0.005518914932176028\n",
            "train loss:0.0028977107632647114\n",
            "train loss:0.0018790797676968643\n",
            "train loss:0.0006749155592934098\n",
            "train loss:0.0008879158846250409\n",
            "train loss:0.00771147413319913\n",
            "train loss:0.005986443600225646\n",
            "train loss:0.011155293420585901\n",
            "train loss:0.0033607901947682894\n",
            "train loss:0.007154583580015722\n",
            "train loss:0.0008268095345613751\n",
            "train loss:0.004648926745956097\n",
            "train loss:0.004882196023553723\n",
            "train loss:0.002736105621015589\n",
            "train loss:0.05336285972151601\n",
            "train loss:0.0016902550940581273\n",
            "train loss:0.0014937246799448492\n",
            "train loss:0.003842686242886057\n",
            "train loss:0.011848143645104453\n",
            "train loss:0.00439791281697894\n",
            "train loss:0.0008363353577647604\n",
            "train loss:0.007983495890247959\n",
            "train loss:0.0015555112250329778\n",
            "train loss:0.0046167001109998634\n",
            "train loss:0.00793156268657042\n",
            "train loss:0.002995574593107566\n",
            "train loss:0.00034180091039114763\n",
            "train loss:0.00660332440450883\n",
            "train loss:0.005600356972033916\n",
            "train loss:0.0030686461635672925\n",
            "train loss:0.005790432836146156\n",
            "train loss:0.0028267936275194194\n",
            "train loss:0.001530414416018743\n",
            "train loss:0.0011278233860922903\n",
            "train loss:0.008465733350869486\n",
            "train loss:0.0026259863337573987\n",
            "train loss:0.0041275155978413745\n",
            "train loss:0.0010568032216273825\n",
            "train loss:0.0017651704418184377\n",
            "train loss:0.0033653612016217977\n",
            "train loss:0.0028485766733976864\n",
            "train loss:0.002857374933698493\n",
            "train loss:0.00398722390860074\n",
            "train loss:0.003612142879131698\n",
            "train loss:0.003291512885642764\n",
            "train loss:0.0031009540292529767\n",
            "train loss:0.002266230082771758\n",
            "train loss:0.0015391007474310533\n",
            "train loss:0.0008874379191567172\n",
            "train loss:0.0011173150453430076\n",
            "train loss:0.003243207149739881\n",
            "train loss:0.08914683236861796\n",
            "train loss:0.00033301133404744116\n",
            "train loss:0.008146349779152682\n",
            "train loss:0.0033923795966013135\n",
            "train loss:0.0010410992464884529\n",
            "train loss:0.005411722429045956\n",
            "train loss:0.00040612221725489926\n",
            "train loss:0.0005896256699754708\n",
            "train loss:0.0044233596384567345\n",
            "train loss:0.00368938509750657\n",
            "train loss:0.0039178179801903105\n",
            "train loss:0.0018715873821525783\n",
            "train loss:0.001958411330364907\n",
            "train loss:0.0014008595064617508\n",
            "train loss:0.0015867316936424253\n",
            "train loss:0.0019543223337534893\n",
            "train loss:0.001959665041364321\n",
            "train loss:0.0005587711764685304\n",
            "train loss:0.012091057374119201\n",
            "train loss:0.00979891488972742\n",
            "train loss:0.004466293732853868\n",
            "train loss:0.0009138941095059829\n",
            "train loss:0.0030556568024710956\n",
            "train loss:0.0077265897798568985\n",
            "train loss:0.0004784608436386391\n",
            "train loss:0.009053921398188268\n",
            "train loss:0.0003723529337318023\n",
            "train loss:0.005711703505136464\n",
            "train loss:0.0009074056280084646\n",
            "train loss:0.0020177248549085643\n",
            "train loss:0.002954112071389104\n",
            "train loss:0.0061501849249076005\n",
            "train loss:0.0013345949978483917\n",
            "train loss:0.0011732198276338333\n",
            "train loss:0.0022453203660390874\n",
            "train loss:0.00866659546887766\n",
            "train loss:0.0024014299284712977\n",
            "train loss:0.003317614230641461\n",
            "train loss:0.00107078782947359\n",
            "train loss:0.0072931624070035886\n",
            "train loss:0.001175750883614829\n",
            "train loss:0.001470476287820633\n",
            "train loss:0.012743630216328919\n",
            "train loss:0.005892601048743918\n",
            "train loss:0.0006956635617151834\n",
            "train loss:0.026516027382788\n",
            "train loss:0.0008949613835217592\n",
            "train loss:0.007221959390640775\n",
            "train loss:0.005491024743582762\n",
            "train loss:0.017252119828000904\n",
            "train loss:0.018103652336465423\n",
            "train loss:0.003159962804485195\n",
            "train loss:0.0024020220874809216\n",
            "train loss:0.001541211970717479\n",
            "train loss:0.006223237844461942\n",
            "train loss:0.005927450464385658\n",
            "train loss:0.00289384310931025\n",
            "train loss:0.016043782083805393\n",
            "train loss:0.0005676695417277411\n",
            "train loss:0.004696987395523273\n",
            "train loss:0.0007438084696487593\n",
            "train loss:0.002236339303080155\n",
            "train loss:0.002142626392707128\n",
            "train loss:0.0010778294598620172\n",
            "train loss:0.005043561691444733\n",
            "train loss:0.0012492963369489859\n",
            "train loss:0.009135320278346558\n",
            "train loss:0.00772603302421768\n",
            "train loss:0.00504968365349701\n",
            "train loss:0.00046778353333823305\n",
            "train loss:0.0006071577068589775\n",
            "train loss:0.0020868095383767674\n",
            "train loss:0.005129101335512049\n",
            "train loss:0.003944213208419219\n",
            "train loss:0.003234976469001839\n",
            "train loss:0.003146422615949734\n",
            "train loss:0.006067897075001823\n",
            "train loss:0.005517117713406658\n",
            "train loss:0.0004660729558871068\n",
            "train loss:0.0014623334361235703\n",
            "train loss:0.00046948375529320914\n",
            "train loss:0.0006456296216342385\n",
            "train loss:0.0032490087125792992\n",
            "train loss:0.002151258978062118\n",
            "train loss:0.0025494419267618317\n",
            "train loss:0.0012183787177144764\n",
            "train loss:0.005567269093834922\n",
            "train loss:0.008576988346682125\n",
            "train loss:0.0033671681370446723\n",
            "train loss:0.0006023484587590014\n",
            "train loss:0.0029492796778800533\n",
            "train loss:0.0040171367684594065\n",
            "train loss:0.008819913105889126\n",
            "train loss:0.00443029280322709\n",
            "train loss:0.002548002853571518\n",
            "train loss:0.002739395499622426\n",
            "train loss:0.045606449784987\n",
            "train loss:0.0013368726876599186\n",
            "train loss:0.00048195630075257954\n",
            "train loss:0.014385450332863854\n",
            "train loss:0.0010608810309584288\n",
            "train loss:0.0023471569935272054\n",
            "train loss:0.003307741251048631\n",
            "train loss:0.0077673763143167925\n",
            "train loss:0.0026809539091634054\n",
            "train loss:0.0013261885900791028\n",
            "train loss:0.0031756386160920825\n",
            "train loss:0.049368454865226195\n",
            "train loss:0.0006787784897453277\n",
            "train loss:0.0005351607534366576\n",
            "train loss:0.002766990232596411\n",
            "train loss:0.00223387016820056\n",
            "train loss:0.0022416611981910545\n",
            "train loss:0.008744991595255373\n",
            "train loss:0.0029616769958374046\n",
            "train loss:0.003987315369758873\n",
            "train loss:0.0017212585982150591\n",
            "train loss:0.0003737539017620724\n",
            "train loss:0.005289408339280559\n",
            "train loss:0.005287912137370299\n",
            "train loss:0.005739595328817314\n",
            "train loss:0.0027052011534835206\n",
            "train loss:0.010113939742319278\n",
            "train loss:0.0026664973756188566\n",
            "train loss:0.0033039583912869794\n",
            "train loss:0.009534494528548675\n",
            "train loss:0.0018898597384286126\n",
            "train loss:0.0015023804746184454\n",
            "train loss:0.002979835200337146\n",
            "train loss:0.01177411786322357\n",
            "train loss:0.0033406434764822563\n",
            "train loss:0.002024357517575791\n",
            "train loss:0.006428749235680219\n",
            "train loss:0.001676799598688636\n",
            "train loss:0.0010804280718526022\n",
            "train loss:0.001088206369747849\n",
            "train loss:0.001359674430286647\n",
            "train loss:0.0015721408467010967\n",
            "train loss:0.0021621447650290665\n",
            "train loss:0.0002860682936437663\n",
            "train loss:0.00042661485445171925\n",
            "train loss:0.00460496926687052\n",
            "train loss:0.003464418782464881\n",
            "train loss:0.002287161890254774\n",
            "train loss:0.0011746577998134494\n",
            "train loss:0.00022497252478179044\n",
            "train loss:0.002965440432565764\n",
            "train loss:0.0016971369889289191\n",
            "train loss:0.0014111148772743129\n",
            "train loss:0.006085477093437138\n",
            "train loss:0.0003030241880643727\n",
            "train loss:0.00246398581707834\n",
            "train loss:0.0037516949035676078\n",
            "train loss:0.0006746359074711431\n",
            "train loss:0.0005020419188949954\n",
            "train loss:0.00011452843221931902\n",
            "train loss:0.005698607161154132\n",
            "train loss:0.0012163527619409023\n",
            "train loss:0.002169261452679179\n",
            "train loss:0.007066016698244384\n",
            "train loss:0.0005761048774652627\n",
            "train loss:0.003568817972606221\n",
            "train loss:0.0060459752437361765\n",
            "train loss:0.0012077226568061388\n",
            "train loss:0.0012663749411204574\n",
            "train loss:0.009717661001694088\n",
            "train loss:0.0022032139778603256\n",
            "train loss:0.00878877759229842\n",
            "train loss:0.0031596390967536464\n",
            "train loss:0.003824136532615562\n",
            "train loss:0.00036377815702872914\n",
            "train loss:0.0031774641110190304\n",
            "train loss:0.001932590159370956\n",
            "train loss:0.0032495776915148033\n",
            "train loss:0.0022754339063365196\n",
            "train loss:0.004003512136502076\n",
            "train loss:0.0029190339627519248\n",
            "train loss:0.004466337731672673\n",
            "train loss:0.01741283219560416\n",
            "train loss:0.00471213784935258\n",
            "train loss:0.0013341049039324844\n",
            "train loss:0.0027492542064016146\n",
            "train loss:0.007149073076010072\n",
            "train loss:0.023293823588898782\n",
            "train loss:0.021331572593559923\n",
            "train loss:0.004181562857542124\n",
            "train loss:0.0061730061002484545\n",
            "train loss:0.0017490118120193543\n",
            "train loss:0.003297287097934012\n",
            "train loss:0.012732174231771838\n",
            "train loss:0.032405301657663796\n",
            "train loss:0.0018287784773577645\n",
            "train loss:0.0028124398628503818\n",
            "train loss:0.0013883774112721863\n",
            "train loss:0.00033255285284480623\n",
            "train loss:0.002549727111404733\n",
            "train loss:0.012885498394227791\n",
            "train loss:0.0028205177755728185\n",
            "train loss:0.0010083571825262798\n",
            "train loss:0.006365979254870786\n",
            "train loss:0.005469768579627498\n",
            "train loss:0.008644917557653363\n",
            "train loss:0.0014197495742706789\n",
            "train loss:0.0037943413233763063\n",
            "train loss:0.0009321612232326569\n",
            "train loss:0.003937349208251916\n",
            "train loss:0.0005604985805229951\n",
            "train loss:0.001392195579592235\n",
            "train loss:0.005340528468967939\n",
            "train loss:0.0005600440530184581\n",
            "train loss:0.0013164352888735418\n",
            "train loss:0.0007412205002846784\n",
            "train loss:0.001296031936927277\n",
            "train loss:0.0007182461779504792\n",
            "train loss:0.006061747982276194\n",
            "train loss:0.0009216378072775602\n",
            "train loss:0.0032658369179250596\n",
            "train loss:0.0010335946888923473\n",
            "train loss:0.00023358158018087208\n",
            "train loss:0.002968199151725159\n",
            "train loss:0.0008321160516270482\n",
            "train loss:0.002157434826471898\n",
            "train loss:0.0015423244920124166\n",
            "train loss:0.002132066383467231\n",
            "train loss:0.04581500471666204\n",
            "train loss:0.001638166008707781\n",
            "train loss:0.001066064288853428\n",
            "train loss:0.0009625639826603447\n",
            "train loss:0.0016394188337075597\n",
            "train loss:0.0002613898527491225\n",
            "train loss:0.001735272260499198\n",
            "train loss:0.0065865305745805795\n",
            "train loss:0.004204387443717923\n",
            "train loss:0.0002900472645837266\n",
            "train loss:0.0008381565850197145\n",
            "train loss:0.0005412222540578417\n",
            "train loss:0.003831859621017669\n",
            "train loss:0.004891655964003038\n",
            "train loss:0.0007891618549858306\n",
            "train loss:0.00031183652619692584\n",
            "train loss:0.0226752718060393\n",
            "train loss:0.002725537870444144\n",
            "train loss:0.009316940564016817\n",
            "train loss:0.004216407062372422\n",
            "train loss:0.001777800409324506\n",
            "train loss:0.021515158744077115\n",
            "train loss:0.000641246513868925\n",
            "train loss:0.001123368517340683\n",
            "train loss:0.0003549263039110951\n",
            "train loss:0.0007277925018836034\n",
            "train loss:0.0016834805075417688\n",
            "train loss:0.003953998755188285\n",
            "train loss:0.0010533657106212928\n",
            "train loss:0.006689321058250961\n",
            "train loss:0.0015649665953608707\n",
            "train loss:0.004006965804320728\n",
            "train loss:0.0006940787862370594\n",
            "train loss:0.0051877830362252846\n",
            "train loss:0.0003929168083707256\n",
            "train loss:0.002966656573836046\n",
            "train loss:0.0021074661567404415\n",
            "train loss:0.0004812325930407999\n",
            "train loss:0.0019075240457863197\n",
            "train loss:0.002348005816427697\n",
            "train loss:0.0017403611248236743\n",
            "train loss:0.0030454840580083254\n",
            "train loss:0.0003405063187829999\n",
            "train loss:0.0031826220473392913\n",
            "train loss:0.0015636559478974462\n",
            "train loss:0.003002184697403438\n",
            "train loss:0.0009749620662503686\n",
            "train loss:0.0016120661503102771\n",
            "train loss:0.00019511505687676628\n",
            "train loss:0.00018197828580041777\n",
            "train loss:0.00102807622405318\n",
            "train loss:0.00014392355451298166\n",
            "train loss:0.002039926252779871\n",
            "train loss:0.0006427947290737195\n",
            "train loss:0.0029488712840627295\n",
            "train loss:0.0030657531981161095\n",
            "train loss:0.02454888544247398\n",
            "train loss:0.003664226155849968\n",
            "train loss:0.0019231277076503454\n",
            "train loss:0.005411873679145206\n",
            "train loss:0.0069482453131572705\n",
            "train loss:0.0013866089919503633\n",
            "train loss:0.00411884459109372\n",
            "train loss:0.001996209659158719\n",
            "train loss:0.003492091228867441\n",
            "train loss:0.0005309005653734608\n",
            "train loss:0.0013956506819785108\n",
            "train loss:0.0007925939416429501\n",
            "train loss:0.008825391601015385\n",
            "train loss:0.002563186855653442\n",
            "train loss:0.035569695864881894\n",
            "train loss:0.002524753502840226\n",
            "train loss:0.0028779816059594805\n",
            "train loss:0.00029264550248373324\n",
            "train loss:0.0011513716790029099\n",
            "train loss:0.0021019514148306693\n",
            "train loss:0.0019557903672545772\n",
            "train loss:0.0030467330212071483\n",
            "train loss:0.0006001007014135855\n",
            "train loss:0.0021058972608147945\n",
            "train loss:0.002083648616256371\n",
            "train loss:0.0012699899500341753\n",
            "train loss:0.0008797924859211849\n",
            "train loss:0.00021759791084075526\n",
            "train loss:0.0017794013936103306\n",
            "train loss:0.0008880221368703268\n",
            "train loss:0.0004747580722900693\n",
            "train loss:0.0019576596996789997\n",
            "train loss:0.021277400315857606\n",
            "train loss:0.0010509181873987305\n",
            "train loss:0.0029405349617179334\n",
            "train loss:0.000566709471944329\n",
            "train loss:0.0012033500453226972\n",
            "train loss:0.001665372405868567\n",
            "train loss:0.0006767331886608392\n",
            "train loss:0.0006547612325973836\n",
            "train loss:0.01191770253114006\n",
            "train loss:0.0023394601643895695\n",
            "train loss:0.0006910427045382487\n",
            "train loss:0.00313937760358745\n",
            "train loss:0.004044419748365802\n",
            "train loss:0.00014241780171261398\n",
            "train loss:0.0005328996727729481\n",
            "train loss:0.002422886957266566\n",
            "train loss:0.0038365252173846055\n",
            "train loss:0.001362144201853945\n",
            "train loss:0.0003651394230908603\n",
            "train loss:0.000782892573501447\n",
            "train loss:0.0014833462957580582\n",
            "train loss:0.001505083220439326\n",
            "train loss:0.0009263739010950587\n",
            "train loss:0.002176919008543171\n",
            "train loss:0.0014406946770695343\n",
            "train loss:0.0010437812810352612\n",
            "train loss:0.011113173606174354\n",
            "train loss:0.0026875061010877706\n",
            "train loss:0.0011337549075013682\n",
            "train loss:0.0009995780361596659\n",
            "train loss:0.0010065636135864293\n",
            "train loss:0.0028895993845994283\n",
            "train loss:0.0014113542992743888\n",
            "train loss:0.007297219943316137\n",
            "train loss:0.001154209605738213\n",
            "train loss:0.0024194610427396234\n",
            "train loss:0.0005842596910855415\n",
            "train loss:0.003002509877273002\n",
            "train loss:0.003991388896497641\n",
            "train loss:0.0027977314415783285\n",
            "train loss:0.0001406266692293474\n",
            "train loss:0.002185870710598947\n",
            "train loss:0.00839361060690187\n",
            "train loss:0.016222120230664046\n",
            "train loss:0.005436237146039446\n",
            "train loss:0.0048548521532840545\n",
            "train loss:0.003893138483158423\n",
            "train loss:0.0012505730846407682\n",
            "train loss:0.002521216753855251\n",
            "train loss:0.0024952577445705396\n",
            "train loss:0.0023665590006251556\n",
            "train loss:0.004415477146534737\n",
            "train loss:0.0005706998163010436\n",
            "train loss:0.00036054197536670876\n",
            "train loss:0.0068854360342844\n",
            "train loss:0.010334058941499755\n",
            "train loss:0.0002991310173491963\n",
            "train loss:0.0015440023223945382\n",
            "train loss:0.0007793654226293642\n",
            "train loss:8.401056716827032e-05\n",
            "train loss:0.005040228648688392\n",
            "train loss:0.001108122759875141\n",
            "train loss:0.0010930853292489683\n",
            "train loss:0.0025553060546484175\n",
            "train loss:0.0032229858277886326\n",
            "train loss:0.0013454894739700239\n",
            "train loss:0.0032302048598289923\n",
            "train loss:0.002191068373348345\n",
            "train loss:0.0012707712453662232\n",
            "train loss:0.0005817299730251772\n",
            "train loss:0.006530902311646042\n",
            "train loss:0.0035164002264610867\n",
            "train loss:0.0009118075982550073\n",
            "train loss:0.0005756786530106193\n",
            "train loss:0.0025445161817074535\n",
            "train loss:0.009685928876889691\n",
            "train loss:0.0036317532594002167\n",
            "train loss:0.007055917041348939\n",
            "train loss:0.003636885916619495\n",
            "train loss:0.0008811627358073805\n",
            "train loss:0.002092196419651011\n",
            "train loss:0.0014706224739707695\n",
            "train loss:0.0034732465539473937\n",
            "train loss:0.007473617156497778\n",
            "train loss:0.0026087002540984322\n",
            "train loss:0.001793237766995808\n",
            "train loss:0.0021683341943690227\n",
            "train loss:0.008566936375028181\n",
            "train loss:0.003837652635712355\n",
            "train loss:0.0009455120341797956\n",
            "train loss:5.0715741562362696e-05\n",
            "train loss:0.003154453704617245\n",
            "train loss:0.00018322317707465015\n",
            "train loss:0.0013056093819729037\n",
            "train loss:0.0005852591783008416\n",
            "train loss:0.001018552984166321\n",
            "train loss:6.775899077271077e-05\n",
            "train loss:0.000105365338868425\n",
            "train loss:0.00021362860983796989\n",
            "train loss:0.0008963548775860588\n",
            "train loss:0.0019588708028517787\n",
            "train loss:0.0009099384786146536\n",
            "train loss:0.0015499515321484858\n",
            "train loss:0.0024014321315150317\n",
            "train loss:0.0008483039583025094\n",
            "train loss:0.0043343595635144255\n",
            "train loss:0.0002582891827620145\n",
            "train loss:0.000627355276649102\n",
            "train loss:0.0007982879196289745\n",
            "train loss:0.0025257020956597183\n",
            "train loss:0.0038924094736949464\n",
            "train loss:0.0006884196963468363\n",
            "train loss:0.000359894504336252\n",
            "train loss:0.0021178986580777455\n",
            "train loss:0.001570609634952395\n",
            "train loss:6.999681809939594e-05\n",
            "train loss:0.0007621338414447492\n",
            "train loss:0.0030436962150531353\n",
            "train loss:0.0011283386375024395\n",
            "train loss:0.00985190949203127\n",
            "train loss:0.00043354814291274643\n",
            "train loss:0.0005595054136629415\n",
            "train loss:0.0003185654436178457\n",
            "train loss:0.004149250795114141\n",
            "train loss:0.0028468970966542967\n",
            "train loss:0.002528892626590494\n",
            "train loss:7.76697051370469e-05\n",
            "train loss:0.018211448469880365\n",
            "train loss:0.0036843580235433044\n",
            "train loss:0.001094571959367481\n",
            "train loss:0.002335961538432897\n",
            "train loss:0.0015477333509101407\n",
            "train loss:0.0016844544246239273\n",
            "train loss:0.0004910309820547327\n",
            "train loss:0.008844836444487629\n",
            "train loss:0.0015873711504133315\n",
            "train loss:0.005097927195190361\n",
            "train loss:0.002186027828778363\n",
            "train loss:0.00273543886755401\n",
            "train loss:0.005179128259967727\n",
            "train loss:0.0001737928552991708\n",
            "train loss:0.0028572525191190136\n",
            "train loss:0.008573705756714838\n",
            "train loss:0.0016224879589810373\n",
            "train loss:0.004245733434342476\n",
            "train loss:0.005264208315313026\n",
            "train loss:0.011024119397273751\n",
            "train loss:0.00011557193099100884\n",
            "train loss:0.0032623659912820947\n",
            "train loss:0.00011561603322573243\n",
            "train loss:0.0011375445337062225\n",
            "train loss:0.0028440504853651516\n",
            "train loss:0.001932855393020774\n",
            "=== epoch:14, train acc:1.0, test acc:0.991 ===\n",
            "train loss:0.0006824014267079211\n",
            "train loss:0.0016713483031516477\n",
            "train loss:0.00335858335526818\n",
            "train loss:0.00379770118975052\n",
            "train loss:0.00022571373588761143\n",
            "train loss:0.0005575162178257508\n",
            "train loss:0.0002716879996049398\n",
            "train loss:0.007858746982024614\n",
            "train loss:0.003578593194985288\n",
            "train loss:0.0007099304428365887\n",
            "train loss:0.0010755478805003647\n",
            "train loss:0.0005753848200877397\n",
            "train loss:0.005963850938596574\n",
            "train loss:0.0013770685652277357\n",
            "train loss:0.002025192032221504\n",
            "train loss:0.0039270361093593\n",
            "train loss:0.020422490323037574\n",
            "train loss:0.010066413002871427\n",
            "train loss:0.0008102260357164904\n",
            "train loss:0.0004060558491306786\n",
            "train loss:0.0035380785710789012\n",
            "train loss:0.02582625245043196\n",
            "train loss:0.00097947506988044\n",
            "train loss:0.0026477180900020257\n",
            "train loss:0.0014273911018299883\n",
            "train loss:0.0017056968317097044\n",
            "train loss:0.017686119136374957\n",
            "train loss:0.0007114042376167223\n",
            "train loss:0.0015852793481275665\n",
            "train loss:0.0820134196558882\n",
            "train loss:0.0013102736806839199\n",
            "train loss:0.004155151566069331\n",
            "train loss:0.006573580485944045\n",
            "train loss:0.017188549075670044\n",
            "train loss:0.0002524280359599341\n",
            "train loss:0.004725320011916634\n",
            "train loss:0.0006607416152210579\n",
            "train loss:0.002135122456533656\n",
            "train loss:0.0015544097813106395\n",
            "train loss:0.0010594625071572723\n",
            "train loss:0.0030684914505542584\n",
            "train loss:0.008233835983076263\n",
            "train loss:0.004061545076056221\n",
            "train loss:0.0017568376273078487\n",
            "train loss:0.0027351148197359283\n",
            "train loss:0.0016806553419544132\n",
            "train loss:0.00048792765129960906\n",
            "train loss:0.00195191015217007\n",
            "train loss:0.0037922585594515647\n",
            "train loss:0.004796079691350381\n",
            "train loss:0.003597415380243683\n",
            "train loss:0.0005991138169520372\n",
            "train loss:0.013487322628910179\n",
            "train loss:0.0020873755098775298\n",
            "train loss:0.002906837146369952\n",
            "train loss:0.004219864367329399\n",
            "train loss:0.001906735450386789\n",
            "train loss:0.019822174509546455\n",
            "train loss:0.001956596317412075\n",
            "train loss:0.007654045758830928\n",
            "train loss:0.003024320493397518\n",
            "train loss:0.0010395508336380067\n",
            "train loss:0.001645525879381418\n",
            "train loss:0.016150004356204683\n",
            "train loss:0.006679622636663163\n",
            "train loss:0.0011703849756163403\n",
            "train loss:0.002716760622086645\n",
            "train loss:0.003940421240473457\n",
            "train loss:0.0036048937926029655\n",
            "train loss:0.010102485880083646\n",
            "train loss:0.00016598830557567934\n",
            "train loss:0.007296510632372708\n",
            "train loss:0.0018921375142608174\n",
            "train loss:0.002934043315244304\n",
            "train loss:0.0018732552919978013\n",
            "train loss:0.005187678156198824\n",
            "train loss:0.00020911357483104138\n",
            "train loss:0.001570229897128622\n",
            "train loss:0.0005424018791742879\n",
            "train loss:0.0014353542836222175\n",
            "train loss:0.010690914048538704\n",
            "train loss:0.0033702921307528703\n",
            "train loss:0.0017970940680972394\n",
            "train loss:0.005245464496323009\n",
            "train loss:0.0020297811390082936\n",
            "train loss:0.0020848805267528335\n",
            "train loss:0.0029450036291215702\n",
            "train loss:0.002277465972413775\n",
            "train loss:0.001795524133697602\n",
            "train loss:0.0009382719319369766\n",
            "train loss:0.0006020703377471926\n",
            "train loss:0.009720135191493555\n",
            "train loss:0.0026698136495186247\n",
            "train loss:0.004694839930382573\n",
            "train loss:0.0010372295736562382\n",
            "train loss:0.0013197029151139849\n",
            "train loss:0.0001255120565485519\n",
            "train loss:0.0037174194979536427\n",
            "train loss:0.0023630150148708177\n",
            "train loss:0.008544630216747512\n",
            "train loss:0.0014963944645491447\n",
            "train loss:0.0003341959340556832\n",
            "train loss:0.0023394947261196784\n",
            "train loss:0.0005182414416722067\n",
            "train loss:0.0011737796242411872\n",
            "train loss:0.003864392562006892\n",
            "train loss:0.07336133920951576\n",
            "train loss:0.004071074656217898\n",
            "train loss:0.027693395106490697\n",
            "train loss:0.003972687084265647\n",
            "train loss:0.004620471488965373\n",
            "train loss:0.0056652505244955\n",
            "train loss:0.0033169224618719035\n",
            "train loss:0.00190576292762346\n",
            "train loss:0.0012665217818794683\n",
            "train loss:0.0003606036070148157\n",
            "train loss:0.0034373499680119633\n",
            "train loss:0.0034827325313658347\n",
            "train loss:0.00269485625287184\n",
            "train loss:0.0011575760191894506\n",
            "train loss:0.0016423242845016934\n",
            "train loss:0.001368075723438364\n",
            "train loss:0.000985056262253659\n",
            "train loss:0.00024175915331090824\n",
            "train loss:0.0010386650443671732\n",
            "train loss:0.001711483878839032\n",
            "train loss:0.0032936084533308403\n",
            "train loss:0.0012329603017462094\n",
            "train loss:0.0013920016337714714\n",
            "train loss:0.0011742517501603141\n",
            "train loss:0.0016593839286280145\n",
            "train loss:0.0007989980090102074\n",
            "train loss:0.00851082991263733\n",
            "train loss:0.004935591453609416\n",
            "train loss:0.0021933459612515733\n",
            "train loss:0.0016497720591920207\n",
            "train loss:0.0012018333134058805\n",
            "train loss:0.005130505188022767\n",
            "train loss:0.0071449129699154915\n",
            "train loss:0.0042276784058032585\n",
            "train loss:0.0034183641942480848\n",
            "train loss:0.04894753997645604\n",
            "train loss:0.010780357212917592\n",
            "train loss:0.010250862041693827\n",
            "train loss:0.0014359227799486994\n",
            "train loss:0.0037776508745096925\n",
            "train loss:0.007409864014932004\n",
            "train loss:0.001328780009220059\n",
            "train loss:0.00036788946042724\n",
            "train loss:0.0030606685011616884\n",
            "train loss:0.002216875964113261\n",
            "train loss:0.0010968140032186526\n",
            "train loss:0.00043346478906028136\n",
            "train loss:0.0014846629541362768\n",
            "train loss:0.00778557871039703\n",
            "train loss:0.009192806626975737\n",
            "train loss:0.0010864045729711272\n",
            "train loss:0.0003134863700756579\n",
            "train loss:0.014447007667151523\n",
            "train loss:0.0005706442615108253\n",
            "train loss:0.0038281901167843296\n",
            "train loss:0.00026432689455035166\n",
            "train loss:0.00024142728569273803\n",
            "train loss:0.0013821862276809926\n",
            "train loss:0.001656879273871359\n",
            "train loss:0.0007297423009884048\n",
            "train loss:0.0051445486575506065\n",
            "train loss:0.003559527372694873\n",
            "train loss:0.0005761920726834554\n",
            "train loss:0.00021494906973962687\n",
            "train loss:0.0009319603298139921\n",
            "train loss:0.005917295330698713\n",
            "train loss:0.0031251497632441916\n",
            "train loss:0.00033681646154306156\n",
            "train loss:0.005000496380027264\n",
            "train loss:0.0017330586626582325\n",
            "train loss:0.0039760367580513635\n",
            "train loss:0.0009896681126664749\n",
            "train loss:0.0004685137043846325\n",
            "train loss:0.000943020807581514\n",
            "train loss:0.008289690834047498\n",
            "train loss:0.0012063011993963093\n",
            "train loss:0.0006909509610099336\n",
            "train loss:0.00167238721537318\n",
            "train loss:0.00426875010301867\n",
            "train loss:0.0019036046949613096\n",
            "train loss:0.017297667420675525\n",
            "train loss:0.002987708012847324\n",
            "train loss:0.013106134927398953\n",
            "train loss:0.0021302631084650116\n",
            "train loss:0.0014705783147464801\n",
            "train loss:0.019389401475297833\n",
            "train loss:0.002652404959737148\n",
            "train loss:0.01161269585759658\n",
            "train loss:0.005061552432675284\n",
            "train loss:0.0006214874932206048\n",
            "train loss:0.002339569353747957\n",
            "train loss:0.0006589344829574329\n",
            "train loss:0.0010643602793859667\n",
            "train loss:0.010698724896984688\n",
            "train loss:0.0023185021251705073\n",
            "train loss:0.0032766166661084826\n",
            "train loss:0.0015882220454360816\n",
            "train loss:0.00022541952507250368\n",
            "train loss:0.000878840084295773\n",
            "train loss:0.00147179431728616\n",
            "train loss:0.014087037920750459\n",
            "train loss:0.0012106285403887418\n",
            "train loss:0.006062226459186249\n",
            "train loss:0.0015454472558971187\n",
            "train loss:0.001962440408247575\n",
            "train loss:0.0011065634505455372\n",
            "train loss:0.0026534727181936075\n",
            "train loss:0.00018272054934580933\n",
            "train loss:0.000898159434551023\n",
            "train loss:0.0014593362638175885\n",
            "train loss:0.003599851444662324\n",
            "train loss:0.0001152516107519953\n",
            "train loss:0.001263420434674651\n",
            "train loss:0.003629992073350616\n",
            "train loss:0.0008847970125257391\n",
            "train loss:0.0022241490481569757\n",
            "train loss:0.003718791321265081\n",
            "train loss:0.0007070245864375196\n",
            "train loss:0.012597152144439891\n",
            "train loss:0.0008692373490164078\n",
            "train loss:0.019343482022573674\n",
            "train loss:0.0014554240334851646\n",
            "train loss:0.0028777549749570415\n",
            "train loss:0.000893690805444686\n",
            "train loss:0.00147934071045545\n",
            "train loss:0.0037851009675293834\n",
            "train loss:0.0008369522027171392\n",
            "train loss:0.0030137129078172714\n",
            "train loss:0.0003897598514679039\n",
            "train loss:0.002924760159790049\n",
            "train loss:0.0017201819675263935\n",
            "train loss:0.0005239858525536324\n",
            "train loss:0.014031032074666192\n",
            "train loss:0.0008449061910774321\n",
            "train loss:0.0010786285513646621\n",
            "train loss:0.0008979969636603323\n",
            "train loss:0.005182736624591985\n",
            "train loss:0.002507907001534391\n",
            "train loss:0.004948225989969055\n",
            "train loss:0.008802313121571767\n",
            "train loss:0.0016381999674395147\n",
            "train loss:0.006554960757438987\n",
            "train loss:0.0006157703107518414\n",
            "train loss:0.0007596485116287911\n",
            "train loss:0.005947506450355705\n",
            "train loss:0.0007517809590728523\n",
            "train loss:0.006505759649166598\n",
            "train loss:0.006368885845374669\n",
            "train loss:0.0021236087607590494\n",
            "train loss:0.0019204842868750212\n",
            "train loss:0.008847907204951346\n",
            "train loss:0.006821640878626234\n",
            "train loss:0.0002940010667861769\n",
            "train loss:0.000668655609843136\n",
            "train loss:0.013981977117627881\n",
            "train loss:0.01185892621003497\n",
            "train loss:0.00022728323719648476\n",
            "train loss:0.0006547430267548168\n",
            "train loss:0.0015938550188444077\n",
            "train loss:0.00448011970981561\n",
            "train loss:0.02404603751585944\n",
            "train loss:0.005374852190540304\n",
            "train loss:0.0002708366151914213\n",
            "train loss:0.002043248914380778\n",
            "train loss:0.0017133473391235078\n",
            "train loss:0.006072223993752077\n",
            "train loss:0.00336473102140144\n",
            "train loss:0.0030550112548733462\n",
            "train loss:0.0012912763448357442\n",
            "train loss:0.003484831361001385\n",
            "train loss:0.0014537539583112532\n",
            "train loss:0.0014861979132595844\n",
            "train loss:0.0014964553873942812\n",
            "train loss:0.06412659478448383\n",
            "train loss:0.0017096495957982251\n",
            "train loss:0.0015985052683100956\n",
            "train loss:0.0005145693576080076\n",
            "train loss:0.0012671529470947346\n",
            "train loss:0.0029039752314075134\n",
            "train loss:0.0029215920932577173\n",
            "train loss:0.0008740310380523383\n",
            "train loss:0.00411681871000396\n",
            "train loss:0.008594012074530701\n",
            "train loss:0.00031880351554904247\n",
            "train loss:0.005328932103927143\n",
            "train loss:0.0011278306782578087\n",
            "train loss:0.0007101841469930047\n",
            "train loss:0.000257920864775625\n",
            "train loss:0.0016796683245986984\n",
            "train loss:0.0018803535832891897\n",
            "train loss:0.0011620557961057917\n",
            "train loss:0.0007680217041143012\n",
            "train loss:0.0014002491067993035\n",
            "train loss:0.00042214379375655304\n",
            "train loss:0.004572382615853907\n",
            "train loss:0.0008039488768897998\n",
            "train loss:0.00038350088212727644\n",
            "train loss:0.000244828184026271\n",
            "train loss:0.0019521251235269472\n",
            "train loss:0.001256431234173807\n",
            "train loss:0.011277688007610497\n",
            "train loss:0.0015881054620290325\n",
            "train loss:0.0022004926418811663\n",
            "train loss:0.002604078688586394\n",
            "train loss:0.0023504821570645147\n",
            "train loss:0.00102162239719339\n",
            "train loss:0.004893745041344868\n",
            "train loss:0.0007675179043423962\n",
            "train loss:0.0005779087262081532\n",
            "train loss:0.0027794018150834143\n",
            "train loss:0.0015322109371750032\n",
            "train loss:0.0008452528292071861\n",
            "train loss:0.0015808580083491932\n",
            "train loss:0.00021903133641266615\n",
            "train loss:0.001033749549635447\n",
            "train loss:0.0007485853340001728\n",
            "train loss:0.0011031627518964659\n",
            "train loss:0.0010984259947339424\n",
            "train loss:0.0002756954904318581\n",
            "train loss:0.0034520030247916296\n",
            "train loss:0.0032414196414451723\n",
            "train loss:0.000426078807634332\n",
            "train loss:0.0014955220451269894\n",
            "train loss:0.0024428326172544032\n",
            "train loss:0.0002984688966691156\n",
            "train loss:0.003687596553138715\n",
            "train loss:0.0731278930705473\n",
            "train loss:0.0028199381159669406\n",
            "train loss:0.00038826203510477315\n",
            "train loss:0.0016614268258383152\n",
            "train loss:0.0029126100807286477\n",
            "train loss:0.011088368723191987\n",
            "train loss:0.002187765499801777\n",
            "train loss:0.01853586969829293\n",
            "train loss:0.0002643079247121325\n",
            "train loss:0.002496747115385399\n",
            "train loss:0.000366409971615262\n",
            "train loss:0.0026869907169592556\n",
            "train loss:0.0016012695508270918\n",
            "train loss:0.005326146502644407\n",
            "train loss:0.001366380230942841\n",
            "train loss:0.006144713530831288\n",
            "train loss:0.004034333562276022\n",
            "train loss:0.0002187685363120564\n",
            "train loss:0.00047328235809124443\n",
            "train loss:0.0023461391241097764\n",
            "train loss:0.000582737602262109\n",
            "train loss:0.0004570963430898259\n",
            "train loss:0.000282254621245279\n",
            "train loss:0.001018431877449656\n",
            "train loss:0.0005088094329728799\n",
            "train loss:0.04990975229688903\n",
            "train loss:0.0016588928539417994\n",
            "train loss:0.0016518413970603113\n",
            "train loss:9.864784219482113e-05\n",
            "train loss:0.0027808863170299524\n",
            "train loss:0.002474787752330447\n",
            "train loss:0.009987482521517137\n",
            "train loss:0.0014358101162941239\n",
            "train loss:0.003852753948521326\n",
            "train loss:0.0003859846960073182\n",
            "train loss:0.005952304663947566\n",
            "train loss:0.0005599214216255212\n",
            "train loss:0.00031909685185784066\n",
            "train loss:0.0006920918628323514\n",
            "train loss:0.00048248556379253967\n",
            "train loss:0.011783841057763486\n",
            "train loss:0.0019335140591092454\n",
            "train loss:0.008880731463335885\n",
            "train loss:0.00046202076908812917\n",
            "train loss:0.001004192399189046\n",
            "train loss:0.002991086401409448\n",
            "train loss:0.003678743518286361\n",
            "train loss:0.008042914177564175\n",
            "train loss:0.00025308706357614607\n",
            "train loss:0.0008563747131162923\n",
            "train loss:0.0019036484361773945\n",
            "train loss:0.00043713914303610837\n",
            "train loss:0.004499607176233175\n",
            "train loss:0.00473337487615312\n",
            "train loss:0.001160399782419065\n",
            "train loss:0.004679837282952272\n",
            "train loss:0.011190064364191567\n",
            "train loss:0.00023043298441377189\n",
            "train loss:0.0029958933293870393\n",
            "train loss:0.01253696286517154\n",
            "train loss:0.002849101023000071\n",
            "train loss:0.009480920425516693\n",
            "train loss:0.004510531034831897\n",
            "train loss:0.0023351528641227847\n",
            "train loss:0.0017855045346249505\n",
            "train loss:0.0028446370952496867\n",
            "train loss:0.0023229693735497214\n",
            "train loss:0.006141546888522551\n",
            "train loss:0.01157116573114617\n",
            "train loss:0.002287963544808799\n",
            "train loss:0.008087975644714952\n",
            "train loss:0.04637753411736755\n",
            "train loss:0.0022069695847275806\n",
            "train loss:0.002407788888529545\n",
            "train loss:0.0118700526506239\n",
            "train loss:0.0011615677096630931\n",
            "train loss:0.0019889273139921367\n",
            "train loss:0.0006962484853639722\n",
            "train loss:0.005932613247065039\n",
            "train loss:0.0012359675424558042\n",
            "train loss:0.0046496230371363325\n",
            "train loss:0.0048527359945267565\n",
            "train loss:0.0009884237489462839\n",
            "train loss:0.005228020290530033\n",
            "train loss:0.005058008321711468\n",
            "train loss:0.0027962532063801337\n",
            "train loss:0.0018169702359776027\n",
            "train loss:0.004094712661963096\n",
            "train loss:0.00877545221012929\n",
            "train loss:0.0012639539224204729\n",
            "train loss:0.003173495789777545\n",
            "train loss:0.0023489917355685543\n",
            "train loss:0.0022405974230579636\n",
            "train loss:0.006727274390043424\n",
            "train loss:0.007812255066473405\n",
            "train loss:0.0001600268941980601\n",
            "train loss:0.006952819693505162\n",
            "train loss:0.0014589276953842498\n",
            "train loss:0.0003301814755704916\n",
            "train loss:0.0006138959803524631\n",
            "train loss:0.0008393033463228033\n",
            "train loss:0.0023195183667605317\n",
            "train loss:0.0027164156910601954\n",
            "train loss:0.0006249745228497\n",
            "train loss:0.01294435974569032\n",
            "train loss:0.0007985158025099918\n",
            "train loss:0.0006368288788958441\n",
            "train loss:0.006524433327957102\n",
            "train loss:0.004946656708318787\n",
            "train loss:0.008528191880345037\n",
            "train loss:0.001547020064548434\n",
            "train loss:0.0002042382500155719\n",
            "train loss:0.00439614691362085\n",
            "train loss:0.0006819173309360029\n",
            "train loss:0.00339326340030848\n",
            "train loss:0.003341902493183002\n",
            "train loss:0.0012880151130933435\n",
            "train loss:0.003826599485012593\n",
            "train loss:0.02190957164395059\n",
            "train loss:0.006426330393430258\n",
            "train loss:0.00014375788024888293\n",
            "train loss:0.00199521548172227\n",
            "train loss:0.0020404919873162224\n",
            "train loss:0.025632804083178605\n",
            "train loss:0.012915170177753723\n",
            "train loss:0.00035185661090225034\n",
            "train loss:0.0007438460371518318\n",
            "train loss:0.002244749447110718\n",
            "train loss:0.0467041589590743\n",
            "train loss:0.00021965352865793673\n",
            "train loss:0.0008841666515456265\n",
            "train loss:0.003600045037050768\n",
            "train loss:0.0028427899247212745\n",
            "train loss:0.005929586771567681\n",
            "train loss:0.0016401553294084\n",
            "train loss:0.0022193226692368588\n",
            "train loss:0.0019216491947719775\n",
            "train loss:0.00802130325679357\n",
            "train loss:0.0007470242120120189\n",
            "train loss:0.0001752803720987966\n",
            "train loss:0.011688349230876733\n",
            "train loss:0.0033556883119828275\n",
            "train loss:0.00038177187366511615\n",
            "train loss:0.0012064663589118297\n",
            "train loss:0.0012418565826096332\n",
            "train loss:0.002607367520823104\n",
            "train loss:0.0013677530110749422\n",
            "train loss:0.0009812547313499261\n",
            "train loss:0.00044008712173989044\n",
            "train loss:0.0036143876020488935\n",
            "train loss:0.008089778065097288\n",
            "train loss:0.01300446928849451\n",
            "train loss:0.0005358000032293033\n",
            "train loss:0.03429068531443833\n",
            "train loss:0.0011688199066700123\n",
            "train loss:0.004098549028445653\n",
            "train loss:0.002171755116601543\n",
            "train loss:0.0019173031403650384\n",
            "train loss:0.0038423885689317593\n",
            "train loss:0.01735755042877764\n",
            "train loss:0.0014854647709319762\n",
            "train loss:0.003471069197918062\n",
            "train loss:0.0034126521397783364\n",
            "train loss:0.006320976330350593\n",
            "train loss:0.0002804140727141842\n",
            "train loss:0.00494222323456061\n",
            "train loss:0.044740138870159436\n",
            "train loss:0.0005654912412524627\n",
            "train loss:0.007125398247680024\n",
            "train loss:0.012646210808521186\n",
            "train loss:0.00497854465707585\n",
            "train loss:0.0005431114360962817\n",
            "train loss:0.0029366442020450083\n",
            "train loss:0.0005431074105094919\n",
            "train loss:0.007107609047898989\n",
            "train loss:0.004150652291253363\n",
            "train loss:0.014638526606031906\n",
            "train loss:0.013313861192841224\n",
            "train loss:0.0071639933727356865\n",
            "train loss:0.0006538910189863446\n",
            "train loss:0.0011040581101038646\n",
            "train loss:0.001650462561529276\n",
            "train loss:0.017692463716968067\n",
            "train loss:0.008589974560548662\n",
            "train loss:0.005176327908995866\n",
            "train loss:0.0018425955696264188\n",
            "train loss:0.0003437299335612338\n",
            "train loss:0.0004249763143245263\n",
            "train loss:0.002473926260545377\n",
            "train loss:0.003463918305708088\n",
            "train loss:0.0255818821963303\n",
            "train loss:0.0002669592015202714\n",
            "train loss:0.0021562020226679806\n",
            "train loss:0.0028678051531554226\n",
            "train loss:0.00541856227843656\n",
            "train loss:0.002105824875684095\n",
            "train loss:0.0012256673441503692\n",
            "train loss:0.0004820960449173408\n",
            "train loss:0.002790181704118217\n",
            "train loss:0.009356452551248711\n",
            "train loss:0.005148768635887788\n",
            "train loss:0.01966255461463151\n",
            "train loss:0.0044912416189503476\n",
            "train loss:0.0007230373249595738\n",
            "train loss:0.000733915254626341\n",
            "train loss:0.0009340775832011125\n",
            "train loss:0.001003096577192458\n",
            "train loss:0.001845536454584679\n",
            "train loss:0.00018286245711289062\n",
            "train loss:0.001149256212364157\n",
            "train loss:0.000486484106147105\n",
            "train loss:0.006402010449365281\n",
            "train loss:0.0005179434168625234\n",
            "train loss:0.0029698314780094363\n",
            "train loss:0.014221390596026224\n",
            "train loss:0.002722936489921819\n",
            "train loss:0.00034883373326558394\n",
            "train loss:0.010336024553980774\n",
            "train loss:0.002879300025240519\n",
            "train loss:0.0010257601363823675\n",
            "train loss:0.0013228454445783128\n",
            "train loss:0.02734538356414169\n",
            "train loss:0.00015313895359512554\n",
            "train loss:0.0019485950488563953\n",
            "train loss:0.03854812716790729\n",
            "train loss:0.003343798512557957\n",
            "train loss:0.00068636890615316\n",
            "train loss:0.001122000483320142\n",
            "train loss:0.0008319009681479016\n",
            "train loss:0.0022390403996661408\n",
            "train loss:0.0015867100774323894\n",
            "train loss:0.009367683980321215\n",
            "train loss:0.0023195289851803908\n",
            "train loss:0.012286249487108934\n",
            "train loss:0.003279811383739072\n",
            "train loss:0.0012823680133151707\n",
            "train loss:0.0031496778318335717\n",
            "train loss:0.0039859887593418\n",
            "train loss:0.0014627168925908523\n",
            "train loss:0.0034923053653043794\n",
            "train loss:0.011023696019367209\n",
            "train loss:0.017073893296586793\n",
            "train loss:0.004813845108099081\n",
            "train loss:0.016115673697901317\n",
            "train loss:0.0025025272980034193\n",
            "train loss:0.0007236197988967542\n",
            "train loss:0.003200466165745885\n",
            "train loss:0.0068174994322213875\n",
            "train loss:0.00037999995834632217\n",
            "train loss:0.023086089321705364\n",
            "train loss:0.0005296442282401052\n",
            "train loss:0.00377081076113631\n",
            "train loss:0.00036302508929163133\n",
            "train loss:0.017374536491322628\n",
            "train loss:0.0005525976771304991\n",
            "train loss:0.0008763868652223148\n",
            "train loss:0.0037466096415724446\n",
            "train loss:0.008058397726850791\n",
            "train loss:0.00983636394393934\n",
            "train loss:0.01196001122205454\n",
            "train loss:0.00032862434172192275\n",
            "train loss:0.0024113617223162783\n",
            "train loss:0.0008571372337339746\n",
            "train loss:0.007669311734922845\n",
            "train loss:0.010080627464165803\n",
            "train loss:0.0023860036106950365\n",
            "train loss:0.003986089629262899\n",
            "train loss:0.0018610848047688308\n",
            "=== epoch:15, train acc:0.996, test acc:0.988 ===\n",
            "train loss:0.005712200013022084\n",
            "train loss:0.0007752445388908495\n",
            "train loss:0.0005775165843383603\n",
            "train loss:0.004477970419669321\n",
            "train loss:0.009092977016261862\n",
            "train loss:0.0011900141739302253\n",
            "train loss:0.002941496010324665\n",
            "train loss:0.0003052278488952214\n",
            "train loss:0.00031495162742078547\n",
            "train loss:0.001605042956596084\n",
            "train loss:0.0010608562135159978\n",
            "train loss:0.006097854908682365\n",
            "train loss:0.003314616213671136\n",
            "train loss:0.001579991003939178\n",
            "train loss:0.0017767124430235143\n",
            "train loss:0.001764571443477787\n",
            "train loss:0.003312659027286611\n",
            "train loss:0.001072667051842561\n",
            "train loss:0.006703059172129352\n",
            "train loss:0.0031977287935693794\n",
            "train loss:0.0002824355022237217\n",
            "train loss:0.0027906600360871284\n",
            "train loss:0.0010990005514021582\n",
            "train loss:0.0019331005699700782\n",
            "train loss:0.000762677797638062\n",
            "train loss:0.0005079924582310772\n",
            "train loss:0.010604166448294284\n",
            "train loss:0.011530325597948567\n",
            "train loss:0.03673942158333662\n",
            "train loss:0.019054522418053395\n",
            "train loss:0.003051674608446237\n",
            "train loss:0.0039388343173327334\n",
            "train loss:0.002697491115485302\n",
            "train loss:0.0003552020362612561\n",
            "train loss:0.006111553265223933\n",
            "train loss:0.0016056728500913066\n",
            "train loss:0.0022534683414251314\n",
            "train loss:0.0017820172447737702\n",
            "train loss:0.0033924759653100158\n",
            "train loss:0.0009043819644036093\n",
            "train loss:0.012070841502558868\n",
            "train loss:0.004302516230714951\n",
            "train loss:0.004852400573332761\n",
            "train loss:0.01651923313971113\n",
            "train loss:0.0034379212940183392\n",
            "train loss:0.003419480147133466\n",
            "train loss:0.004508768712003632\n",
            "train loss:0.015209551191128656\n",
            "train loss:0.00221654071117935\n",
            "train loss:0.0020281112545074793\n",
            "train loss:0.003635722072330182\n",
            "train loss:0.0003236105186460973\n",
            "train loss:0.0012248468966270692\n",
            "train loss:0.0047135343840725595\n",
            "train loss:0.004357143329541248\n",
            "train loss:0.0037022107550481855\n",
            "train loss:0.006844348700820796\n",
            "train loss:0.0021834125356328545\n",
            "train loss:0.00012303427170780453\n",
            "train loss:0.005379506876433598\n",
            "train loss:0.012455473750388463\n",
            "train loss:0.08752376898069132\n",
            "train loss:0.0009572797180206318\n",
            "train loss:0.0005078855261081248\n",
            "train loss:0.0027324838991401195\n",
            "train loss:0.0012126066266765231\n",
            "train loss:0.0013207354899117693\n",
            "train loss:0.01018965134707805\n",
            "train loss:0.04322167577942513\n",
            "train loss:0.009429769017934625\n",
            "train loss:0.002647887010514547\n",
            "train loss:0.0020081956804883483\n",
            "train loss:0.0026357082573350366\n",
            "train loss:0.002245717711288429\n",
            "train loss:0.0023589503056264476\n",
            "train loss:0.0027767537590987473\n",
            "train loss:0.012427868218527656\n",
            "train loss:0.015104272146297863\n",
            "train loss:0.004521678341752761\n",
            "train loss:0.01474542329403743\n",
            "train loss:0.001719416306689637\n",
            "train loss:0.033544630404460235\n",
            "train loss:0.07611989660746984\n",
            "train loss:0.012013419077445173\n",
            "train loss:0.0008913997571852309\n",
            "train loss:0.006599269670980471\n",
            "train loss:0.003525072703967248\n",
            "train loss:0.0088709816138108\n",
            "train loss:0.001580742351242756\n",
            "train loss:0.0035494788938148477\n",
            "train loss:0.0007410981029001286\n",
            "train loss:0.013671553610323926\n",
            "train loss:0.004690755948523152\n",
            "train loss:0.002376208407014201\n",
            "train loss:0.010438467120614142\n",
            "train loss:0.044746638548834615\n",
            "train loss:0.00048498489228346444\n",
            "train loss:0.0740953369484542\n",
            "train loss:0.00173706782251598\n",
            "train loss:0.00618723516796192\n",
            "train loss:0.00033043126720670805\n",
            "train loss:0.012407245771970322\n",
            "train loss:0.000435949690440396\n",
            "train loss:0.007712165556563348\n",
            "train loss:0.038646870985389825\n",
            "train loss:0.002191593476472126\n",
            "train loss:0.005285060813662108\n",
            "train loss:0.00015948171489717187\n",
            "train loss:0.0005919234775702648\n",
            "train loss:0.002302487014363478\n",
            "train loss:0.007049834851224999\n",
            "train loss:0.0017338561500895647\n",
            "train loss:0.009527327813133897\n",
            "train loss:0.0017508764057114657\n",
            "train loss:0.0019065774346079195\n",
            "train loss:0.005724479350271212\n",
            "train loss:0.011664804763893978\n",
            "train loss:0.0018045674504556142\n",
            "train loss:0.0039448464500287\n",
            "train loss:0.004541306122684849\n",
            "train loss:0.008204012639310459\n",
            "train loss:0.008264636610521153\n",
            "train loss:0.004256005921490959\n",
            "train loss:0.0008966708120453363\n",
            "train loss:0.02926090490509435\n",
            "train loss:0.00958829954349042\n",
            "train loss:0.0004744751036695238\n",
            "train loss:0.004310913088607692\n",
            "train loss:0.01220310297629307\n",
            "train loss:0.0015290888714857604\n",
            "train loss:0.0024640703926642935\n",
            "train loss:0.0026621981170290563\n",
            "train loss:0.0009671311131523362\n",
            "train loss:0.009828199622420066\n",
            "train loss:0.002073813620954239\n",
            "train loss:0.0033717384772990167\n",
            "train loss:0.012184030307692457\n",
            "train loss:0.0003035207517634026\n",
            "train loss:0.00033115304993432354\n",
            "train loss:0.001224023147779584\n",
            "train loss:0.004870941931000708\n",
            "train loss:0.01922571356562196\n",
            "train loss:0.006982184747778127\n",
            "train loss:0.00027320718104751176\n",
            "train loss:0.002040671290825391\n",
            "train loss:0.053469358712186124\n",
            "train loss:0.0011356035376344235\n",
            "train loss:0.001499348813863714\n",
            "train loss:0.0004234264660032462\n",
            "train loss:0.0030494642173915293\n",
            "train loss:0.002275821446165094\n",
            "train loss:0.0051550922522287605\n",
            "train loss:0.021279651613478966\n",
            "train loss:0.008312900931053245\n",
            "train loss:0.005677099791810508\n",
            "train loss:0.0003958578678249589\n",
            "train loss:0.0005512645109369184\n",
            "train loss:0.003702643873719752\n",
            "train loss:0.0003793198722804776\n",
            "train loss:0.00040803623562201644\n",
            "train loss:0.007523537184329975\n",
            "train loss:0.0018949157571637723\n",
            "train loss:0.0015474298909204168\n",
            "train loss:0.004068713314216986\n",
            "train loss:0.0010822421520559813\n",
            "train loss:0.0015475755204881945\n",
            "train loss:0.006191185457255036\n",
            "train loss:0.0004542586238056511\n",
            "train loss:0.0022318119423814773\n",
            "train loss:0.009179548981431682\n",
            "train loss:0.0030897305377168777\n",
            "train loss:0.007843663893781168\n",
            "train loss:0.0013281302401752359\n",
            "train loss:0.008364917650968797\n",
            "train loss:0.0016309249454363732\n",
            "train loss:0.0029459510989767925\n",
            "train loss:0.002266981303187731\n",
            "train loss:0.014372386790542509\n",
            "train loss:0.0015640124381071058\n",
            "train loss:0.0023295860281902416\n",
            "train loss:0.003842104235788071\n",
            "train loss:0.0028839682876975253\n",
            "train loss:0.0005648365213174024\n",
            "train loss:0.0005548884267235932\n",
            "train loss:0.010405550927017268\n",
            "train loss:0.004463958215131247\n",
            "train loss:0.003089365348062657\n",
            "train loss:0.0017276862965823598\n",
            "train loss:0.0030144921916816646\n",
            "train loss:0.003844590040886725\n",
            "train loss:0.0007257364721612147\n",
            "train loss:0.0008719897712977878\n",
            "train loss:0.00025832118868551\n",
            "train loss:0.007603907302452195\n",
            "train loss:0.003183426015162946\n",
            "train loss:0.00021291025929837934\n",
            "train loss:0.0009556001995599533\n",
            "train loss:0.0005722797272036012\n",
            "train loss:0.005371151968537105\n",
            "train loss:0.0003744218757438507\n",
            "train loss:0.00692462494390984\n",
            "train loss:0.0007612841143976562\n",
            "train loss:0.002945992202813264\n",
            "train loss:0.013291932914139912\n",
            "train loss:0.005988328815110436\n",
            "train loss:0.00020497632603250844\n",
            "train loss:0.012576710737366397\n",
            "train loss:0.00028384989336119115\n",
            "train loss:0.0008135993363631899\n",
            "train loss:0.0007625962929589115\n",
            "train loss:0.0018722140152014216\n",
            "train loss:0.0030857846105232347\n",
            "train loss:0.005373569191467281\n",
            "train loss:0.007522623386929608\n",
            "train loss:0.0002986671233998908\n",
            "train loss:0.009334437285217986\n",
            "train loss:7.064107505787877e-05\n",
            "train loss:0.0003544847979711979\n",
            "train loss:0.006553135484342515\n",
            "train loss:0.005930184506538482\n",
            "train loss:0.0006143027042108265\n",
            "train loss:0.0010687054941301182\n",
            "train loss:0.002984297591126621\n",
            "train loss:0.0036483987353534836\n",
            "train loss:0.01016289246075511\n",
            "train loss:0.00025369798480813185\n",
            "train loss:0.00029916230248360333\n",
            "train loss:0.004448093418575426\n",
            "train loss:0.002147835237508517\n",
            "train loss:8.494360840553357e-05\n",
            "train loss:0.0015804804910813971\n",
            "train loss:0.0031001504898922534\n",
            "train loss:0.002702374293612307\n",
            "train loss:0.002792981320303333\n",
            "train loss:0.00022042790297784366\n",
            "train loss:0.0038784343173093707\n",
            "train loss:0.0011872603677996192\n",
            "train loss:0.000587397342823388\n",
            "train loss:0.0009100532610573772\n",
            "train loss:0.0016573703157378248\n",
            "train loss:0.0007144621901513932\n",
            "train loss:0.0005543883417494076\n",
            "train loss:0.016517323337626785\n",
            "train loss:0.006305043437023522\n",
            "train loss:0.0035748422423991497\n",
            "train loss:0.0002692945251759347\n",
            "train loss:0.0005949178232382028\n",
            "train loss:0.0026476336405561914\n",
            "train loss:0.003061888154037627\n",
            "train loss:0.0021423582465010236\n",
            "train loss:0.006187683409191521\n",
            "train loss:0.002533700275257491\n",
            "train loss:0.0006055163553578359\n",
            "train loss:0.0033986454214051053\n",
            "train loss:0.0021843366342473498\n",
            "train loss:0.0017218550963091581\n",
            "train loss:0.0007698041051132651\n",
            "train loss:0.0014263646988063533\n",
            "train loss:0.0008822866926264493\n",
            "train loss:0.001897115957122212\n",
            "train loss:0.001318937907215795\n",
            "train loss:0.009817434468259008\n",
            "train loss:0.0008722674174202847\n",
            "train loss:0.002654413225650826\n",
            "train loss:0.014243890772545701\n",
            "train loss:0.0013788159155106977\n",
            "train loss:0.0034019533337563555\n",
            "train loss:0.0004561233249334754\n",
            "train loss:0.0032597629966245423\n",
            "train loss:0.000908253900128305\n",
            "train loss:0.0010345883461237767\n",
            "train loss:0.00029146814015061287\n",
            "train loss:0.006458070544098646\n",
            "train loss:0.0024779519970014015\n",
            "train loss:0.0009271132095506153\n",
            "train loss:0.0016766029298758936\n",
            "train loss:0.004548205806287741\n",
            "train loss:0.0018259639491855443\n",
            "train loss:0.00033909663285424903\n",
            "train loss:0.000545489883443434\n",
            "train loss:0.005733323231260408\n",
            "train loss:0.0020466558001352043\n",
            "train loss:0.0005251788835381917\n",
            "train loss:0.0006606855891602228\n",
            "train loss:0.0002764132932886155\n",
            "train loss:0.004119491473975293\n",
            "train loss:0.0005216341593646221\n",
            "train loss:0.003789484040797472\n",
            "train loss:0.0040013591630149396\n",
            "train loss:0.0016095105902044781\n",
            "train loss:0.016526866227006123\n",
            "train loss:0.0018360812371044344\n",
            "train loss:0.004054037162787273\n",
            "train loss:0.0003569600560111643\n",
            "train loss:0.0030542919629445637\n",
            "train loss:0.0032881118017061726\n",
            "train loss:0.0017197937758395938\n",
            "train loss:0.002049303891562679\n",
            "train loss:0.00032288010034910934\n",
            "train loss:0.0038315920865968824\n",
            "train loss:0.00028931693496269165\n",
            "train loss:0.00948476879969565\n",
            "train loss:0.00317628087888589\n",
            "train loss:0.0015550325756745525\n",
            "train loss:0.010236258862846353\n",
            "train loss:0.0005792334935299918\n",
            "train loss:0.0004453015677476581\n",
            "train loss:0.001507588594088692\n",
            "train loss:0.0018904737719312127\n",
            "train loss:0.00016872423105546632\n",
            "train loss:0.0019029934037157292\n",
            "train loss:0.003183651190205517\n",
            "train loss:0.004213982972143368\n",
            "train loss:0.001312031976254761\n",
            "train loss:0.008841719752806532\n",
            "train loss:0.0016772506582771048\n",
            "train loss:0.0009121060144951843\n",
            "train loss:0.002975583938683174\n",
            "train loss:0.0013211554451319485\n",
            "train loss:0.0013095697962117645\n",
            "train loss:0.00029249879838999103\n",
            "train loss:0.001093372498413508\n",
            "train loss:0.0011258826980209884\n",
            "train loss:0.001649688753319864\n",
            "train loss:0.006118961056652159\n",
            "train loss:0.0002996927874635412\n",
            "train loss:0.001769933187857731\n",
            "train loss:0.00036080510971094444\n",
            "train loss:0.000260787779658838\n",
            "train loss:0.0014630221765285691\n",
            "train loss:0.005317023521442237\n",
            "train loss:0.004039568012857803\n",
            "train loss:0.002353630136870575\n",
            "train loss:0.0004574937086104663\n",
            "train loss:0.0051945090132253895\n",
            "train loss:0.008788189809136333\n",
            "train loss:0.0006053382302443806\n",
            "train loss:0.002400300886415501\n",
            "train loss:0.003448998970490626\n",
            "train loss:0.0009460468605142324\n",
            "train loss:0.001365632442651669\n",
            "train loss:0.0014198629125334403\n",
            "train loss:0.03481582146856035\n",
            "train loss:0.0011344721829677884\n",
            "train loss:0.0010055048739068101\n",
            "train loss:0.0015107071332620895\n",
            "train loss:0.006619658648804125\n",
            "train loss:0.0048528396046863395\n",
            "train loss:0.0006977895683603841\n",
            "train loss:0.0010827596288496251\n",
            "train loss:0.0030310074683002714\n",
            "train loss:0.00900510850443765\n",
            "train loss:0.00525445922652508\n",
            "train loss:0.00048426694253406546\n",
            "train loss:0.0006804700243459295\n",
            "train loss:0.000994848454749283\n",
            "train loss:0.0002943497875914054\n",
            "train loss:0.00581661194650524\n",
            "train loss:0.0002791096555191828\n",
            "train loss:0.0003729562168667232\n",
            "train loss:8.809785921293841e-05\n",
            "train loss:0.0008507270943470292\n",
            "train loss:0.0005914485594647424\n",
            "train loss:0.0009909547964984853\n",
            "train loss:0.00024592395133890663\n",
            "train loss:0.0020254647282960457\n",
            "train loss:0.005099774188644366\n",
            "train loss:0.0005421133023256025\n",
            "train loss:0.001767902497877253\n",
            "train loss:0.014441238925534781\n",
            "train loss:0.0030840574460062257\n",
            "train loss:0.0003465585118030938\n",
            "train loss:0.0006789169112123419\n",
            "train loss:0.000391293529961564\n",
            "train loss:0.0020027767877076014\n",
            "train loss:0.002033802117854568\n",
            "train loss:0.001338098885904689\n",
            "train loss:0.0006140293205242794\n",
            "train loss:0.0001638341273717202\n",
            "train loss:0.0019830355007103085\n",
            "train loss:0.0008748145841753583\n",
            "train loss:0.0011736059451152381\n",
            "train loss:0.00020041091329124343\n",
            "train loss:0.002993171524049998\n",
            "train loss:0.002116005854080082\n",
            "train loss:0.001690742277675597\n",
            "train loss:0.0014271450204264222\n",
            "train loss:0.0017556609762035486\n",
            "train loss:0.002323079689012048\n",
            "train loss:0.01684100276221598\n",
            "train loss:0.001410572603982162\n",
            "train loss:0.0008292783215175445\n",
            "train loss:0.0009407264215450674\n",
            "train loss:0.00041390731673133575\n",
            "train loss:0.0028926817935978756\n",
            "train loss:0.000452790025521079\n",
            "train loss:0.0007956755157103482\n",
            "train loss:0.0001557637422690158\n",
            "train loss:0.00043543165734144615\n",
            "train loss:0.00045167661787152877\n",
            "train loss:0.0007077690557019716\n",
            "train loss:0.0010743240835947138\n",
            "train loss:0.0003886973863305544\n",
            "train loss:0.007084689611258449\n",
            "train loss:8.091444732843961e-05\n",
            "train loss:0.0012728665126110305\n",
            "train loss:0.000433098164131893\n",
            "train loss:0.004615364969446406\n",
            "train loss:0.004923926207505763\n",
            "train loss:0.000762242734041537\n",
            "train loss:0.00018921230486909285\n",
            "train loss:0.0018564481110541259\n",
            "train loss:0.00029978607231584723\n",
            "train loss:0.00062606171825619\n",
            "train loss:0.0006605955953489191\n",
            "train loss:0.005240816880347885\n",
            "train loss:0.0016647559182872574\n",
            "train loss:0.0006105163868975261\n",
            "train loss:0.0035262566648005937\n",
            "train loss:0.003039606908778075\n",
            "train loss:5.607752101496045e-05\n",
            "train loss:0.0016069748833874007\n",
            "train loss:0.00039289588694426487\n",
            "train loss:0.00038825128262703066\n",
            "train loss:0.002053895386235437\n",
            "train loss:0.001473620357356201\n",
            "train loss:0.0005151915134728127\n",
            "train loss:0.0012107048382363238\n",
            "train loss:0.00023527016242726022\n",
            "train loss:0.0005890552586461954\n",
            "train loss:8.939933029355004e-05\n",
            "train loss:0.00472079252490557\n",
            "train loss:0.0010984808756348628\n",
            "train loss:0.00020439611342673185\n",
            "train loss:0.004325122067473866\n",
            "train loss:0.000759681313698255\n",
            "train loss:0.00018140185170120303\n",
            "train loss:0.0010456090429093307\n",
            "train loss:0.010563890744089944\n",
            "train loss:0.004861593875024906\n",
            "train loss:0.006942490253418341\n",
            "train loss:0.00021312510485809428\n",
            "train loss:0.0004160976623042035\n",
            "train loss:0.001028188611088445\n",
            "train loss:0.0005488773255043578\n",
            "train loss:0.001883947936697518\n",
            "train loss:0.001744742494528833\n",
            "train loss:0.010817815192322533\n",
            "train loss:0.0010816462011696319\n",
            "train loss:0.000633972341079545\n",
            "train loss:0.0015329892945775058\n",
            "train loss:6.938583295937641e-05\n",
            "train loss:0.0016810791998454549\n",
            "train loss:0.0026697651883743896\n",
            "train loss:0.0030269675977177475\n",
            "train loss:0.003745787481974023\n",
            "train loss:0.0018018857917617096\n",
            "train loss:0.003491035963819629\n",
            "train loss:0.000764817419468839\n",
            "train loss:0.0003030743440236963\n",
            "train loss:0.0030340863052460694\n",
            "train loss:0.004210986026469294\n",
            "train loss:0.00041210875083695067\n",
            "train loss:0.000820860580990554\n",
            "train loss:0.0037081011007396715\n",
            "train loss:0.0009467590241684832\n",
            "train loss:0.0021547042790590995\n",
            "train loss:0.0008094170091409363\n",
            "train loss:0.002716651133089122\n",
            "train loss:0.00015587800963336418\n",
            "train loss:0.001934806890570595\n",
            "train loss:0.0012483828569812509\n",
            "train loss:0.002341206072852962\n",
            "train loss:0.002291611634573336\n",
            "train loss:0.0008993654109727695\n",
            "train loss:0.0031503648263962593\n",
            "train loss:0.0005124115991099832\n",
            "train loss:0.0020664260492027535\n",
            "train loss:0.0004386891225280404\n",
            "train loss:0.0009167219357374339\n",
            "train loss:0.002715963004797731\n",
            "train loss:0.0010851760020320416\n",
            "train loss:0.00038844250754335627\n",
            "train loss:0.0003974778813319873\n",
            "train loss:0.001532322325815371\n",
            "train loss:0.0016676478016423087\n",
            "train loss:0.002347215044201382\n",
            "train loss:0.001192458371613885\n",
            "train loss:0.0002841303162601669\n",
            "train loss:0.0012436508316798806\n",
            "train loss:0.012487956997579366\n",
            "train loss:0.000252748658921212\n",
            "train loss:0.0006339768268519582\n",
            "train loss:0.00018051098583680178\n",
            "train loss:0.003285740318574572\n",
            "train loss:0.0007090054473659283\n",
            "train loss:0.0002572532039022334\n",
            "train loss:0.0017956283836229425\n",
            "train loss:0.0009656748538723659\n",
            "train loss:0.00010823906436551555\n",
            "train loss:0.16155512578691789\n",
            "train loss:0.0030065697604124077\n",
            "train loss:0.0010840973666957532\n",
            "train loss:6.461446142334905e-05\n",
            "train loss:0.0026477139249795317\n",
            "train loss:0.007732683025863595\n",
            "train loss:0.0034012471553197536\n",
            "train loss:0.0011224495315936998\n",
            "train loss:0.0023032856385900454\n",
            "train loss:0.0006408645462061071\n",
            "train loss:0.0019423350634635542\n",
            "train loss:0.0003498627723800369\n",
            "train loss:0.0006605791621286377\n",
            "train loss:0.0014511190650687118\n",
            "train loss:0.0041481557584320365\n",
            "train loss:0.00047727234192593206\n",
            "train loss:0.0029724229504845255\n",
            "train loss:0.009320431994736459\n",
            "train loss:0.0002461793303375551\n",
            "train loss:0.0073262979058664034\n",
            "train loss:0.009624409350403651\n",
            "train loss:0.0015242793507711872\n",
            "train loss:0.004053613106092035\n",
            "train loss:0.002958490598538609\n",
            "train loss:0.0035008234962842734\n",
            "train loss:0.004107002573004276\n",
            "train loss:0.005856425158431916\n",
            "train loss:0.00017921241989375357\n",
            "train loss:0.003703601049607168\n",
            "train loss:0.00011506593786722389\n",
            "train loss:0.000680601317566746\n",
            "train loss:0.0022826628190692074\n",
            "train loss:0.0035209091382173412\n",
            "train loss:0.0017529159871974012\n",
            "train loss:0.005728606653920788\n",
            "train loss:0.008264665869758827\n",
            "train loss:0.002201997963276756\n",
            "train loss:0.0005234947772517142\n",
            "train loss:0.004079142126138958\n",
            "train loss:0.00047062102977239734\n",
            "train loss:0.0031836559603031465\n",
            "train loss:0.0001379285749183212\n",
            "train loss:0.0004842558373246519\n",
            "train loss:0.0011514793932800272\n",
            "train loss:0.004640713555898374\n",
            "train loss:0.001999355729977027\n",
            "train loss:0.0010221907189656221\n",
            "train loss:0.004340109734467908\n",
            "train loss:0.0016288971384820754\n",
            "train loss:0.002556933928423808\n",
            "train loss:0.0034216205797862227\n",
            "train loss:0.001214308180374223\n",
            "train loss:0.0035924677107320585\n",
            "train loss:0.005577303241095022\n",
            "train loss:0.0026316165376372223\n",
            "train loss:0.0006780718115328335\n",
            "train loss:0.0033757386311749136\n",
            "train loss:0.0021532759739913763\n",
            "train loss:0.0010747794013439334\n",
            "train loss:0.0022027275783440786\n",
            "train loss:0.0005262572507929539\n",
            "train loss:0.0009424114199001199\n",
            "train loss:0.001370208124530495\n",
            "train loss:0.0034285853739621725\n",
            "train loss:0.0014930480241945778\n",
            "train loss:0.0044407650792351585\n",
            "train loss:0.0005566815198799849\n",
            "train loss:0.0039168489088631355\n",
            "train loss:0.0002829534025196581\n",
            "train loss:0.007798369942352838\n",
            "train loss:0.0028659168145875862\n",
            "train loss:0.015623364912723463\n",
            "train loss:0.0020353452392773566\n",
            "train loss:0.0012496006428788355\n",
            "train loss:0.0025276788067159785\n",
            "train loss:0.004157127598102591\n",
            "train loss:0.0001616529520629814\n",
            "train loss:0.00839747188561524\n",
            "train loss:0.0007588366642615389\n",
            "train loss:0.003925826367917643\n",
            "train loss:0.0019195213320819388\n",
            "train loss:0.0015505641202697088\n",
            "train loss:0.0019971850069119062\n",
            "train loss:0.0012694990656359592\n",
            "train loss:0.000589325752159439\n",
            "train loss:0.0015261803400865356\n",
            "train loss:0.0013420379005552605\n",
            "train loss:0.004305034341013161\n",
            "train loss:0.0021818621016430504\n",
            "train loss:0.0032999242922202\n",
            "train loss:0.003396506491157083\n",
            "train loss:0.012012542382982884\n",
            "train loss:0.00019075951624990918\n",
            "train loss:0.003124919429364335\n",
            "train loss:0.0006272025421170623\n",
            "train loss:0.006983826642942725\n",
            "train loss:0.001430775582437799\n",
            "train loss:0.003923656107660018\n",
            "train loss:0.0012907496578915727\n",
            "train loss:0.00210321436134822\n",
            "=== epoch:16, train acc:0.994, test acc:0.988 ===\n",
            "train loss:0.0006279172753443915\n",
            "train loss:0.0004390536093457411\n",
            "train loss:0.0036779800984619946\n",
            "train loss:0.0019042899879689829\n",
            "train loss:0.0038996701232866153\n",
            "train loss:0.005402520104747884\n",
            "train loss:0.00285003568399213\n",
            "train loss:0.005633026927313386\n",
            "train loss:0.0026467202065053085\n",
            "train loss:0.000983351051519405\n",
            "train loss:0.0020943834547742014\n",
            "train loss:0.00316921305615653\n",
            "train loss:0.0008319966853419574\n",
            "train loss:0.006077169488674914\n",
            "train loss:0.00037910730419917605\n",
            "train loss:0.0009783763031023125\n",
            "train loss:0.0031479409132297693\n",
            "train loss:0.0018745521079987968\n",
            "train loss:0.002437095684048227\n",
            "train loss:0.0019505138944433506\n",
            "train loss:0.0014131456848238354\n",
            "train loss:0.00014187629420148905\n",
            "train loss:0.0042521635570722536\n",
            "train loss:0.00046900542078627365\n",
            "train loss:0.0031544089440464825\n",
            "train loss:0.000167271668662412\n",
            "train loss:0.0010195250931593022\n",
            "train loss:0.001208612271826761\n",
            "train loss:0.0006600686032814802\n",
            "train loss:0.006982203829570638\n",
            "train loss:0.0048375761218683285\n",
            "train loss:0.004800210654351291\n",
            "train loss:0.000830935177407693\n",
            "train loss:0.005016426611370321\n",
            "train loss:0.005300812263290645\n",
            "train loss:0.003971641424857343\n",
            "train loss:0.004256644552231829\n",
            "train loss:0.0008550825302900563\n",
            "train loss:0.002317518703417551\n",
            "train loss:0.00379540136923785\n",
            "train loss:0.0020311919510893127\n",
            "train loss:0.00805860606237489\n",
            "train loss:0.0012232407314143507\n",
            "train loss:0.001127734991092836\n",
            "train loss:0.0006636255072483739\n",
            "train loss:0.0013601751920204312\n",
            "train loss:0.0003448281398048861\n",
            "train loss:0.000466518595124466\n",
            "train loss:0.0008359031571114383\n",
            "train loss:0.00022308761122642433\n",
            "train loss:0.0002675216032870282\n",
            "train loss:0.003737769638985347\n",
            "train loss:0.0019465424223417576\n",
            "train loss:0.001437164502112948\n",
            "train loss:0.0003253983621117696\n",
            "train loss:0.012151375899116707\n",
            "train loss:0.0006096750637487782\n",
            "train loss:0.0009230422415493886\n",
            "train loss:0.0014159200845109013\n",
            "train loss:0.0006536123492217469\n",
            "train loss:0.002965466263716134\n",
            "train loss:0.0015238849584155887\n",
            "train loss:0.00011124004752047525\n",
            "train loss:0.0019211110495481784\n",
            "train loss:0.00034830185762103444\n",
            "train loss:0.007973113832550022\n",
            "train loss:0.002308466850136072\n",
            "train loss:0.001759417554146554\n",
            "train loss:0.0011265290095273238\n",
            "train loss:0.0008314238532087622\n",
            "train loss:0.0001983786190497571\n",
            "train loss:0.00835113699173961\n",
            "train loss:0.005094746688651145\n",
            "train loss:0.002120283964836242\n",
            "train loss:9.101801064610812e-05\n",
            "train loss:0.0016236076770095628\n",
            "train loss:0.0020370210709053723\n",
            "train loss:0.00276747042928958\n",
            "train loss:0.006190568257552876\n",
            "train loss:0.004437065879133759\n",
            "train loss:0.0015917514479175656\n",
            "train loss:0.0008084432944375035\n",
            "train loss:0.0005323226908907873\n",
            "train loss:9.526575680880455e-05\n",
            "train loss:0.0006724857297200086\n",
            "train loss:0.003923238547469925\n",
            "train loss:0.0010597499503897793\n",
            "train loss:0.0006202942557926509\n",
            "train loss:0.0009330668909025916\n",
            "train loss:0.0014649087786652694\n",
            "train loss:0.0009714896744281657\n",
            "train loss:0.0025465619559042613\n",
            "train loss:0.0016088372301059255\n",
            "train loss:0.0002575405752829558\n",
            "train loss:0.00012995655187845662\n",
            "train loss:0.0007704683912871151\n",
            "train loss:0.0019213883067543838\n",
            "train loss:0.0009516365170195772\n",
            "train loss:0.00272052556847662\n",
            "train loss:0.0001282665306399627\n",
            "train loss:0.002142579991617367\n",
            "train loss:0.0033378289656169724\n",
            "train loss:0.0045856567287668945\n",
            "train loss:0.0008763429359415373\n",
            "train loss:0.0033718003246912685\n",
            "train loss:0.0003157430282813553\n",
            "train loss:0.017554955590474815\n",
            "train loss:0.0006466533496762965\n",
            "train loss:0.00018073643014124532\n",
            "train loss:9.02697292391824e-05\n",
            "train loss:0.0015097545642256526\n",
            "train loss:0.002572839709688311\n",
            "train loss:0.007225362115187828\n",
            "train loss:0.007744852862874383\n",
            "train loss:0.0030437473608101325\n",
            "train loss:0.00125500849580914\n",
            "train loss:0.0072412018269298615\n",
            "train loss:0.0011937719325978418\n",
            "train loss:0.003457705599586029\n",
            "train loss:0.0005040796656721046\n",
            "train loss:0.007439757423482251\n",
            "train loss:0.0008053006167186694\n",
            "train loss:0.0017577875629063342\n",
            "train loss:0.001409125042735749\n",
            "train loss:0.003407173679790469\n",
            "train loss:0.0011163505324329915\n",
            "train loss:0.0032702817896612785\n",
            "train loss:0.0009952048175220667\n",
            "train loss:0.004440401420706064\n",
            "train loss:0.0009241236225426809\n",
            "train loss:0.0014726641455923598\n",
            "train loss:0.0034056390227698635\n",
            "train loss:0.0012410214551921922\n",
            "train loss:0.0006679185511386278\n",
            "train loss:0.0017345240915077607\n",
            "train loss:0.0022594296439206467\n",
            "train loss:0.0006714076546770403\n",
            "train loss:0.002159885919320571\n",
            "train loss:0.003039294225048571\n",
            "train loss:0.005816440185092262\n",
            "train loss:0.0012735975205337435\n",
            "train loss:0.0010203623141709186\n",
            "train loss:9.393706708074351e-05\n",
            "train loss:0.00016023873293475097\n",
            "train loss:0.0009879264928925198\n",
            "train loss:0.00880810004459338\n",
            "train loss:0.007049993673758204\n",
            "train loss:0.0004016418419527548\n",
            "train loss:0.0033385987965600145\n",
            "train loss:0.0016823634611140062\n",
            "train loss:0.0018522583765198484\n",
            "train loss:4.319957695637079e-05\n",
            "train loss:0.0024866563394359124\n",
            "train loss:0.0011876192873018485\n",
            "train loss:0.001987884361061087\n",
            "train loss:0.0005932652910540322\n",
            "train loss:0.00538347573231899\n",
            "train loss:0.001223253443520167\n",
            "train loss:0.0007270897953776083\n",
            "train loss:0.0016225856584819664\n",
            "train loss:0.0021719806278331123\n",
            "train loss:0.000551995659720887\n",
            "train loss:0.0007419413978292932\n",
            "train loss:0.00010851700059402143\n",
            "train loss:0.0008204536279182284\n",
            "train loss:0.002644439183476809\n",
            "train loss:0.005207318455399525\n",
            "train loss:0.000690061367024877\n",
            "train loss:0.000993691568749175\n",
            "train loss:0.0008370583735662972\n",
            "train loss:0.0016223898503758946\n",
            "train loss:0.0004467098136419965\n",
            "train loss:0.0006211855729956034\n",
            "train loss:0.002270640849327672\n",
            "train loss:0.0003821605935621644\n",
            "train loss:0.0007454277171175603\n",
            "train loss:0.011890931840742667\n",
            "train loss:0.006248084818967943\n",
            "train loss:0.0038847403615579483\n",
            "train loss:0.00039921184996172173\n",
            "train loss:0.004942985642454658\n",
            "train loss:0.00021270778278764544\n",
            "train loss:0.010411017795054987\n",
            "train loss:0.002567189048358543\n",
            "train loss:0.0014583646114185293\n",
            "train loss:0.0020816516783452393\n",
            "train loss:0.0001392682658343405\n",
            "train loss:0.0016675599395118237\n",
            "train loss:0.003451456918730002\n",
            "train loss:0.001379464461694572\n",
            "train loss:0.001077813270825568\n",
            "train loss:0.0039141163750323945\n",
            "train loss:0.00039558924618683113\n",
            "train loss:0.0006636896284288735\n",
            "train loss:0.0001415576053441867\n",
            "train loss:0.0002908177756432368\n",
            "train loss:8.889372696235987e-05\n",
            "train loss:0.0002044617883907064\n",
            "train loss:0.0044603004984150344\n",
            "train loss:0.0003559779834036949\n",
            "train loss:0.0011343721434401166\n",
            "train loss:0.017780616017415964\n",
            "train loss:3.926984117191612e-05\n",
            "train loss:0.00012204976315898522\n",
            "train loss:0.0009493539239712181\n",
            "train loss:0.0007591456127218917\n",
            "train loss:0.002910084750327587\n",
            "train loss:0.0056989186573184365\n",
            "train loss:0.0003879669603801663\n",
            "train loss:0.0059319065858695706\n",
            "train loss:0.004765627390979728\n",
            "train loss:0.0055404592407866625\n",
            "train loss:0.0003513810771000212\n",
            "train loss:0.006474545975971528\n",
            "train loss:2.14747657391354e-05\n",
            "train loss:0.0025789917737608294\n",
            "train loss:0.0027470577339494063\n",
            "train loss:0.0014831919572393542\n",
            "train loss:0.0001224161723829084\n",
            "train loss:0.0016053106863603119\n",
            "train loss:0.002025082465400686\n",
            "train loss:0.0007322738308165758\n",
            "train loss:0.0011756002525893845\n",
            "train loss:0.001792546906263124\n",
            "train loss:0.00028574294474832245\n",
            "train loss:0.0025083769670471846\n",
            "train loss:0.00025481089340099214\n",
            "train loss:0.0002589795331759594\n",
            "train loss:0.0025310700144003965\n",
            "train loss:0.0017387030259347657\n",
            "train loss:0.0036241022770040964\n",
            "train loss:0.0006262065634255208\n",
            "train loss:0.0010430253305716425\n",
            "train loss:0.007614930897509281\n",
            "train loss:0.00034684210921890333\n",
            "train loss:0.002162443777620994\n",
            "train loss:0.0034686303570286405\n",
            "train loss:0.006850790712244104\n",
            "train loss:0.003140685272857453\n",
            "train loss:0.03581038670726335\n",
            "train loss:0.002108842912386589\n",
            "train loss:0.00036035745196188817\n",
            "train loss:0.00011375068208236838\n",
            "train loss:0.0015000515396730424\n",
            "train loss:0.001103008300905546\n",
            "train loss:0.00821522382031271\n",
            "train loss:0.0013488396891934607\n",
            "train loss:0.002078534200008026\n",
            "train loss:0.0003675063862941582\n",
            "train loss:0.028990464806125035\n",
            "train loss:0.005499191812053994\n",
            "train loss:0.010507286320422244\n",
            "train loss:0.0022142230787358043\n",
            "train loss:0.0012960917965387996\n",
            "train loss:0.0028169744719403235\n",
            "train loss:0.0022439465936033966\n",
            "train loss:0.003942632297050692\n",
            "train loss:0.0033806793489395944\n",
            "train loss:0.002097519122201695\n",
            "train loss:0.0014760446275698732\n",
            "train loss:0.003212999523241078\n",
            "train loss:0.0030695626918843966\n",
            "train loss:0.0015026274166187884\n",
            "train loss:0.0009303303610801068\n",
            "train loss:0.006249077293688239\n",
            "train loss:0.005583047060977017\n",
            "train loss:0.0002851496773499569\n",
            "train loss:0.001659543682406186\n",
            "train loss:0.0007392944720107994\n",
            "train loss:0.0018909018736945067\n",
            "train loss:0.001579161618982779\n",
            "train loss:0.0010700986722464285\n",
            "train loss:0.0009129697714886375\n",
            "train loss:0.0006760360382856395\n",
            "train loss:0.02231925499044061\n",
            "train loss:0.00084210230144093\n",
            "train loss:0.0007703540921470082\n",
            "train loss:0.007475382062573076\n",
            "train loss:0.0021769094225206876\n",
            "train loss:0.008842521307764011\n",
            "train loss:0.002283403350521229\n",
            "train loss:0.0010518955403025004\n",
            "train loss:0.0016934978241513845\n",
            "train loss:0.0002029049862179793\n",
            "train loss:0.0023576610952134322\n",
            "train loss:0.0026714908072290712\n",
            "train loss:0.0008583637881000827\n",
            "train loss:0.00034270423090933553\n",
            "train loss:0.002686149712298473\n",
            "train loss:0.0014625511305148514\n",
            "train loss:0.019841351128155157\n",
            "train loss:0.004030472536225877\n",
            "train loss:0.001223280110561672\n",
            "train loss:0.005588338858454116\n",
            "train loss:0.00790058273305696\n",
            "train loss:0.00034194125902619276\n",
            "train loss:0.001299833410734679\n",
            "train loss:0.0004691154766265565\n",
            "train loss:0.007024943511897859\n",
            "train loss:0.0006088144532626526\n",
            "train loss:0.0004967868878793259\n",
            "train loss:0.0029849825162782373\n",
            "train loss:0.00285464295955547\n",
            "train loss:0.0077114761973250765\n",
            "train loss:0.0023249110211843206\n",
            "train loss:0.0041901024517534725\n",
            "train loss:0.0009739098335171585\n",
            "train loss:0.0051061876933025056\n",
            "train loss:5.2705541980540936e-05\n",
            "train loss:0.004572746778031308\n",
            "train loss:0.00035968278842035727\n",
            "train loss:0.003449953042249373\n",
            "train loss:0.00046213628503523194\n",
            "train loss:0.0003496968946147653\n",
            "train loss:0.004502952725550503\n",
            "train loss:0.00046777645549103767\n",
            "train loss:0.006257642306481713\n",
            "train loss:0.002108993291612824\n",
            "train loss:0.0017697142471963662\n",
            "train loss:0.003589377554197062\n",
            "train loss:0.010563933201110829\n",
            "train loss:0.0035495845687996672\n",
            "train loss:0.09342811302952486\n",
            "train loss:0.00035552685094038326\n",
            "train loss:0.0009453064276829016\n",
            "train loss:0.0010716811792819752\n",
            "train loss:0.0018928311039929308\n",
            "train loss:0.008978611835292052\n",
            "train loss:0.002207034365822953\n",
            "train loss:0.0029662670765112716\n",
            "train loss:0.0005478120901252041\n",
            "train loss:0.0028698996594310527\n",
            "train loss:0.004194735560119739\n",
            "train loss:0.005286721866559505\n",
            "train loss:0.0016974854441245244\n",
            "train loss:0.0015066706752194501\n",
            "train loss:0.0016713384149403807\n",
            "train loss:0.0025357010757307845\n",
            "train loss:0.0013123799132934025\n",
            "train loss:0.002710458535295437\n",
            "train loss:0.0019352447732201511\n",
            "train loss:0.00025711113513138376\n",
            "train loss:0.0007806331994440677\n",
            "train loss:0.00036271812551772424\n",
            "train loss:0.0028662203317607153\n",
            "train loss:0.00035721246076732277\n",
            "train loss:0.0007740256734311908\n",
            "train loss:0.0009619617671023309\n",
            "train loss:0.005957323850572488\n",
            "train loss:0.002386154642981688\n",
            "train loss:0.0009768583508988748\n",
            "train loss:0.0008707838609710719\n",
            "train loss:0.0005116569949277443\n",
            "train loss:0.00092396748805061\n",
            "train loss:0.003960099866450312\n",
            "train loss:0.00023492428737108953\n",
            "train loss:0.008435576732272866\n",
            "train loss:0.0010322907881789825\n",
            "train loss:0.0025092604548439994\n",
            "train loss:0.0003990662638256941\n",
            "train loss:0.004757679465850325\n",
            "train loss:0.002106235972857258\n",
            "train loss:0.0029914493136359633\n",
            "train loss:0.0014219691354172143\n",
            "train loss:0.0017202837473277827\n",
            "train loss:0.0005126439351194876\n",
            "train loss:0.0025345442376769494\n",
            "train loss:0.014563305273111747\n",
            "train loss:0.0026837210639578963\n",
            "train loss:0.03156641494405985\n",
            "train loss:0.0009255245441705825\n",
            "train loss:0.0003778373653294197\n",
            "train loss:0.00011562004248250423\n",
            "train loss:0.000840628966394065\n",
            "train loss:0.003199449616012466\n",
            "train loss:0.0018057327950553015\n",
            "train loss:0.0018240041261513612\n",
            "train loss:0.002134231491053312\n",
            "train loss:0.0030777334036110756\n",
            "train loss:0.0002973158276226148\n",
            "train loss:0.0006971010247255579\n",
            "train loss:0.009158869936234914\n",
            "train loss:0.0014159037642583847\n",
            "train loss:0.004213528295049585\n",
            "train loss:0.00022955628700517425\n",
            "train loss:0.0005707744941110826\n",
            "train loss:0.0002747163133871735\n",
            "train loss:0.0063833592539372045\n",
            "train loss:0.007992202432580553\n",
            "train loss:0.00029174262193029063\n",
            "train loss:0.0010610411156686105\n",
            "train loss:0.00036150880578181825\n",
            "train loss:0.005043733351892328\n",
            "train loss:0.001688916725126188\n",
            "train loss:0.0009014887732068446\n",
            "train loss:0.0005079192966255504\n",
            "train loss:0.0003057186188723086\n",
            "train loss:0.0014004213892666326\n",
            "train loss:0.0017875670234985886\n",
            "train loss:0.003078447382984574\n",
            "train loss:0.0005320224032213876\n",
            "train loss:0.0007609598247264245\n",
            "train loss:0.006578364524619031\n",
            "train loss:0.0008275681183586932\n",
            "train loss:0.0007584146606593675\n",
            "train loss:0.0003610543226985115\n",
            "train loss:0.00483787558089421\n",
            "train loss:0.002100210000969422\n",
            "train loss:0.0013849826982798607\n",
            "train loss:0.007008391718788245\n",
            "train loss:0.0004600787189980495\n",
            "train loss:0.0001740750411919271\n",
            "train loss:0.000588491272679592\n",
            "train loss:0.00044585600203260814\n",
            "train loss:0.012912575545528404\n",
            "train loss:0.001617975464502927\n",
            "train loss:0.0031717289084364224\n",
            "train loss:0.0035185612694947506\n",
            "train loss:0.0005974408951714469\n",
            "train loss:0.0003097878154537964\n",
            "train loss:0.001532743334846179\n",
            "train loss:0.0011925218287539895\n",
            "train loss:0.0022892297640251026\n",
            "train loss:0.0007458844083582393\n",
            "train loss:0.000606624436666097\n",
            "train loss:0.0025043643489568103\n",
            "train loss:0.0006624337769076914\n",
            "train loss:0.0003098112169688759\n",
            "train loss:0.0007649484094629336\n",
            "train loss:0.009124198692016326\n",
            "train loss:0.005700557393104517\n",
            "train loss:0.001429838321840309\n",
            "train loss:0.001140570351056625\n",
            "train loss:0.0015411668409471336\n",
            "train loss:0.0010839548994750923\n",
            "train loss:0.00579407771886981\n",
            "train loss:0.00014953961493348629\n",
            "train loss:0.0002449794638172997\n",
            "train loss:0.0006957698807939616\n",
            "train loss:0.0015158092685092868\n",
            "train loss:0.000986826851969705\n",
            "train loss:0.001140415131994778\n",
            "train loss:9.97053846041255e-05\n",
            "train loss:0.0019478800364384373\n",
            "train loss:0.0009228600891631521\n",
            "train loss:0.001578794332959459\n",
            "train loss:0.007576813697369813\n",
            "train loss:0.0019499261697420534\n",
            "train loss:9.729941481792063e-05\n",
            "train loss:0.0004671401900236971\n",
            "train loss:0.0005404645561997992\n",
            "train loss:0.036621097360843555\n",
            "train loss:0.0003718487774077258\n",
            "train loss:0.0020034774891264494\n",
            "train loss:0.0028240397952272083\n",
            "train loss:0.0010866080001021416\n",
            "train loss:0.000191407283598792\n",
            "train loss:0.002698007972848138\n",
            "train loss:0.0019740724013475896\n",
            "train loss:0.00046564412430906\n",
            "train loss:0.0022867402810675206\n",
            "train loss:0.012306089635219947\n",
            "train loss:0.002672008060718487\n",
            "train loss:0.000944646649181518\n",
            "train loss:0.0013205070688114431\n",
            "train loss:0.0004290562183645359\n",
            "train loss:0.0031426720606852615\n",
            "train loss:0.0008282986561479655\n",
            "train loss:0.0003967081858266014\n",
            "train loss:0.0004582718589959121\n",
            "train loss:0.009008238689551978\n",
            "train loss:0.00047974398916976723\n",
            "train loss:0.003487626865060908\n",
            "train loss:0.0009106731672681013\n",
            "train loss:0.0007797174748270558\n",
            "train loss:0.00017313467510821068\n",
            "train loss:0.0004565412499022857\n",
            "train loss:0.007370969096233916\n",
            "train loss:0.0006066610489453639\n",
            "train loss:0.0011361439607674253\n",
            "train loss:0.0015103160585301736\n",
            "train loss:0.002686379462997974\n",
            "train loss:0.0008995844249595527\n",
            "train loss:0.0003659508667743175\n",
            "train loss:0.0011023295504506258\n",
            "train loss:0.0002853677945813906\n",
            "train loss:0.0003615413078614419\n",
            "train loss:0.0004794276427428156\n",
            "train loss:0.0014762933961107512\n",
            "train loss:0.005587671254510618\n",
            "train loss:0.0005598230836831263\n",
            "train loss:0.0019618668116239846\n",
            "train loss:0.00019952570776626523\n",
            "train loss:0.004306894858973912\n",
            "train loss:0.0006135396090774664\n",
            "train loss:0.0024681250845545832\n",
            "train loss:0.0013624018788722166\n",
            "train loss:0.00043660518561725937\n",
            "train loss:0.000927551883901686\n",
            "train loss:8.05818772292103e-05\n",
            "train loss:0.0007790872604568527\n",
            "train loss:0.00022667244456461513\n",
            "train loss:0.0014480198050637618\n",
            "train loss:0.00183100850850521\n",
            "train loss:0.0036026423863153223\n",
            "train loss:0.0006449748941315113\n",
            "train loss:0.00013864849053086696\n",
            "train loss:0.0014020660227708904\n",
            "train loss:0.0017575230893672419\n",
            "train loss:0.00034610314199685915\n",
            "train loss:0.000281426829647809\n",
            "train loss:0.0009421364333370947\n",
            "train loss:0.0023574684141691297\n",
            "train loss:0.0004658989674987352\n",
            "train loss:0.0017535340997746611\n",
            "train loss:0.00046410871944850925\n",
            "train loss:7.883845930255227e-05\n",
            "train loss:0.0005149492277116796\n",
            "train loss:0.0002996228323653715\n",
            "train loss:0.0012246447662745933\n",
            "train loss:0.005112759947753038\n",
            "train loss:0.0037588419250908587\n",
            "train loss:0.00010455251956288796\n",
            "train loss:0.00144507268200551\n",
            "train loss:0.0028617421706733886\n",
            "train loss:0.001635396932313334\n",
            "train loss:0.0017500006203655842\n",
            "train loss:0.0012553877937726732\n",
            "train loss:0.0002195740740753885\n",
            "train loss:0.00015220176688147608\n",
            "train loss:0.0018540732839717425\n",
            "train loss:0.0014039979605195297\n",
            "train loss:0.00044858591546289206\n",
            "train loss:0.00030754148220919905\n",
            "train loss:0.0002978169588365977\n",
            "train loss:0.0013780083986033112\n",
            "train loss:0.0030171896320724447\n",
            "train loss:0.0002230235961754119\n",
            "train loss:0.0022403078392246195\n",
            "train loss:0.0018157157956215145\n",
            "train loss:0.0006950880097720446\n",
            "train loss:0.00018398444249430386\n",
            "train loss:0.0007182994777094614\n",
            "train loss:0.00218149212763028\n",
            "train loss:0.0013078200543038198\n",
            "train loss:0.00023349546951087388\n",
            "train loss:7.412416500476672e-05\n",
            "train loss:0.000279726399632213\n",
            "train loss:0.0005231688389791087\n",
            "train loss:0.0021901833230015995\n",
            "train loss:0.0007047166276655172\n",
            "train loss:0.003961597401838212\n",
            "train loss:0.001997669073362349\n",
            "train loss:0.002490548637727734\n",
            "train loss:0.0002609618917912396\n",
            "train loss:0.0030818964120093773\n",
            "train loss:0.00013540118404888\n",
            "train loss:0.0006435356345997443\n",
            "train loss:0.0005617616832612509\n",
            "train loss:0.000843712221868526\n",
            "train loss:0.006954388102509086\n",
            "train loss:0.00046597086959135114\n",
            "train loss:0.0038754322577067664\n",
            "train loss:0.00046975817451454287\n",
            "train loss:0.0013325096599903824\n",
            "train loss:0.000470164229437563\n",
            "train loss:0.00020722465151858612\n",
            "train loss:0.0006499469119545301\n",
            "train loss:0.002235766817951592\n",
            "train loss:0.0001705902282964663\n",
            "train loss:0.006624903147779231\n",
            "train loss:0.00013594923242652934\n",
            "train loss:0.0010579822321017589\n",
            "train loss:0.0007863792040419359\n",
            "train loss:0.003414040717995059\n",
            "train loss:0.0025144759694742556\n",
            "train loss:0.0005740810451787466\n",
            "train loss:0.0019240849059156426\n",
            "train loss:0.0023437502412766067\n",
            "train loss:0.004286239225785148\n",
            "train loss:0.00019556197689351448\n",
            "train loss:0.00012202255376819064\n",
            "train loss:0.004629994587146536\n",
            "train loss:0.00023213008774210396\n",
            "train loss:0.0016123191910452387\n",
            "train loss:0.002766336024917221\n",
            "train loss:0.00016176913731546355\n",
            "train loss:0.0022854313638177156\n",
            "train loss:0.00010248654889041116\n",
            "train loss:0.0013438731370820503\n",
            "train loss:0.004311342293008652\n",
            "train loss:0.00017959205432757552\n",
            "train loss:0.0008319578893140884\n",
            "train loss:0.0014937750164686256\n",
            "train loss:0.0019710254885052943\n",
            "train loss:0.0006132371999247927\n",
            "train loss:0.002907121329242834\n",
            "train loss:0.00518057430403928\n",
            "train loss:0.0006250526572249576\n",
            "train loss:0.0007885917003708746\n",
            "=== epoch:17, train acc:0.996, test acc:0.989 ===\n",
            "train loss:0.0010365090400722443\n",
            "train loss:0.0002921562123664737\n",
            "train loss:0.0007229501954880562\n",
            "train loss:0.0005586364373942319\n",
            "train loss:0.04451475285979213\n",
            "train loss:0.00012027928877586797\n",
            "train loss:9.759151788249636e-05\n",
            "train loss:0.000961965546689196\n",
            "train loss:0.0010633802863043492\n",
            "train loss:0.0020619400744644385\n",
            "train loss:0.00046937077681622044\n",
            "train loss:0.0002192197318508052\n",
            "train loss:0.00021790895149759434\n",
            "train loss:0.0014042795800705806\n",
            "train loss:0.0005775888033669924\n",
            "train loss:0.0011028339389414176\n",
            "train loss:0.006383848750589968\n",
            "train loss:0.001991672552597164\n",
            "train loss:0.0008984106359796066\n",
            "train loss:0.001163274221757597\n",
            "train loss:0.004096235758970561\n",
            "train loss:0.02915451548854151\n",
            "train loss:0.014868627878081887\n",
            "train loss:0.000790130070861613\n",
            "train loss:0.0002066945137273857\n",
            "train loss:0.0011557108158201342\n",
            "train loss:0.0012264780506666508\n",
            "train loss:0.001677846842582662\n",
            "train loss:0.012738780260244392\n",
            "train loss:0.0030480660729683114\n",
            "train loss:0.007943448902473204\n",
            "train loss:0.003102418773998061\n",
            "train loss:0.00020770603616100653\n",
            "train loss:0.0055369589598918845\n",
            "train loss:0.002366813984695995\n",
            "train loss:0.003733537932103079\n",
            "train loss:0.012843793166325366\n",
            "train loss:0.0036373669201665575\n",
            "train loss:0.00022142115523177093\n",
            "train loss:0.010755760747080822\n",
            "train loss:0.003203336102208435\n",
            "train loss:0.0017672392566196093\n",
            "train loss:0.0029629786653707817\n",
            "train loss:0.0020691817456537144\n",
            "train loss:0.0016408877753842394\n",
            "train loss:0.0013491833829985269\n",
            "train loss:9.63407962358969e-05\n",
            "train loss:0.0004636496924370667\n",
            "train loss:0.014882391358341083\n",
            "train loss:0.00011913870269023763\n",
            "train loss:0.0015678206526127017\n",
            "train loss:0.008928934382950896\n",
            "train loss:0.006071871536926601\n",
            "train loss:0.0029408044743138807\n",
            "train loss:0.017244960840598206\n",
            "train loss:0.000714593871118868\n",
            "train loss:0.0031962753367569775\n",
            "train loss:0.0011604149408199081\n",
            "train loss:0.001895841772955225\n",
            "train loss:0.0005564509017330508\n",
            "train loss:0.014097256236383865\n",
            "train loss:0.0003564400941299379\n",
            "train loss:0.0005409527895372161\n",
            "train loss:0.0010582367999838731\n",
            "train loss:0.000946207648776262\n",
            "train loss:0.0021973373897078417\n",
            "train loss:0.0010584898753401202\n",
            "train loss:0.0014799071659661463\n",
            "train loss:0.001279611807282607\n",
            "train loss:0.0011820149597990904\n",
            "train loss:0.001240706166525456\n",
            "train loss:0.000553096921609274\n",
            "train loss:0.00069369085751141\n",
            "train loss:0.0008149849722820994\n",
            "train loss:0.0001083288151770107\n",
            "train loss:0.0010521266504716362\n",
            "train loss:0.012991267058057815\n",
            "train loss:0.0011628371246306015\n",
            "train loss:0.0008265485988341948\n",
            "train loss:0.0011946773879871523\n",
            "train loss:0.0022353824481320247\n",
            "train loss:0.0016585284608718206\n",
            "train loss:0.00011159136464065271\n",
            "train loss:0.0024027863390111356\n",
            "train loss:0.002843995801371951\n",
            "train loss:0.005123621150542864\n",
            "train loss:0.00047338279683805556\n",
            "train loss:0.0033674664321625115\n",
            "train loss:0.0011864147593878551\n",
            "train loss:5.998740367064423e-05\n",
            "train loss:0.00033659635211148286\n",
            "train loss:0.004131932040802976\n",
            "train loss:0.008608714812454931\n",
            "train loss:0.003778332263495393\n",
            "train loss:0.01769852831562581\n",
            "train loss:0.00012830998192107638\n",
            "train loss:0.0004955777754220239\n",
            "train loss:0.002757074056381221\n",
            "train loss:0.009149706891646226\n",
            "train loss:0.002035140053602927\n",
            "train loss:0.0005226299478487628\n",
            "train loss:0.00030142510728334147\n",
            "train loss:0.00027776516426203184\n",
            "train loss:0.00012193286962869121\n",
            "train loss:0.0001446843247637933\n",
            "train loss:0.0047756038597600555\n",
            "train loss:0.007227372062539865\n",
            "train loss:0.0003661927963907315\n",
            "train loss:0.0002714182994458891\n",
            "train loss:0.004540532195009166\n",
            "train loss:0.0005003500667054301\n",
            "train loss:0.0014105620694549318\n",
            "train loss:0.0015279280479487817\n",
            "train loss:0.001692981910616248\n",
            "train loss:0.002315854798526069\n",
            "train loss:0.0007444568643773024\n",
            "train loss:0.0013492518003981068\n",
            "train loss:0.0007564802898376132\n",
            "train loss:0.0024369501878593824\n",
            "train loss:0.0005304233510561157\n",
            "train loss:0.0002680846202810469\n",
            "train loss:0.00016671980259589633\n",
            "train loss:0.0010774322275791833\n",
            "train loss:0.002649777196998701\n",
            "train loss:0.00019014079284589844\n",
            "train loss:0.0033394837768827225\n",
            "train loss:0.006828691096636611\n",
            "train loss:0.0003485893714997314\n",
            "train loss:0.006940095960216688\n",
            "train loss:0.0027654444109098245\n",
            "train loss:0.0034392038350041496\n",
            "train loss:0.004867017619855348\n",
            "train loss:0.0009741240594902354\n",
            "train loss:0.00048218912899140637\n",
            "train loss:0.0026138257696739853\n",
            "train loss:0.0003827125568824493\n",
            "train loss:0.0017714522185155771\n",
            "train loss:0.0002975896973528799\n",
            "train loss:0.0008862889671227584\n",
            "train loss:0.0010265028211739744\n",
            "train loss:0.0033576219253278468\n",
            "train loss:0.0039219645682196085\n",
            "train loss:0.002136412372993119\n",
            "train loss:0.0043670239992595035\n",
            "train loss:0.033555856702242905\n",
            "train loss:0.004465083543539109\n",
            "train loss:0.006516652964656503\n",
            "train loss:0.00022824620612134152\n",
            "train loss:0.0019629251605169217\n",
            "train loss:0.0008041172974016478\n",
            "train loss:0.00431295906894975\n",
            "train loss:0.0015474344580616813\n",
            "train loss:0.0019362943635008823\n",
            "train loss:0.0003525440164216645\n",
            "train loss:0.0003426711626569818\n",
            "train loss:0.0010454169520356187\n",
            "train loss:0.01870798608018337\n",
            "train loss:0.003173998939068729\n",
            "train loss:0.0020849932348199905\n",
            "train loss:0.0002630301211038137\n",
            "train loss:0.0005065394458520342\n",
            "train loss:0.0022267979132864694\n",
            "train loss:0.0019585300588349135\n",
            "train loss:0.0002125744803339486\n",
            "train loss:0.004536132467092301\n",
            "train loss:0.001132984649466275\n",
            "train loss:0.0009539300147570391\n",
            "train loss:0.00035169390305281984\n",
            "train loss:0.001324074785159543\n",
            "train loss:0.002153196785392513\n",
            "train loss:0.0018653506514361975\n",
            "train loss:0.00046917940752123807\n",
            "train loss:0.0015427963726156724\n",
            "train loss:0.0026668083767289207\n",
            "train loss:0.0001607524456814601\n",
            "train loss:0.0011354453511632978\n",
            "train loss:0.0037581295414599826\n",
            "train loss:0.000552330567342106\n",
            "train loss:0.000304763084157654\n",
            "train loss:0.000359956450836405\n",
            "train loss:0.0026531598740212065\n",
            "train loss:0.0012497057885007474\n",
            "train loss:0.03507247340690101\n",
            "train loss:0.005432681731878382\n",
            "train loss:0.001564293820609878\n",
            "train loss:0.0022390395291077437\n",
            "train loss:0.0005580002364696773\n",
            "train loss:0.0017719559145994892\n",
            "train loss:0.0004406612383046788\n",
            "train loss:0.0014726661094833949\n",
            "train loss:0.0011646451351799672\n",
            "train loss:0.0003576249598785978\n",
            "train loss:0.0031976922738509938\n",
            "train loss:0.0048782920986363415\n",
            "train loss:0.0029799485063484793\n",
            "train loss:2.6375914455659976e-05\n",
            "train loss:0.0008673709694731118\n",
            "train loss:0.0006167186290180785\n",
            "train loss:0.0013295156619897155\n",
            "train loss:0.003699159721658254\n",
            "train loss:0.000641100179963485\n",
            "train loss:0.0018914664042609824\n",
            "train loss:0.0008264542359201248\n",
            "train loss:0.002531417316761722\n",
            "train loss:0.0010683817467536582\n",
            "train loss:0.0007759868950031963\n",
            "train loss:0.0006025299611818759\n",
            "train loss:0.0014703399268001516\n",
            "train loss:0.002840801037059465\n",
            "train loss:0.002676227697060458\n",
            "train loss:0.0009371955440938182\n",
            "train loss:0.00032464179875707613\n",
            "train loss:0.00017445044281091566\n",
            "train loss:0.0006265078303885123\n",
            "train loss:0.003196480928562385\n",
            "train loss:0.00023711677986829733\n",
            "train loss:0.0008416851879671172\n",
            "train loss:0.0004100473214164619\n",
            "train loss:0.0020442179645133378\n",
            "train loss:0.0011794078065819036\n",
            "train loss:0.001813728778972171\n",
            "train loss:0.00021581668672884268\n",
            "train loss:0.002832912667738643\n",
            "train loss:0.0011289042061780198\n",
            "train loss:0.00025678868255884216\n",
            "train loss:0.003650123060288195\n",
            "train loss:0.0003770115106928979\n",
            "train loss:0.0006155968351733639\n",
            "train loss:7.918492832032642e-05\n",
            "train loss:0.03060286557965839\n",
            "train loss:0.00512636986041336\n",
            "train loss:6.434778066786393e-05\n",
            "train loss:0.0008741605676667348\n",
            "train loss:0.00866782438752691\n",
            "train loss:0.0059313969843524965\n",
            "train loss:0.012576762331194497\n",
            "train loss:0.0005766603480421889\n",
            "train loss:0.003491384725138917\n",
            "train loss:0.0002500431531332421\n",
            "train loss:0.0014217471621835302\n",
            "train loss:0.0015061947860457346\n",
            "train loss:0.00020440698607968787\n",
            "train loss:0.005509181021892905\n",
            "train loss:0.001790886951778375\n",
            "train loss:0.0006076046547562622\n",
            "train loss:0.0029294659439746623\n",
            "train loss:0.002834885492972567\n",
            "train loss:0.0019644028536058974\n",
            "train loss:0.0029889719617889605\n",
            "train loss:0.0005248398312356288\n",
            "train loss:0.00042211110119325595\n",
            "train loss:0.00585201753296286\n",
            "train loss:0.0011707977296884318\n",
            "train loss:0.006603891214217084\n",
            "train loss:0.0013237536585190184\n",
            "train loss:0.0016870201853445843\n",
            "train loss:0.0034156864005854114\n",
            "train loss:0.007669570604447327\n",
            "train loss:0.0030880931088033753\n",
            "train loss:0.0025487803107183547\n",
            "train loss:0.00872805351068733\n",
            "train loss:0.004048992670923939\n",
            "train loss:0.0024710492998165057\n",
            "train loss:0.0023671074763071008\n",
            "train loss:0.00027033303985317576\n",
            "train loss:0.0021138633193584747\n",
            "train loss:0.00039221185731215925\n",
            "train loss:9.122472485195405e-05\n",
            "train loss:0.0004645019884421591\n",
            "train loss:0.00209224906378063\n",
            "train loss:0.0011195720307035607\n",
            "train loss:0.011626668102191649\n",
            "train loss:0.002092372267831841\n",
            "train loss:0.0019488739095231073\n",
            "train loss:0.00997432893770132\n",
            "train loss:0.005485963403207616\n",
            "train loss:0.009181318953168207\n",
            "train loss:0.0016907638642006068\n",
            "train loss:0.0006517511930705844\n",
            "train loss:0.030390689474136146\n",
            "train loss:0.018636433480914677\n",
            "train loss:0.003486856417302152\n",
            "train loss:0.0005530575509515244\n",
            "train loss:0.00020319555578281027\n",
            "train loss:0.0035575572497653814\n",
            "train loss:0.0020340015523659396\n",
            "train loss:0.0016132843192722855\n",
            "train loss:0.0009386380239135097\n",
            "train loss:0.0010581129831210585\n",
            "train loss:0.00048492839802597424\n",
            "train loss:0.0008696117028273401\n",
            "train loss:0.004315251992271772\n",
            "train loss:0.004480607510064453\n",
            "train loss:0.001083875700753184\n",
            "train loss:0.002100341470250913\n",
            "train loss:0.0006033004276753198\n",
            "train loss:0.0019074138927940268\n",
            "train loss:0.000253916811913188\n",
            "train loss:0.0019596435761139734\n",
            "train loss:0.0002452042450913255\n",
            "train loss:0.0038949717735181273\n",
            "train loss:0.0005737038569946101\n",
            "train loss:0.001536906353700552\n",
            "train loss:0.003983582966218013\n",
            "train loss:0.0010488422586246548\n",
            "train loss:0.005101536470649437\n",
            "train loss:0.001117279983092926\n",
            "train loss:0.0017412309411196742\n",
            "train loss:0.0010426795070583208\n",
            "train loss:0.001861683158104429\n",
            "train loss:0.004096395586767838\n",
            "train loss:0.0010072912798892613\n",
            "train loss:0.001888754399698815\n",
            "train loss:0.0024119457834696724\n",
            "train loss:0.0007290717727258502\n",
            "train loss:9.887040843749366e-05\n",
            "train loss:0.0004793062888677561\n",
            "train loss:0.0016952580876144472\n",
            "train loss:0.0012431964171051343\n",
            "train loss:0.0026900754740453027\n",
            "train loss:0.002531860076600857\n",
            "train loss:0.002110016913931907\n",
            "train loss:0.001124580792669521\n",
            "train loss:0.000902867165095091\n",
            "train loss:0.0035212034457286693\n",
            "train loss:0.0009002612005288901\n",
            "train loss:0.00593212729884548\n",
            "train loss:0.0005171162089903965\n",
            "train loss:0.00036437540615306843\n",
            "train loss:0.0021201538091467314\n",
            "train loss:0.0003072569421542309\n",
            "train loss:0.00046716976265013567\n",
            "train loss:0.0008705549315651659\n",
            "train loss:0.00030942892179783304\n",
            "train loss:0.00015610562161307392\n",
            "train loss:0.0009624397972310568\n",
            "train loss:0.0007970104895729212\n",
            "train loss:0.0021020055435269458\n",
            "train loss:0.0014456046248484849\n",
            "train loss:0.0008814924568412703\n",
            "train loss:0.00023479965467379814\n",
            "train loss:0.0036802478235502777\n",
            "train loss:0.0032808486354404306\n",
            "train loss:0.0005132749167452585\n",
            "train loss:0.0056000174124865975\n",
            "train loss:0.005159110300458672\n",
            "train loss:0.005567386566385732\n",
            "train loss:7.011061573232757e-05\n",
            "train loss:0.005202118756864027\n",
            "train loss:0.0006017778757289703\n",
            "train loss:0.0005449840328599412\n",
            "train loss:0.001664858436326611\n",
            "train loss:0.0010221506267186077\n",
            "train loss:4.797478339084045e-05\n",
            "train loss:0.0036518940381282746\n",
            "train loss:0.0003721124737068804\n",
            "train loss:0.0001354918731303643\n",
            "train loss:0.0011009879295562305\n",
            "train loss:0.012284032355206188\n",
            "train loss:0.00033887102050231065\n",
            "train loss:0.005234058526000932\n",
            "train loss:9.492624026242919e-05\n",
            "train loss:0.00407894974010547\n",
            "train loss:0.00011904840621433699\n",
            "train loss:8.537965162496858e-05\n",
            "train loss:0.0007175139126489082\n",
            "train loss:0.0002805706668888281\n",
            "train loss:0.0002601492254337099\n",
            "train loss:0.0010777264155872302\n",
            "train loss:0.0031586532708921585\n",
            "train loss:0.004439284897482252\n",
            "train loss:0.0011242061251465682\n",
            "train loss:0.002679135554785252\n",
            "train loss:0.0006475875947627415\n",
            "train loss:0.0004666917878602949\n",
            "train loss:0.0001891709887915899\n",
            "train loss:0.0010377657738891576\n",
            "train loss:0.0010424810309669528\n",
            "train loss:0.0007468434395894215\n",
            "train loss:0.0022706830709018112\n",
            "train loss:0.0003743518649146151\n",
            "train loss:0.0011849079655847218\n",
            "train loss:0.00017271598422835449\n",
            "train loss:0.0001023826956264013\n",
            "train loss:0.0017811504696326566\n",
            "train loss:0.015085583406198089\n",
            "train loss:0.00515194675181399\n",
            "train loss:0.00041092216487751055\n",
            "train loss:0.0010161198084361466\n",
            "train loss:3.888815103417384e-05\n",
            "train loss:0.0011129462221073517\n",
            "train loss:4.02706190825223e-05\n",
            "train loss:0.000883476293006114\n",
            "train loss:0.0031278221965603055\n",
            "train loss:0.004967054816702399\n",
            "train loss:0.006241226082535207\n",
            "train loss:0.0030463251170042673\n",
            "train loss:0.0009730793230019314\n",
            "train loss:0.0018578560241205035\n",
            "train loss:0.0023942567717711257\n",
            "train loss:0.0011780514087762987\n",
            "train loss:0.004004813892000179\n",
            "train loss:0.0023348546381637544\n",
            "train loss:0.015541865777268326\n",
            "train loss:9.034977381178164e-05\n",
            "train loss:0.001992607264797694\n",
            "train loss:0.0012330856071381899\n",
            "train loss:0.0009471999780023012\n",
            "train loss:0.0010445870672664075\n",
            "train loss:0.0006461307568093347\n",
            "train loss:0.0015456250216152692\n",
            "train loss:0.0029447736345314295\n",
            "train loss:0.002320984089180161\n",
            "train loss:0.0009957837011286305\n",
            "train loss:0.0026398281745653907\n",
            "train loss:0.0016670699255627516\n",
            "train loss:0.00025667309425636146\n",
            "train loss:0.0022164063407348407\n",
            "train loss:9.097159687423155e-05\n",
            "train loss:0.00048608985020766424\n",
            "train loss:0.007431548113880024\n",
            "train loss:0.003903654357268507\n",
            "train loss:0.00031516915133719486\n",
            "train loss:0.0012873362487792524\n",
            "train loss:0.0009722614443543526\n",
            "train loss:0.0004824335250218907\n",
            "train loss:0.00391351579904664\n",
            "train loss:0.0003839705215853747\n",
            "train loss:0.005909105836364054\n",
            "train loss:0.0013494561435312522\n",
            "train loss:0.007413499492502853\n",
            "train loss:0.0017738789400665392\n",
            "train loss:0.00031730594412502744\n",
            "train loss:0.004870054508457848\n",
            "train loss:0.0013990576503221893\n",
            "train loss:0.00029835713219623575\n",
            "train loss:0.0005917077338938199\n",
            "train loss:0.00125018628466903\n",
            "train loss:0.0010552329199030998\n",
            "train loss:0.000640435976150667\n",
            "train loss:0.00011468713283426234\n",
            "train loss:0.0035163713326973396\n",
            "train loss:0.0007100915639127423\n",
            "train loss:0.004574665217727242\n",
            "train loss:0.004857993102033211\n",
            "train loss:0.0011274416170514937\n",
            "train loss:0.0015366861577221413\n",
            "train loss:0.00013488858044737135\n",
            "train loss:0.004258960817460642\n",
            "train loss:0.0009382628281590208\n",
            "train loss:6.025712543526557e-05\n",
            "train loss:0.0003162996204578389\n",
            "train loss:0.0051031002159641825\n",
            "train loss:0.007443942258757101\n",
            "train loss:0.00024882896221470097\n",
            "train loss:0.0006120205871812196\n",
            "train loss:0.000919884184811933\n",
            "train loss:0.0024647151704006946\n",
            "train loss:9.404519566594785e-05\n",
            "train loss:0.004735499604218213\n",
            "train loss:0.0002163946258156711\n",
            "train loss:0.002534163198919395\n",
            "train loss:0.0013532504715333992\n",
            "train loss:0.000746806108764835\n",
            "train loss:0.0010103209776493054\n",
            "train loss:0.00026664820957378056\n",
            "train loss:0.002288354101621029\n",
            "train loss:9.426355751780485e-05\n",
            "train loss:0.008053279189250372\n",
            "train loss:0.0005580666684282295\n",
            "train loss:0.00022432769499532528\n",
            "train loss:0.012845213617374207\n",
            "train loss:0.00021753548824718846\n",
            "train loss:0.0026300291970200707\n",
            "train loss:0.0008327873109075809\n",
            "train loss:0.03142170303590193\n",
            "train loss:0.0017421873217568465\n",
            "train loss:0.005023748505343044\n",
            "train loss:0.0020063392176689803\n",
            "train loss:0.0015614341656730276\n",
            "train loss:0.0007580760380561664\n",
            "train loss:0.007693860392779873\n",
            "train loss:0.0004931578847192561\n",
            "train loss:0.002792905512762137\n",
            "train loss:0.0066921243524451154\n",
            "train loss:2.491589510031466e-05\n",
            "train loss:0.0005520232853479698\n",
            "train loss:0.00016281601561486051\n",
            "train loss:0.00017692941145359977\n",
            "train loss:0.00040518974625155765\n",
            "train loss:0.003961700278191314\n",
            "train loss:0.0006350093287790132\n",
            "train loss:0.0016778704658470898\n",
            "train loss:0.0005977500910516978\n",
            "train loss:0.002111932261430431\n",
            "train loss:0.0004446482303694223\n",
            "train loss:0.006030631497298964\n",
            "train loss:6.986232126223385e-05\n",
            "train loss:0.005222065650271579\n",
            "train loss:0.000262977187156577\n",
            "train loss:0.00013445802053557398\n",
            "train loss:0.001064388118646533\n",
            "train loss:7.734797058421527e-05\n",
            "train loss:0.002301259789703898\n",
            "train loss:0.0008658344623755528\n",
            "train loss:0.0006686858595923849\n",
            "train loss:0.0008053055778697255\n",
            "train loss:0.000993987508837368\n",
            "train loss:0.00019370248736822634\n",
            "train loss:0.0040685572391006685\n",
            "train loss:0.002931521630199455\n",
            "train loss:0.0011340374488194476\n",
            "train loss:0.0021061291425921573\n",
            "train loss:0.004381839737938095\n",
            "train loss:0.0014631968126513357\n",
            "train loss:0.0010451420121069086\n",
            "train loss:0.0009228024051999816\n",
            "train loss:0.0016356109535498218\n",
            "train loss:0.0020909248021417724\n",
            "train loss:0.000888173647372382\n",
            "train loss:0.00012976216642528816\n",
            "train loss:0.0004055028397028915\n",
            "train loss:0.0013314393706498299\n",
            "train loss:8.894329310871149e-05\n",
            "train loss:0.0007344522263663486\n",
            "train loss:2.6550349940488714e-05\n",
            "train loss:0.0007481105806177113\n",
            "train loss:0.0010705254319536975\n",
            "train loss:0.00034113826455851995\n",
            "train loss:0.002408685714108505\n",
            "train loss:0.0026227238168239336\n",
            "train loss:0.00018832732801401305\n",
            "train loss:0.0020255097425020034\n",
            "train loss:0.009354559497863782\n",
            "train loss:0.00029732286173137764\n",
            "train loss:0.0037423470525251375\n",
            "train loss:0.00042739143881948243\n",
            "train loss:0.0034859098108800707\n",
            "train loss:0.0010825132618464835\n",
            "train loss:0.0005992694939796751\n",
            "train loss:0.004951476693037481\n",
            "train loss:0.00011406043626467231\n",
            "train loss:0.003440732579513233\n",
            "train loss:0.00035275613608023835\n",
            "train loss:0.0037866286140675496\n",
            "train loss:0.0007284357841489637\n",
            "train loss:0.001039452510843621\n",
            "train loss:0.00019361417074450322\n",
            "train loss:0.0026420164770320385\n",
            "train loss:0.0006239827644691081\n",
            "train loss:0.00027989163239931796\n",
            "train loss:0.0037396076815456487\n",
            "train loss:0.0026104633053170073\n",
            "train loss:0.0006414442552844552\n",
            "train loss:0.0010525510310113015\n",
            "train loss:0.00016195680368318453\n",
            "train loss:0.00045877323035692184\n",
            "train loss:0.0003996638274795819\n",
            "train loss:0.0013992857645699524\n",
            "train loss:0.0007190830535358666\n",
            "train loss:0.0016213262140088122\n",
            "train loss:0.01651129026593041\n",
            "train loss:0.00044858721012439214\n",
            "train loss:0.0014640351950309244\n",
            "train loss:0.004207288418404585\n",
            "train loss:0.0010070531462783104\n",
            "train loss:0.00446730263048348\n",
            "train loss:4.180399137124457e-05\n",
            "train loss:0.0006524851727308102\n",
            "train loss:0.0038128556663994057\n",
            "train loss:0.05059758891060548\n",
            "train loss:0.004040288936163535\n",
            "train loss:0.00014028929466806768\n",
            "train loss:0.0016782448202676769\n",
            "train loss:0.00014817494111549055\n",
            "train loss:0.0013697792831057853\n",
            "train loss:0.003075091994527913\n",
            "train loss:0.0013273043309103743\n",
            "train loss:0.0013944273050828196\n",
            "train loss:0.003315946212248689\n",
            "train loss:0.0014711990311878308\n",
            "train loss:0.00038054121706176204\n",
            "train loss:0.0018837980521570993\n",
            "train loss:0.00034676864327733083\n",
            "train loss:0.00889493610974575\n",
            "train loss:0.0011037005979960177\n",
            "train loss:0.00016688613003878585\n",
            "train loss:0.0016853344380187171\n",
            "train loss:0.0006791821592113126\n",
            "train loss:0.00032033608811324403\n",
            "train loss:0.00030933206116155737\n",
            "train loss:0.0028335874498801577\n",
            "train loss:0.0008318210374936553\n",
            "train loss:0.0005039583350150172\n",
            "train loss:0.00010779516274453814\n",
            "train loss:0.0027209378353105777\n",
            "train loss:0.00013241669375670025\n",
            "train loss:0.0011692105147746053\n",
            "train loss:0.0003311295416243711\n",
            "train loss:0.0018606089202229108\n",
            "=== epoch:18, train acc:0.997, test acc:0.988 ===\n",
            "train loss:0.001689801101020419\n",
            "train loss:0.0001517899499211912\n",
            "train loss:0.0002426423243203753\n",
            "train loss:0.0019888964050628843\n",
            "train loss:4.062490903402488e-05\n",
            "train loss:0.00032389473338702166\n",
            "train loss:0.00016823131027792954\n",
            "train loss:0.001388736581586129\n",
            "train loss:0.0006086132487381056\n",
            "train loss:0.0021184563779761897\n",
            "train loss:0.002195687377816447\n",
            "train loss:0.0004989646617804402\n",
            "train loss:0.001685517445164365\n",
            "train loss:0.0012342013221241066\n",
            "train loss:0.03498454748447772\n",
            "train loss:0.0034005705333925045\n",
            "train loss:0.0014998898493098242\n",
            "train loss:0.0029940035676415244\n",
            "train loss:0.0004135000157942695\n",
            "train loss:0.0006103127040802761\n",
            "train loss:0.0015601420999939606\n",
            "train loss:0.0003336930565517045\n",
            "train loss:0.0006504751639467328\n",
            "train loss:7.071091489304214e-05\n",
            "train loss:0.004402540154885495\n",
            "train loss:0.0010460433472780678\n",
            "train loss:0.000126733264690791\n",
            "train loss:0.0003101781673292911\n",
            "train loss:0.004098062100312263\n",
            "train loss:0.0005857064505327648\n",
            "train loss:0.0015167976041093006\n",
            "train loss:0.004414636451679112\n",
            "train loss:0.0015369489823142503\n",
            "train loss:0.0022679205841387674\n",
            "train loss:9.69647587035612e-05\n",
            "train loss:0.00040987713508850755\n",
            "train loss:0.0011562351257248691\n",
            "train loss:0.0017181832415426143\n",
            "train loss:0.0040565924361304325\n",
            "train loss:0.00031219412137994376\n",
            "train loss:0.003249281154176968\n",
            "train loss:0.0022243385382577727\n",
            "train loss:0.0020014598907976836\n",
            "train loss:0.0033952716647930275\n",
            "train loss:0.00221953953964891\n",
            "train loss:0.00045295446458596837\n",
            "train loss:0.002367330782474947\n",
            "train loss:0.004864166397151781\n",
            "train loss:0.00019997515618642387\n",
            "train loss:0.00025594980736878923\n",
            "train loss:0.0001132285665765306\n",
            "train loss:0.0033756942041324165\n",
            "train loss:0.000586050253854336\n",
            "train loss:0.006683837067670848\n",
            "train loss:0.0012481003259117267\n",
            "train loss:0.03052409911224864\n",
            "train loss:0.0011018254166861759\n",
            "train loss:0.004863076453460344\n",
            "train loss:0.0021428790212230093\n",
            "train loss:0.0008636571199035428\n",
            "train loss:7.094681608203102e-05\n",
            "train loss:0.001989512665692121\n",
            "train loss:0.000578077568641065\n",
            "train loss:0.0008394823591865085\n",
            "train loss:0.0034637240042042017\n",
            "train loss:0.0021001345825995494\n",
            "train loss:0.0003460822550556869\n",
            "train loss:0.001173655270123368\n",
            "train loss:0.003908858535783432\n",
            "train loss:0.0008403363434891921\n",
            "train loss:0.0017343441103378824\n",
            "train loss:0.0025990267817848555\n",
            "train loss:0.0002918533315406601\n",
            "train loss:0.0005616647111753923\n",
            "train loss:0.0006804064119165543\n",
            "train loss:0.0014376545377260744\n",
            "train loss:0.00409195113442342\n",
            "train loss:0.005379554879782818\n",
            "train loss:0.0003754107786135491\n",
            "train loss:0.001122483975860862\n",
            "train loss:0.006052174397809446\n",
            "train loss:0.00046302370650517225\n",
            "train loss:0.0007981939474328314\n",
            "train loss:0.004058292471157318\n",
            "train loss:0.0007093629703338178\n",
            "train loss:0.0008542970145413352\n",
            "train loss:0.004118146452047911\n",
            "train loss:0.00021039526565539598\n",
            "train loss:0.0005227100818363718\n",
            "train loss:0.02680059123496791\n",
            "train loss:6.0735508789038495e-05\n",
            "train loss:0.00015088568696349793\n",
            "train loss:0.0005370580947106783\n",
            "train loss:0.0045414536007959165\n",
            "train loss:0.0016100462073690546\n",
            "train loss:0.0010285341198755507\n",
            "train loss:0.0006939910916382293\n",
            "train loss:0.0005181953310019939\n",
            "train loss:0.0004774389808039531\n",
            "train loss:0.002437211859621735\n",
            "train loss:0.0019150972189839702\n",
            "train loss:0.0002467711060774265\n",
            "train loss:0.001681121494480389\n",
            "train loss:0.001597446432670077\n",
            "train loss:0.00046703504240297125\n",
            "train loss:0.0003500425340454382\n",
            "train loss:0.00024110356878196724\n",
            "train loss:0.003537839689494159\n",
            "train loss:0.0005029550558801894\n",
            "train loss:0.00035827388838719794\n",
            "train loss:0.0006720212999239258\n",
            "train loss:0.003993978926417569\n",
            "train loss:0.00040889059407953434\n",
            "train loss:0.002728500014658012\n",
            "train loss:0.00011092794705312084\n",
            "train loss:0.0014526187750207734\n",
            "train loss:0.0010907165868907843\n",
            "train loss:0.00035447283546178476\n",
            "train loss:0.0016402593523605795\n",
            "train loss:0.0008815258106152894\n",
            "train loss:0.0008829477722777348\n",
            "train loss:0.0006285799146547069\n",
            "train loss:0.0006012275251099416\n",
            "train loss:0.0012502469062461745\n",
            "train loss:0.00020535820165997135\n",
            "train loss:0.0023608564183997616\n",
            "train loss:0.0006294906335224497\n",
            "train loss:0.004087367634406789\n",
            "train loss:0.001377241940658288\n",
            "train loss:0.0013030901587432503\n",
            "train loss:0.008121493118217285\n",
            "train loss:0.0025899124261279543\n",
            "train loss:0.001735408022241886\n",
            "train loss:0.0019849585378368313\n",
            "train loss:0.0009228434552564897\n",
            "train loss:0.00046228739658386175\n",
            "train loss:0.0004985686487217716\n",
            "train loss:0.0006126701395379719\n",
            "train loss:0.0007901274033831683\n",
            "train loss:0.0007357894294381596\n",
            "train loss:0.002778901339566268\n",
            "train loss:0.0009004473959045459\n",
            "train loss:0.0012105972860365747\n",
            "train loss:0.00022490585499815435\n",
            "train loss:0.0023733912739891884\n",
            "train loss:0.0033398455596449004\n",
            "train loss:0.002950379735890302\n",
            "train loss:0.0016255743629380276\n",
            "train loss:0.001342530010681821\n",
            "train loss:0.011645902090457915\n",
            "train loss:0.0009771097638307904\n",
            "train loss:0.00012431142021773165\n",
            "train loss:0.000370991397047379\n",
            "train loss:0.0008122501797211946\n",
            "train loss:0.006351805372287762\n",
            "train loss:0.0023057902890970122\n",
            "train loss:0.0006893189688703835\n",
            "train loss:0.0003036104273736709\n",
            "train loss:0.0003849595005914136\n",
            "train loss:5.210478220607767e-05\n",
            "train loss:0.0006199205965147578\n",
            "train loss:0.0004750948964832349\n",
            "train loss:0.000813454925562796\n",
            "train loss:0.005819051427533919\n",
            "train loss:0.010814829657405727\n",
            "train loss:0.007348981363193868\n",
            "train loss:0.004860937585292507\n",
            "train loss:0.0005008578195300976\n",
            "train loss:0.00010125426504938015\n",
            "train loss:0.0019543847457467304\n",
            "train loss:0.00025873178583983645\n",
            "train loss:0.0002798305785534313\n",
            "train loss:0.00023563554658290263\n",
            "train loss:0.00014360110393829025\n",
            "train loss:0.0008014813728770807\n",
            "train loss:0.0002919208241149571\n",
            "train loss:0.0007065304407747071\n",
            "train loss:0.00012071528168696985\n",
            "train loss:0.0009079264119018714\n",
            "train loss:0.001223526314514174\n",
            "train loss:7.10324776734092e-05\n",
            "train loss:0.004461099986247271\n",
            "train loss:0.0009243312703621754\n",
            "train loss:0.002707979980188113\n",
            "train loss:0.0025871642631403575\n",
            "train loss:0.0009484174590646496\n",
            "train loss:0.0011233047495045104\n",
            "train loss:0.000340082349534346\n",
            "train loss:0.003144135236906201\n",
            "train loss:6.0534284769843564e-05\n",
            "train loss:0.005507071195997998\n",
            "train loss:8.869932717337425e-05\n",
            "train loss:0.0015361970164208213\n",
            "train loss:0.001418565770022575\n",
            "train loss:0.0033024656640293915\n",
            "train loss:9.05746243352554e-05\n",
            "train loss:0.0006835121548158023\n",
            "train loss:0.0026312194533465293\n",
            "train loss:0.0036629547511089987\n",
            "train loss:6.118664722391215e-05\n",
            "train loss:0.002695023921948263\n",
            "train loss:2.3211713271504724e-05\n",
            "train loss:0.00280800885534491\n",
            "train loss:0.0008244184348252448\n",
            "train loss:0.004068314401627799\n",
            "train loss:0.0011137378778937263\n",
            "train loss:4.4553244818963345e-05\n",
            "train loss:0.002068058209207345\n",
            "train loss:0.0007690682496346465\n",
            "train loss:0.004017849901702839\n",
            "train loss:0.0014806701774775965\n",
            "train loss:0.000195866634021675\n",
            "train loss:0.0003058576736868729\n",
            "train loss:0.0017466227334557588\n",
            "train loss:3.848362274991002e-05\n",
            "train loss:4.4823383923298725e-05\n",
            "train loss:0.0004249954786222937\n",
            "train loss:0.0014842272264001192\n",
            "train loss:0.0003006227092614781\n",
            "train loss:0.0003396901259829663\n",
            "train loss:0.0028710787387562354\n",
            "train loss:0.00027255230812605355\n",
            "train loss:0.003254702956550979\n",
            "train loss:2.944955368588005e-05\n",
            "train loss:0.0010961964096614619\n",
            "train loss:9.57601788331288e-05\n",
            "train loss:0.001336902020001788\n",
            "train loss:0.0019885957573263424\n",
            "train loss:0.0014092310835530464\n",
            "train loss:0.003935765845452213\n",
            "train loss:0.0007310439815774545\n",
            "train loss:0.002819769153914507\n",
            "train loss:0.0001425722502183559\n",
            "train loss:0.0009029753545793645\n",
            "train loss:0.002289408698867319\n",
            "train loss:0.025707328105301915\n",
            "train loss:0.0007665452384510764\n",
            "train loss:0.0026115919997936983\n",
            "train loss:0.0037387250437502894\n",
            "train loss:0.002393395186669466\n",
            "train loss:0.00013654417967946098\n",
            "train loss:0.0011761116986533297\n",
            "train loss:0.00039551159566335336\n",
            "train loss:0.0033433384706150608\n",
            "train loss:0.003093157038407364\n",
            "train loss:0.006951382906277313\n",
            "train loss:0.0033846096126052866\n",
            "train loss:0.0023953761106015638\n",
            "train loss:0.0008355791838632917\n",
            "train loss:0.0003166890598327171\n",
            "train loss:0.004384887463522212\n",
            "train loss:0.002613017557031298\n",
            "train loss:0.0030750183184338397\n",
            "train loss:0.002023027937875255\n",
            "train loss:0.0012049830718627206\n",
            "train loss:0.002205440234719305\n",
            "train loss:0.0001469772930215571\n",
            "train loss:0.0007345071863088537\n",
            "train loss:0.0006453612666755136\n",
            "train loss:0.008342100899353926\n",
            "train loss:0.0005315372932747388\n",
            "train loss:0.0022162004370708437\n",
            "train loss:0.0003804535474961403\n",
            "train loss:0.004934339551891648\n",
            "train loss:0.014999540211691267\n",
            "train loss:0.00039053919938247355\n",
            "train loss:0.0005875139329868175\n",
            "train loss:0.0007538414611770207\n",
            "train loss:4.743683787328533e-05\n",
            "train loss:0.018821693480270466\n",
            "train loss:0.0005391864584039747\n",
            "train loss:0.00012518890933980732\n",
            "train loss:0.005639827000483702\n",
            "train loss:0.0002656354710583507\n",
            "train loss:0.0019863868850777014\n",
            "train loss:0.00024817403399757297\n",
            "train loss:0.0011525809889415344\n",
            "train loss:0.001193644007675217\n",
            "train loss:0.04884358262399885\n",
            "train loss:0.0035981435587657674\n",
            "train loss:0.00011211852043931359\n",
            "train loss:0.0011599738009021561\n",
            "train loss:0.001379047399468652\n",
            "train loss:0.001336547710500834\n",
            "train loss:0.008810472255137959\n",
            "train loss:0.003079864938901303\n",
            "train loss:0.004166762984826119\n",
            "train loss:0.0036725834773892623\n",
            "train loss:0.0011375883619897701\n",
            "train loss:0.0026987677273397938\n",
            "train loss:0.00021289437591840078\n",
            "train loss:0.0052642319334234725\n",
            "train loss:0.0013587855009489294\n",
            "train loss:0.0014229908538480412\n",
            "train loss:0.0012597643733107003\n",
            "train loss:3.3141021813216455e-05\n",
            "train loss:0.001970929430645109\n",
            "train loss:0.004439521557945552\n",
            "train loss:0.0021091326141915545\n",
            "train loss:0.0034522993133881013\n",
            "train loss:0.0007559413382038089\n",
            "train loss:0.0020627918013748146\n",
            "train loss:0.0015062718487485007\n",
            "train loss:0.0018107635004376238\n",
            "train loss:0.0011093062788302274\n",
            "train loss:0.0003009324597173092\n",
            "train loss:0.00011936697028328791\n",
            "train loss:2.2045311094998263e-05\n",
            "train loss:0.0004153009142189356\n",
            "train loss:0.0001870246389673473\n",
            "train loss:0.0019930872420517976\n",
            "train loss:0.002653101712716638\n",
            "train loss:0.0023806495181073308\n",
            "train loss:0.000738787353532898\n",
            "train loss:0.0010818535037279567\n",
            "train loss:0.0011497074667920358\n",
            "train loss:0.016896595937989475\n",
            "train loss:0.0005396358731158224\n",
            "train loss:0.00012251056182122788\n",
            "train loss:0.002510122264663842\n",
            "train loss:0.0004731717341302293\n",
            "train loss:6.0538483920394725e-05\n",
            "train loss:0.0009207661985263512\n",
            "train loss:0.0015184253395350957\n",
            "train loss:0.0021839554060087895\n",
            "train loss:0.0019287699339927738\n",
            "train loss:0.0030977531914331753\n",
            "train loss:9.38730546007164e-05\n",
            "train loss:9.019481723811365e-05\n",
            "train loss:1.3371353600523895e-05\n",
            "train loss:0.0015636254309354866\n",
            "train loss:0.0008683164461399477\n",
            "train loss:3.3083262840849354e-05\n",
            "train loss:0.005253277195388413\n",
            "train loss:0.0007714666037065286\n",
            "train loss:9.162008352530159e-05\n",
            "train loss:0.001731071367091893\n",
            "train loss:0.0007287630026485163\n",
            "train loss:0.00037341671814553165\n",
            "train loss:0.003606072722106155\n",
            "train loss:0.00028043231991038594\n",
            "train loss:0.0056934405769568696\n",
            "train loss:0.0007620997134396093\n",
            "train loss:0.0006997902577731546\n",
            "train loss:0.0008994990353253611\n",
            "train loss:9.743976857891773e-05\n",
            "train loss:0.0003804475228868462\n",
            "train loss:0.00041717232408358847\n",
            "train loss:0.0008938770426946445\n",
            "train loss:0.0020068031687834977\n",
            "train loss:0.0006451972874324918\n",
            "train loss:0.0006968783051555794\n",
            "train loss:0.004858924471324605\n",
            "train loss:0.00043418548044891217\n",
            "train loss:7.392244550664342e-05\n",
            "train loss:0.003887105147021096\n",
            "train loss:0.00027809505279876104\n",
            "train loss:0.0015796691523995122\n",
            "train loss:0.00012021763466281562\n",
            "train loss:0.004332427244935127\n",
            "train loss:5.9013265109770656e-05\n",
            "train loss:0.001533819058754664\n",
            "train loss:0.004197761784058883\n",
            "train loss:0.0008252239818902188\n",
            "train loss:0.00010514792812982388\n",
            "train loss:0.0013968449422340845\n",
            "train loss:0.00044020693203922705\n",
            "train loss:0.0001759322935586243\n",
            "train loss:0.00034337057925133083\n",
            "train loss:0.0006273145773443392\n",
            "train loss:0.0024067478117553356\n",
            "train loss:0.00037410991336135754\n",
            "train loss:7.08733626491571e-05\n",
            "train loss:0.00018484864887371972\n",
            "train loss:0.0003556086682734803\n",
            "train loss:0.00022472490558870923\n",
            "train loss:0.00016431601005307997\n",
            "train loss:0.0012103006640333451\n",
            "train loss:0.00027001244103913477\n",
            "train loss:0.0015058238071336883\n",
            "train loss:9.267727623042422e-05\n",
            "train loss:0.0003385751284057726\n",
            "train loss:0.0019352277292020278\n",
            "train loss:0.0004503957172148718\n",
            "train loss:0.0003579634895772246\n",
            "train loss:0.005251519998581088\n",
            "train loss:0.0008652005415057268\n",
            "train loss:0.0009046412683006736\n",
            "train loss:0.00026463080266512064\n",
            "train loss:0.001725178185692223\n",
            "train loss:0.0016872705314332302\n",
            "train loss:0.00027093713318765346\n",
            "train loss:0.0035894027181515133\n",
            "train loss:0.005698941295769473\n",
            "train loss:0.0012307272961098851\n",
            "train loss:0.0008544592416849897\n",
            "train loss:0.0003571260465612514\n",
            "train loss:0.020323284533411853\n",
            "train loss:0.0007028522738839449\n",
            "train loss:0.0022855767797065286\n",
            "train loss:0.0007830641508388209\n",
            "train loss:0.0008353310808348064\n",
            "train loss:0.0009864988795153253\n",
            "train loss:0.0004472007484779431\n",
            "train loss:0.003013266257225195\n",
            "train loss:0.00011353240768214871\n",
            "train loss:0.00025491369659077774\n",
            "train loss:0.0008249930113793938\n",
            "train loss:0.0019648845057248445\n",
            "train loss:0.0009007885833557021\n",
            "train loss:0.0029666751669746015\n",
            "train loss:0.0032399593864387106\n",
            "train loss:0.00017616707641991876\n",
            "train loss:0.0018712174058886871\n",
            "train loss:0.0025864291680529027\n",
            "train loss:0.0014969983756252597\n",
            "train loss:3.435878055110368e-05\n",
            "train loss:0.0003642755868089709\n",
            "train loss:0.00042542789475123565\n",
            "train loss:0.002140440129147978\n",
            "train loss:0.0001591089744913371\n",
            "train loss:0.0012145946523214686\n",
            "train loss:0.00022390906128405945\n",
            "train loss:0.0012661761035727334\n",
            "train loss:0.0003635370806263042\n",
            "train loss:0.002668926013203435\n",
            "train loss:0.0005022382111926941\n",
            "train loss:0.0012257915245076343\n",
            "train loss:0.00018667927432457722\n",
            "train loss:0.0026778449139051976\n",
            "train loss:0.0008482844475107793\n",
            "train loss:0.00015205147066294368\n",
            "train loss:0.0005871210636156497\n",
            "train loss:0.0007617683263858811\n",
            "train loss:0.0008035282566516604\n",
            "train loss:0.0002618807625707229\n",
            "train loss:0.0007523268871704086\n",
            "train loss:4.843690462002984e-05\n",
            "train loss:7.729850336758773e-05\n",
            "train loss:0.0012016443658043883\n",
            "train loss:0.00551139180569158\n",
            "train loss:0.0004398231970050273\n",
            "train loss:4.1746524081077136e-05\n",
            "train loss:0.00020897915514332712\n",
            "train loss:8.710481293299392e-05\n",
            "train loss:0.00013385625208198928\n",
            "train loss:0.0003104850199440318\n",
            "train loss:0.00020746108218880607\n",
            "train loss:0.002156300956437663\n",
            "train loss:0.00025921407032342543\n",
            "train loss:0.0008261364938783352\n",
            "train loss:0.002255307172649538\n",
            "train loss:0.0011688992826474403\n",
            "train loss:0.0009567227656147195\n",
            "train loss:0.0004368685757804805\n",
            "train loss:0.00014325411199125554\n",
            "train loss:2.104520673579278e-05\n",
            "train loss:0.0029500031719697927\n",
            "train loss:0.00029219092059319185\n",
            "train loss:0.00035279656565795196\n",
            "train loss:0.00037624745697038435\n",
            "train loss:0.00044824507675731795\n",
            "train loss:0.00022480996534997736\n",
            "train loss:0.002668887199470175\n",
            "train loss:0.00027082821320138567\n",
            "train loss:0.0009033090921656771\n",
            "train loss:0.002095488831286198\n",
            "train loss:0.00027485984975181315\n",
            "train loss:0.003936697364118245\n",
            "train loss:0.0001056846743631305\n",
            "train loss:0.00027202697375542647\n",
            "train loss:6.109463922205208e-05\n",
            "train loss:6.0385994112838176e-05\n",
            "train loss:0.0007248941277254578\n",
            "train loss:0.004716540273631338\n",
            "train loss:0.003266624589693695\n",
            "train loss:0.003478832125738236\n",
            "train loss:0.0003818676832738267\n",
            "train loss:0.0007816262871922996\n",
            "train loss:0.0013923047716969646\n",
            "train loss:0.0002596270327929974\n",
            "train loss:0.001431567522512956\n",
            "train loss:0.00016430766133755867\n",
            "train loss:0.0006491334797257885\n",
            "train loss:0.000100286724453242\n",
            "train loss:0.0016337091951587265\n",
            "train loss:0.0006028397006423629\n",
            "train loss:0.025366806204061378\n",
            "train loss:0.0006490277547701094\n",
            "train loss:0.0010069806039100264\n",
            "train loss:0.0028534213812350974\n",
            "train loss:0.0008663969642474624\n",
            "train loss:0.001572932395128316\n",
            "train loss:0.0023762145870461298\n",
            "train loss:0.0035375357433059464\n",
            "train loss:0.00025485365247776933\n",
            "train loss:0.011581000640660937\n",
            "train loss:0.0018120590696056246\n",
            "train loss:0.0006966000532640259\n",
            "train loss:0.0023907424857074655\n",
            "train loss:0.0010070115983416548\n",
            "train loss:0.00163211665760105\n",
            "train loss:0.0001610069425718154\n",
            "train loss:0.0011712565247307667\n",
            "train loss:0.00919815326188041\n",
            "train loss:0.004192159961299293\n",
            "train loss:0.002477279161289933\n",
            "train loss:0.001011507819213607\n",
            "train loss:0.0006995767588001719\n",
            "train loss:0.0024435726811057757\n",
            "train loss:0.0004476265190049106\n",
            "train loss:0.00042680544327848764\n",
            "train loss:0.00556714737315029\n",
            "train loss:0.00028596017844969895\n",
            "train loss:0.008732251416482404\n",
            "train loss:0.0022981116194650073\n",
            "train loss:0.0016549339682153295\n",
            "train loss:0.0004801923234935267\n",
            "train loss:0.0018265542791514214\n",
            "train loss:0.002202404781434774\n",
            "train loss:0.00020959952743758692\n",
            "train loss:0.002436415978869471\n",
            "train loss:0.0025320955103547856\n",
            "train loss:0.0233982734785031\n",
            "train loss:0.004357531208519369\n",
            "train loss:0.0006139731536836461\n",
            "train loss:0.00044705883841124144\n",
            "train loss:0.0008642653697949091\n",
            "train loss:0.0031250561847013373\n",
            "train loss:0.0014622409213750015\n",
            "train loss:0.0038059106814389678\n",
            "train loss:0.001100176143275235\n",
            "train loss:8.735826294309683e-05\n",
            "train loss:0.0008005978527815069\n",
            "train loss:0.0036820136996019753\n",
            "train loss:0.004232520438781483\n",
            "train loss:0.0012774270548227711\n",
            "train loss:0.003977296610410336\n",
            "train loss:0.0015526615149071168\n",
            "train loss:0.010031157863699718\n",
            "train loss:0.000380997416633807\n",
            "train loss:0.008333708396438787\n",
            "train loss:0.0010833357047963959\n",
            "train loss:0.00038368588556526954\n",
            "train loss:0.0029689473465492194\n",
            "train loss:0.003970637790234922\n",
            "train loss:0.00034475437877642056\n",
            "train loss:0.0023605738462525176\n",
            "train loss:0.006142968547060208\n",
            "train loss:0.0007491853012349065\n",
            "train loss:0.0013257748476237854\n",
            "train loss:0.0008584344981577859\n",
            "train loss:0.0005451313962444775\n",
            "train loss:0.0010608459304829237\n",
            "train loss:0.0028361517698293815\n",
            "train loss:0.0013973607690236241\n",
            "train loss:0.020337016462406283\n",
            "train loss:0.0008003897475209852\n",
            "train loss:0.0003934809092260513\n",
            "train loss:0.0008098714853092999\n",
            "train loss:0.0033859235169268363\n",
            "train loss:4.350862457757615e-05\n",
            "train loss:0.0007726894891077777\n",
            "train loss:0.0005683927015527765\n",
            "train loss:0.0004076873361400712\n",
            "train loss:0.000571543927420064\n",
            "train loss:0.00045746853781653137\n",
            "train loss:0.0005606066943075425\n",
            "train loss:0.0035197517364117504\n",
            "train loss:0.00022397074634914371\n",
            "train loss:0.0006258954896148296\n",
            "train loss:0.0017784771306590301\n",
            "train loss:0.002832310598762024\n",
            "train loss:0.00022482741191823943\n",
            "train loss:0.00012817695652196661\n",
            "train loss:0.0022387702290045837\n",
            "train loss:0.0018031575403819802\n",
            "train loss:0.0005296971963481272\n",
            "train loss:0.000510169829620516\n",
            "train loss:0.0004915362691328102\n",
            "train loss:0.0012816990018345727\n",
            "train loss:0.0007998069980685169\n",
            "train loss:0.0025459437883828557\n",
            "train loss:0.0008828790013578238\n",
            "train loss:0.0007264154700205835\n",
            "train loss:0.00013812129117578355\n",
            "train loss:0.002475076841166074\n",
            "train loss:0.0009709897755668965\n",
            "train loss:0.0003020017016913521\n",
            "train loss:0.00021986062917067007\n",
            "train loss:0.000298941157926071\n",
            "train loss:0.006790551203786736\n",
            "train loss:0.001357846186998826\n",
            "train loss:0.002498207894101624\n",
            "train loss:0.0028154646214854718\n",
            "train loss:0.000690075414227901\n",
            "train loss:0.0009439339875009397\n",
            "train loss:0.002971259116959178\n",
            "train loss:0.000267799805321457\n",
            "train loss:0.0009832171521412007\n",
            "=== epoch:19, train acc:0.998, test acc:0.987 ===\n",
            "train loss:0.0030272990888874\n",
            "train loss:0.001359853107640464\n",
            "train loss:0.00041280288572434883\n",
            "train loss:0.001371007128982155\n",
            "train loss:0.00017174043295523677\n",
            "train loss:0.001023983881806905\n",
            "train loss:5.499575494037991e-05\n",
            "train loss:0.0009978933848903036\n",
            "train loss:0.0010777683807955227\n",
            "train loss:0.00019816561165521507\n",
            "train loss:0.00011310897870439119\n",
            "train loss:5.6265976966707875e-05\n",
            "train loss:0.0026659782269424144\n",
            "train loss:0.0003472390931523449\n",
            "train loss:0.00016123639890265755\n",
            "train loss:0.002556545908676449\n",
            "train loss:0.0005166534323909474\n",
            "train loss:0.001399835290466281\n",
            "train loss:6.893772419585409e-05\n",
            "train loss:0.0006158549086967306\n",
            "train loss:0.002464206333614639\n",
            "train loss:0.00040852902515974953\n",
            "train loss:0.0021906561668433287\n",
            "train loss:0.00019199195943063596\n",
            "train loss:0.001771540266951088\n",
            "train loss:0.0003114928461212665\n",
            "train loss:0.0003949128455160934\n",
            "train loss:0.0026598453644676974\n",
            "train loss:0.000823000542890682\n",
            "train loss:0.0017062356013430513\n",
            "train loss:0.0046899831175197535\n",
            "train loss:0.00027573157665370746\n",
            "train loss:0.00030282869127249676\n",
            "train loss:8.065833102791813e-05\n",
            "train loss:0.0012241611876941069\n",
            "train loss:0.008127043299273336\n",
            "train loss:0.0003017477216045175\n",
            "train loss:0.0021188774077513387\n",
            "train loss:3.186004764232354e-05\n",
            "train loss:0.0030693854019842944\n",
            "train loss:0.0014176055975111338\n",
            "train loss:0.001354625930875739\n",
            "train loss:5.442320231885491e-05\n",
            "train loss:0.0011664332774769927\n",
            "train loss:0.0028507911756761894\n",
            "train loss:0.0004271309358432511\n",
            "train loss:0.015015779843678584\n",
            "train loss:0.00030209282943713915\n",
            "train loss:0.000691396597933167\n",
            "train loss:0.0033372670968472985\n",
            "train loss:0.002750900700327588\n",
            "train loss:0.0018477991372188615\n",
            "train loss:4.359123876298604e-05\n",
            "train loss:0.002198089080626619\n",
            "train loss:0.0021771426242628276\n",
            "train loss:0.00033349816770842066\n",
            "train loss:0.0033149100330199326\n",
            "train loss:0.0004456730675724314\n",
            "train loss:0.00012257623326602377\n",
            "train loss:0.0025496564514717163\n",
            "train loss:0.0009770144781465916\n",
            "train loss:0.00040105646418400656\n",
            "train loss:0.002951945351893368\n",
            "train loss:0.0002083210605280944\n",
            "train loss:0.0012085290663441163\n",
            "train loss:0.023917415068456724\n",
            "train loss:0.00022977221901660354\n",
            "train loss:0.0008742433353235399\n",
            "train loss:0.002695310996143867\n",
            "train loss:0.004657357270830709\n",
            "train loss:0.0008786425774579082\n",
            "train loss:0.0022077536459240966\n",
            "train loss:0.005949026740395072\n",
            "train loss:0.0002922237318173605\n",
            "train loss:0.0011639663532422683\n",
            "train loss:0.00012650250896163802\n",
            "train loss:0.0006597688523425481\n",
            "train loss:0.0008519593662537312\n",
            "train loss:0.0015983660108226713\n",
            "train loss:0.0014527696919442489\n",
            "train loss:0.0003373218072924334\n",
            "train loss:0.0011174709838854121\n",
            "train loss:0.008710657127475931\n",
            "train loss:0.00018284952684334365\n",
            "train loss:0.017436326236660013\n",
            "train loss:0.0007067408613012632\n",
            "train loss:0.00037386492154965924\n",
            "train loss:0.0011312777049974356\n",
            "train loss:0.00722311970927464\n",
            "train loss:0.0030566426239150048\n",
            "train loss:0.0006503818011229528\n",
            "train loss:6.039754592809992e-05\n",
            "train loss:0.012437964120930093\n",
            "train loss:8.946908683007542e-05\n",
            "train loss:0.001999986202171437\n",
            "train loss:0.0006099593333490226\n",
            "train loss:0.03311007215028633\n",
            "train loss:0.0425907662829491\n",
            "train loss:0.0005079017254326697\n",
            "train loss:0.01065817974417689\n",
            "train loss:0.0005104438466382223\n",
            "train loss:0.004449744088179283\n",
            "train loss:0.0021174113592800758\n",
            "train loss:0.0013029515069724635\n",
            "train loss:0.04641316279763892\n",
            "train loss:0.012541058198567202\n",
            "train loss:0.003293055025702695\n",
            "train loss:1.6045713591159083e-05\n",
            "train loss:0.0008115202338369336\n",
            "train loss:0.0016604666794062777\n",
            "train loss:0.001335452510720249\n",
            "train loss:0.005763410756236559\n",
            "train loss:0.0016601255713367352\n",
            "train loss:0.015451579762109731\n",
            "train loss:0.0021642930819775518\n",
            "train loss:0.0017599541206440878\n",
            "train loss:0.015653774171924238\n",
            "train loss:0.002959324861758616\n",
            "train loss:0.00015830218277478603\n",
            "train loss:0.010287787960162397\n",
            "train loss:0.0020595095025355823\n",
            "train loss:0.0004705871687079227\n",
            "train loss:0.0018590232939352553\n",
            "train loss:0.0009002257361472631\n",
            "train loss:0.007338657939659542\n",
            "train loss:0.00025245275460202704\n",
            "train loss:0.005761472531340149\n",
            "train loss:0.0013271982141688271\n",
            "train loss:0.0010371456515586694\n",
            "train loss:0.0005980737472407735\n",
            "train loss:0.00011075105632500167\n",
            "train loss:0.0014676776883581803\n",
            "train loss:0.00032495829134579116\n",
            "train loss:0.01400714496089581\n",
            "train loss:0.007243072871653017\n",
            "train loss:0.0098316090516173\n",
            "train loss:0.0002533539380728929\n",
            "train loss:0.0037996134868851796\n",
            "train loss:9.507466449386712e-05\n",
            "train loss:0.005243724456185821\n",
            "train loss:0.004519282532158239\n",
            "train loss:0.0005680579615029964\n",
            "train loss:0.0037041244195585837\n",
            "train loss:0.0008087897313310566\n",
            "train loss:0.021001253452182976\n",
            "train loss:0.000906398278941783\n",
            "train loss:0.00040221652421459675\n",
            "train loss:6.483859420153627e-05\n",
            "train loss:0.0019559154095749828\n",
            "train loss:0.0019782675234810147\n",
            "train loss:0.0003763831116390236\n",
            "train loss:0.0004291426921295198\n",
            "train loss:0.004477757427143115\n",
            "train loss:0.00044752366610812446\n",
            "train loss:1.776888544580154e-05\n",
            "train loss:0.0028881019359907768\n",
            "train loss:0.00012166976041514599\n",
            "train loss:0.00027930642076126594\n",
            "train loss:0.0030249818929910877\n",
            "train loss:0.003381523489804209\n",
            "train loss:0.0008492254204783546\n",
            "train loss:0.0003897354400731047\n",
            "train loss:9.028086579625981e-05\n",
            "train loss:0.004039287045665449\n",
            "train loss:0.0026149562977068213\n",
            "train loss:0.0004786363951696474\n",
            "train loss:0.0003770648194850918\n",
            "train loss:0.0030120872037073193\n",
            "train loss:0.0006131993403153923\n",
            "train loss:0.0004632489462468964\n",
            "train loss:0.001841660295800292\n",
            "train loss:0.0012946170498158596\n",
            "train loss:0.002441875808082572\n",
            "train loss:0.0038906421788436575\n",
            "train loss:0.004785718585689803\n",
            "train loss:0.00038617270416173473\n",
            "train loss:0.0017269538506593269\n",
            "train loss:0.007790525423884266\n",
            "train loss:0.002008029459914425\n",
            "train loss:0.0012264934175813319\n",
            "train loss:4.1569057489009255e-05\n",
            "train loss:0.0007497659483237365\n",
            "train loss:0.0014301669875481388\n",
            "train loss:0.0010544991743643222\n",
            "train loss:0.005313071413095161\n",
            "train loss:0.0002763662771489201\n",
            "train loss:0.0007794994458434308\n",
            "train loss:0.005257362162230117\n",
            "train loss:0.008908038811702474\n",
            "train loss:0.0014139034575737972\n",
            "train loss:0.000493742185247802\n",
            "train loss:0.004391611816401743\n",
            "train loss:5.7600221076768326e-05\n",
            "train loss:0.001224478006310968\n",
            "train loss:0.0002881499123043973\n",
            "train loss:0.0001516015888822202\n",
            "train loss:0.0034827059469413855\n",
            "train loss:0.0007808319134800369\n",
            "train loss:0.0024287623366962777\n",
            "train loss:0.001498920016764637\n",
            "train loss:0.00017754569283972448\n",
            "train loss:0.0009165705835234455\n",
            "train loss:0.0015415104837372038\n",
            "train loss:0.00045149539617026657\n",
            "train loss:0.0008128673385535819\n",
            "train loss:0.0003598145111889282\n",
            "train loss:0.0013208531847283267\n",
            "train loss:0.007648238697407446\n",
            "train loss:0.00030940213406364063\n",
            "train loss:0.00045801107847303653\n",
            "train loss:0.0019177991213504115\n",
            "train loss:0.0025069299743100755\n",
            "train loss:0.00015286523505768078\n",
            "train loss:0.0007219859783611643\n",
            "train loss:0.000160526187767773\n",
            "train loss:0.004283615464431732\n",
            "train loss:0.0013469162009579069\n",
            "train loss:0.013019708903906399\n",
            "train loss:0.0015217577955710937\n",
            "train loss:0.010463277998146283\n",
            "train loss:0.0005336462641550015\n",
            "train loss:0.0015567019832343721\n",
            "train loss:0.00012952753087937436\n",
            "train loss:0.002179408752933136\n",
            "train loss:0.0005429456795173481\n",
            "train loss:0.001109243160784896\n",
            "train loss:0.0011853296315782614\n",
            "train loss:0.0020141615904462474\n",
            "train loss:0.002165407902107414\n",
            "train loss:0.0010120456288475394\n",
            "train loss:0.0004096909057835005\n",
            "train loss:0.0010324378369112478\n",
            "train loss:0.005727532237218494\n",
            "train loss:0.0010110996513536031\n",
            "train loss:0.0026893077136267794\n",
            "train loss:4.195833486626093e-05\n",
            "train loss:0.0018742076078347518\n",
            "train loss:0.0023689132165258015\n",
            "train loss:0.0007267912663286854\n",
            "train loss:0.00046080740323891576\n",
            "train loss:0.0014059771314507755\n",
            "train loss:0.0006912024645048312\n",
            "train loss:0.0017457612135862897\n",
            "train loss:0.004994309733054025\n",
            "train loss:0.0008840778508600988\n",
            "train loss:0.013843012423783376\n",
            "train loss:0.00014805819125190836\n",
            "train loss:0.0020307953207514226\n",
            "train loss:0.0008088655772294902\n",
            "train loss:0.0005482303420258544\n",
            "train loss:0.00032127539645182573\n",
            "train loss:7.53708805871846e-05\n",
            "train loss:0.006973715500607276\n",
            "train loss:0.001527281380165703\n",
            "train loss:0.000961835362929504\n",
            "train loss:0.002417456805793042\n",
            "train loss:0.0011716674559419378\n",
            "train loss:0.0013900904617990583\n",
            "train loss:0.0017748159772618035\n",
            "train loss:0.0003629815680590662\n",
            "train loss:0.002583919731096877\n",
            "train loss:0.004434577038347242\n",
            "train loss:0.007180349377086599\n",
            "train loss:0.0024331275336407277\n",
            "train loss:0.001817525118890716\n",
            "train loss:0.000151660127184476\n",
            "train loss:0.0007950621113567236\n",
            "train loss:0.00044140898838079904\n",
            "train loss:0.004011148981464925\n",
            "train loss:0.00040198531289318916\n",
            "train loss:0.0019193073751090403\n",
            "train loss:0.0011212495669991412\n",
            "train loss:0.002613372281231094\n",
            "train loss:0.00014318402039684288\n",
            "train loss:0.0011214443041482674\n",
            "train loss:0.000525628329591964\n",
            "train loss:0.0001629306502035465\n",
            "train loss:0.00037427811201385346\n",
            "train loss:0.002928506700963455\n",
            "train loss:0.001014950254257179\n",
            "train loss:0.0007822333812846574\n",
            "train loss:0.0018444733458130073\n",
            "train loss:0.002781339521348421\n",
            "train loss:0.0015481993733161066\n",
            "train loss:0.0005426094926057354\n",
            "train loss:0.0012735104868329087\n",
            "train loss:0.004927161312505726\n",
            "train loss:0.0010621361524938762\n",
            "train loss:0.004934100552519311\n",
            "train loss:0.0004697159449349416\n",
            "train loss:0.0021811899868314543\n",
            "train loss:0.0005179208859114357\n",
            "train loss:0.0019023343599016574\n",
            "train loss:0.0001843603533446104\n",
            "train loss:0.0007617048650899802\n",
            "train loss:0.002020468484286644\n",
            "train loss:0.0005781555549862011\n",
            "train loss:0.00019442245607037553\n",
            "train loss:0.0009684226985125187\n",
            "train loss:0.0004032546567600969\n",
            "train loss:0.0015888839123696116\n",
            "train loss:0.00017425279691539073\n",
            "train loss:0.0033331404270455\n",
            "train loss:0.0011512817508618062\n",
            "train loss:0.000967987133900616\n",
            "train loss:0.0004898391117577911\n",
            "train loss:0.0008018841039334093\n",
            "train loss:0.0014757557614749875\n",
            "train loss:0.00020186236185677355\n",
            "train loss:0.0004272700052501929\n",
            "train loss:0.00040357401967025037\n",
            "train loss:0.00681120948257537\n",
            "train loss:6.934784042307808e-05\n",
            "train loss:8.424939410713497e-05\n",
            "train loss:0.0009253456123490576\n",
            "train loss:0.0018536077376144619\n",
            "train loss:0.004961612073265751\n",
            "train loss:0.0006977633259433477\n",
            "train loss:0.004443834788894322\n",
            "train loss:9.928430341416315e-05\n",
            "train loss:0.00014101012362528832\n",
            "train loss:7.314860898466532e-05\n",
            "train loss:0.0043760185573564125\n",
            "train loss:9.5312960957556e-05\n",
            "train loss:0.00512530401344613\n",
            "train loss:0.0005883085048981026\n",
            "train loss:0.0010034162912432919\n",
            "train loss:0.00103089004198532\n",
            "train loss:0.0016750919928419242\n",
            "train loss:0.0015246607962743088\n",
            "train loss:0.0012556235752836057\n",
            "train loss:0.0006084921028195975\n",
            "train loss:0.0012973100282460638\n",
            "train loss:0.0029674017888920816\n",
            "train loss:0.002758632178606417\n",
            "train loss:0.0032771840004492264\n",
            "train loss:0.001620512357996355\n",
            "train loss:0.0003161047390276071\n",
            "train loss:0.0013366919298232827\n",
            "train loss:0.00017342455773948829\n",
            "train loss:0.001159978485691998\n",
            "train loss:0.0027864472135250627\n",
            "train loss:0.0004360805242265685\n",
            "train loss:0.000325621925059668\n",
            "train loss:0.00040137809307899783\n",
            "train loss:0.0007062328021151558\n",
            "train loss:7.43266624422131e-05\n",
            "train loss:0.00022126119686033616\n",
            "train loss:0.0022984112525857563\n",
            "train loss:0.000854750605266234\n",
            "train loss:0.001058120906627635\n",
            "train loss:0.001266180109523841\n",
            "train loss:0.00047775680615584257\n",
            "train loss:0.002379124421634947\n",
            "train loss:0.004793216495006553\n",
            "train loss:0.00042915006324705163\n",
            "train loss:0.0013889121727980395\n",
            "train loss:0.0004726467929299261\n",
            "train loss:0.00024838049243931256\n",
            "train loss:0.0008279744849799536\n",
            "train loss:0.0018309720550597158\n",
            "train loss:0.0011861561335532998\n",
            "train loss:0.00041535778572980423\n",
            "train loss:0.00012028338478732741\n",
            "train loss:0.0002692778827428964\n",
            "train loss:8.08637573325228e-05\n",
            "train loss:0.0001362466535720967\n",
            "train loss:0.001335223742037287\n",
            "train loss:0.001983601165132396\n",
            "train loss:0.00013079377041867702\n",
            "train loss:0.0002057928449075789\n",
            "train loss:8.594268925594931e-05\n",
            "train loss:0.008459015794809592\n",
            "train loss:0.0010994522942149177\n",
            "train loss:0.0008072775548326386\n",
            "train loss:0.00044807583383676416\n",
            "train loss:0.0014457668296885987\n",
            "train loss:2.6844024607492753e-05\n",
            "train loss:0.0014751488388071454\n",
            "train loss:0.0011439971932498461\n",
            "train loss:0.0018661833285396208\n",
            "train loss:0.00035310088768451624\n",
            "train loss:0.0007996007134480471\n",
            "train loss:0.0013139529139333725\n",
            "train loss:0.003479701923146605\n",
            "train loss:6.790239366240554e-05\n",
            "train loss:0.002568588963100527\n",
            "train loss:0.0004074113751675386\n",
            "train loss:0.01896044983315198\n",
            "train loss:0.0003692416964433881\n",
            "train loss:0.0011645755286636245\n",
            "train loss:0.0026179316089804815\n",
            "train loss:0.00043909411035562016\n",
            "train loss:0.0007744818413750114\n",
            "train loss:0.001509173633644072\n",
            "train loss:0.00019845137008131714\n",
            "train loss:0.0026747188967890935\n",
            "train loss:0.00159158085515648\n",
            "train loss:0.0012666523013439845\n",
            "train loss:0.0002997164799793182\n",
            "train loss:0.0008448843699380658\n",
            "train loss:0.0003139223370542099\n",
            "train loss:0.0019393298108117219\n",
            "train loss:0.0009737537824639022\n",
            "train loss:0.0007525543545232727\n",
            "train loss:0.0011753604723165987\n",
            "train loss:0.00013157595818533884\n",
            "train loss:7.50435831440173e-05\n",
            "train loss:0.0004090827723952\n",
            "train loss:0.004684083222804812\n",
            "train loss:0.001291380838202412\n",
            "train loss:0.0006314519268790509\n",
            "train loss:0.0008673131704301228\n",
            "train loss:0.00037844203342764747\n",
            "train loss:0.011441319795821556\n",
            "train loss:0.0015161043987434145\n",
            "train loss:0.00015533744903671273\n",
            "train loss:0.0005439078132132167\n",
            "train loss:0.0013997810788112502\n",
            "train loss:0.002132087573928557\n",
            "train loss:0.0010697517724188753\n",
            "train loss:0.00018402191182098225\n",
            "train loss:0.000575872373228037\n",
            "train loss:0.0001813090857425427\n",
            "train loss:0.0007316679185254277\n",
            "train loss:0.0011542721741043633\n",
            "train loss:0.0004118479756659919\n",
            "train loss:0.0008803488370269937\n",
            "train loss:0.00024507351207807537\n",
            "train loss:0.0009033749611615173\n",
            "train loss:0.0002381883832731333\n",
            "train loss:0.007278434348991994\n",
            "train loss:0.000576712002103002\n",
            "train loss:0.00019981293753030998\n",
            "train loss:0.0016993825743400696\n",
            "train loss:0.0001688181666659881\n",
            "train loss:0.0009884760486129158\n",
            "train loss:0.0005221978918833841\n",
            "train loss:0.0003373400066560759\n",
            "train loss:0.0003524130772192283\n",
            "train loss:0.0010903852216860531\n",
            "train loss:0.00020052981876553008\n",
            "train loss:0.00010832153739632072\n",
            "train loss:1.7497048696602295e-05\n",
            "train loss:0.0006558690354618044\n",
            "train loss:0.0032731092943052574\n",
            "train loss:0.0006673159935150748\n",
            "train loss:0.0007502547284709784\n",
            "train loss:0.0009234812511847945\n",
            "train loss:0.0006728581751624846\n",
            "train loss:4.6014522561159556e-05\n",
            "train loss:0.0013080475370849873\n",
            "train loss:0.0023438173417689775\n",
            "train loss:5.939348530960264e-05\n",
            "train loss:0.0008805611904808716\n",
            "train loss:0.0006282985746467999\n",
            "train loss:0.0017084972458939562\n",
            "train loss:0.00024759932602574295\n",
            "train loss:0.002335286947761422\n",
            "train loss:0.000713105339201875\n",
            "train loss:0.00044550868341310687\n",
            "train loss:0.001501857244915917\n",
            "train loss:0.003880328438611993\n",
            "train loss:0.003380772300342212\n",
            "train loss:0.0034800938486383016\n",
            "train loss:0.0002718387516201598\n",
            "train loss:0.0003745356132723941\n",
            "train loss:0.0015716915504105022\n",
            "train loss:0.0008285484255035649\n",
            "train loss:4.2883614570554484e-05\n",
            "train loss:9.943684215974503e-05\n",
            "train loss:0.00019912945887878597\n",
            "train loss:0.00030091916256081623\n",
            "train loss:0.003455583845963978\n",
            "train loss:0.00147524612852213\n",
            "train loss:0.0001491100839732532\n",
            "train loss:0.00010745920434652137\n",
            "train loss:0.0010629331767578799\n",
            "train loss:0.0026511655539877815\n",
            "train loss:0.004942593118358039\n",
            "train loss:0.001988749522683927\n",
            "train loss:0.00011420153852247734\n",
            "train loss:0.00018612569733393974\n",
            "train loss:0.00018159736534156145\n",
            "train loss:0.0005181614905984009\n",
            "train loss:0.0007565856171098811\n",
            "train loss:0.0012679298232162806\n",
            "train loss:0.0010452693200463583\n",
            "train loss:0.000520505809338523\n",
            "train loss:0.0005003148286657029\n",
            "train loss:0.0005718952171104662\n",
            "train loss:0.0004185716065306648\n",
            "train loss:0.0025710189006276085\n",
            "train loss:0.001629050083768414\n",
            "train loss:0.004527753009288479\n",
            "train loss:0.0008821600619079577\n",
            "train loss:0.00031226241402903024\n",
            "train loss:0.002296559746339807\n",
            "train loss:0.00012385976582554334\n",
            "train loss:0.0003373268535043904\n",
            "train loss:0.00047350348671296974\n",
            "train loss:0.001198890215082254\n",
            "train loss:0.00037419103994195064\n",
            "train loss:0.00021921230364098634\n",
            "train loss:8.87704157594091e-05\n",
            "train loss:0.001416303905983319\n",
            "train loss:0.0012245353263309812\n",
            "train loss:0.00042381690046866\n",
            "train loss:0.0009556415867847905\n",
            "train loss:0.00019232227794852756\n",
            "train loss:0.0002807483579673251\n",
            "train loss:0.00019469167633529217\n",
            "train loss:0.0009045592581879395\n",
            "train loss:0.0007731704095854695\n",
            "train loss:2.6187119134179174e-05\n",
            "train loss:0.0005448135173748439\n",
            "train loss:0.0005144675282606956\n",
            "train loss:0.0007222138954414842\n",
            "train loss:0.0021058713375541973\n",
            "train loss:0.0006450213343249207\n",
            "train loss:0.0006003903332464485\n",
            "train loss:0.00013239707106148566\n",
            "train loss:0.0005454149987286123\n",
            "train loss:0.00016267574075619633\n",
            "train loss:0.0015155243335661095\n",
            "train loss:0.0008575477421272898\n",
            "train loss:0.0008801398564655408\n",
            "train loss:0.002997161279893711\n",
            "train loss:0.00052354031228662\n",
            "train loss:8.994569189561126e-05\n",
            "train loss:0.00030944858649288\n",
            "train loss:8.174691636853319e-05\n",
            "train loss:0.0006698468795118846\n",
            "train loss:0.0001678410212390366\n",
            "train loss:0.004947843910137826\n",
            "train loss:4.2785779778190435e-05\n",
            "train loss:0.00022548504152314152\n",
            "train loss:0.00034196400604832714\n",
            "train loss:0.0003823070606540331\n",
            "train loss:0.004001493061165736\n",
            "train loss:0.001454824073516883\n",
            "train loss:0.0028608622765814388\n",
            "train loss:0.0010575531298416634\n",
            "train loss:0.0016786448332661932\n",
            "train loss:0.00016266959601858927\n",
            "train loss:0.0028554316531384603\n",
            "train loss:0.0030192766184498764\n",
            "train loss:0.0011774132406753375\n",
            "train loss:0.00040850127951479567\n",
            "train loss:0.0008020215684070992\n",
            "train loss:2.4927181936885073e-05\n",
            "train loss:0.00018293631503213503\n",
            "train loss:3.295352162591586e-05\n",
            "train loss:0.0001551958322610693\n",
            "train loss:0.0008339105979494111\n",
            "train loss:0.0007185619411713761\n",
            "train loss:7.03162939366084e-05\n",
            "train loss:0.000505504488140367\n",
            "train loss:0.00022304613685437993\n",
            "train loss:0.0007102985001950564\n",
            "train loss:0.0002359687608108618\n",
            "train loss:6.478878628767397e-05\n",
            "train loss:0.0009350447282706154\n",
            "train loss:0.00022150765934125438\n",
            "train loss:0.0013038575165694349\n",
            "train loss:0.002188328584032824\n",
            "train loss:0.0005101248894009081\n",
            "train loss:0.0008350001161132542\n",
            "train loss:0.001134678938566323\n",
            "train loss:0.00032804405264418317\n",
            "train loss:0.00016585142810825976\n",
            "train loss:0.008295824202275258\n",
            "train loss:0.00039861634031645505\n",
            "train loss:0.000843921148897948\n",
            "train loss:0.0011764636254551072\n",
            "train loss:2.5636580531520547e-05\n",
            "train loss:0.0010210237126835318\n",
            "train loss:0.00024832395420918796\n",
            "train loss:0.0010494992138111484\n",
            "train loss:0.002003662184764956\n",
            "train loss:0.00020023892324197535\n",
            "train loss:0.001963280172745956\n",
            "train loss:0.0023894740709387643\n",
            "train loss:4.1725718230903555e-05\n",
            "train loss:5.0974212926567664e-05\n",
            "train loss:0.0009672716789367114\n",
            "train loss:0.00010026507994139315\n",
            "train loss:0.0014749627265567514\n",
            "train loss:2.9859493218849673e-05\n",
            "train loss:5.631105258882037e-05\n",
            "train loss:0.0006721043570795427\n",
            "train loss:0.0029528742412147247\n",
            "train loss:0.0006523340340334227\n",
            "train loss:0.00028391909511383845\n",
            "train loss:7.503877123270054e-05\n",
            "train loss:0.00037182012882151436\n",
            "train loss:0.0015346459025638607\n",
            "train loss:0.0067902136835455654\n",
            "train loss:0.00015124494318780434\n",
            "train loss:1.6852006148858743e-05\n",
            "=== epoch:20, train acc:0.998, test acc:0.987 ===\n",
            "train loss:0.00033453899084071254\n",
            "train loss:0.0008541919990824899\n",
            "train loss:0.00271527729666544\n",
            "train loss:0.00020328191918913128\n",
            "train loss:0.00044670465104728517\n",
            "train loss:0.0027794583826830717\n",
            "train loss:0.003463453157326214\n",
            "train loss:0.00040223284264487616\n",
            "train loss:0.0001986310456919194\n",
            "train loss:0.0025173593263272155\n",
            "train loss:0.001607299893855097\n",
            "train loss:0.00022867468544083603\n",
            "train loss:0.0013789654269408614\n",
            "train loss:0.0013729984454342195\n",
            "train loss:0.0006117268196157246\n",
            "train loss:8.747185125941135e-05\n",
            "train loss:0.0010261367202922746\n",
            "train loss:3.16246125542445e-05\n",
            "train loss:0.004275519549200408\n",
            "train loss:0.0023930299606236\n",
            "train loss:0.0014160974648577667\n",
            "train loss:0.011699039327668354\n",
            "train loss:0.0012344449596059287\n",
            "train loss:0.008486574504517187\n",
            "train loss:0.00048751495267509553\n",
            "train loss:0.00038857848921435896\n",
            "train loss:0.00011651260784511761\n",
            "train loss:0.0015074442818494174\n",
            "train loss:0.00395127359082872\n",
            "train loss:6.012616939270968e-05\n",
            "train loss:0.00032597696800895886\n",
            "train loss:0.004265059344623703\n",
            "train loss:0.0004552423408759825\n",
            "train loss:0.002295047255412645\n",
            "train loss:0.002557945763674362\n",
            "train loss:0.0026044366822039183\n",
            "train loss:0.00041792571574395876\n",
            "train loss:0.0001626109588452147\n",
            "train loss:0.0001416346460952239\n",
            "train loss:7.054037020684063e-05\n",
            "train loss:0.0008366648654840946\n",
            "train loss:0.004004241528233038\n",
            "train loss:0.014404038596668025\n",
            "train loss:0.000702731988787867\n",
            "train loss:0.00030760393355924905\n",
            "train loss:0.0003230975889661591\n",
            "train loss:5.1652368259572894e-05\n",
            "train loss:0.0005291567830743203\n",
            "train loss:0.002593591436397858\n",
            "train loss:0.0004124002006189141\n",
            "train loss:0.00020424504645686085\n",
            "train loss:0.0012733681645930868\n",
            "train loss:0.0028635195464984486\n",
            "train loss:0.0005209966270740499\n",
            "train loss:0.00024325168107694754\n",
            "train loss:0.001603028765026686\n",
            "train loss:0.04509656237209994\n",
            "train loss:0.003737982649182266\n",
            "train loss:0.00026647433822047566\n",
            "train loss:0.0003579296290738871\n",
            "train loss:0.0014230587952677265\n",
            "train loss:0.0013559308303334255\n",
            "train loss:9.605829821843304e-06\n",
            "train loss:0.0002661230610247125\n",
            "train loss:0.0005779880897193374\n",
            "train loss:0.000309837889919839\n",
            "train loss:0.004638303264479723\n",
            "train loss:0.0003620104650647306\n",
            "train loss:0.0002159292004275153\n",
            "train loss:0.001220050752711639\n",
            "train loss:5.94555782487013e-05\n",
            "train loss:0.0007409852312776394\n",
            "train loss:0.00023052829504920992\n",
            "train loss:0.0037729381808116248\n",
            "train loss:0.0004612979338197942\n",
            "train loss:0.0004300349922375639\n",
            "train loss:0.010084186310727742\n",
            "train loss:0.0003553704305077402\n",
            "train loss:0.0017523962319910163\n",
            "train loss:0.0013667562577468933\n",
            "train loss:0.00012127339026884377\n",
            "train loss:6.286700402189257e-05\n",
            "train loss:0.00027365839179765067\n",
            "train loss:0.000583358770017568\n",
            "train loss:0.00013214411484524134\n",
            "train loss:0.0009387768860967161\n",
            "train loss:0.003232918841036863\n",
            "train loss:0.00042779651183052684\n",
            "train loss:0.00022669447607276153\n",
            "train loss:0.0008925433075011\n",
            "train loss:0.0008050538292929642\n",
            "train loss:0.0007724034305978344\n",
            "train loss:0.00043774872820697883\n",
            "train loss:0.0011260635493300886\n",
            "train loss:0.0010846596726913273\n",
            "train loss:0.004087885906456529\n",
            "train loss:0.005777044239451336\n",
            "train loss:0.005227000607605248\n",
            "train loss:0.004669397073228934\n",
            "train loss:0.0005984702318934983\n",
            "train loss:0.0031506617461436004\n",
            "train loss:0.0003897179629322023\n",
            "train loss:0.0007974024958565374\n",
            "train loss:0.000566331100179252\n",
            "train loss:0.00030454629986853977\n",
            "train loss:0.003400832326595078\n",
            "train loss:0.002092792510831977\n",
            "train loss:0.00010464274254477502\n",
            "train loss:6.381591833858757e-05\n",
            "train loss:0.0004419090837849042\n",
            "train loss:6.678797348776027e-05\n",
            "train loss:0.0013750917921689946\n",
            "train loss:0.004525266895706332\n",
            "train loss:0.0014450963248536911\n",
            "train loss:0.0037884903045735372\n",
            "train loss:0.0026523016741406\n",
            "train loss:0.006487969107315428\n",
            "train loss:0.0005689147400724451\n",
            "train loss:0.00022818954714183494\n",
            "train loss:0.010804207382101114\n",
            "train loss:0.0010113756348741167\n",
            "train loss:0.00040819958363763034\n",
            "train loss:0.0004322531602065252\n",
            "train loss:0.0016645587571680444\n",
            "train loss:0.0006886180157353729\n",
            "train loss:0.0018240220580645\n",
            "train loss:0.00022611052645464004\n",
            "train loss:0.0015702489778301636\n",
            "train loss:0.0007887503758239106\n",
            "train loss:0.00433744133087189\n",
            "train loss:0.00022100221453943684\n",
            "train loss:0.0030397127688156144\n",
            "train loss:0.0007270893186474532\n",
            "train loss:0.01455189191366399\n",
            "train loss:0.000749375542190766\n",
            "train loss:0.00011155446459899381\n",
            "train loss:0.0008276218027963859\n",
            "train loss:0.0005914449888314539\n",
            "train loss:0.00299701968675058\n",
            "train loss:0.0005947457302232479\n",
            "train loss:0.001591309908071037\n",
            "train loss:0.0002736304245472564\n",
            "train loss:0.002301732487524986\n",
            "train loss:0.004130693654141367\n",
            "train loss:0.000185266652639474\n",
            "train loss:0.00038257608180298797\n",
            "train loss:0.004223322381626248\n",
            "train loss:0.007959611455578592\n",
            "train loss:0.0019560266417645127\n",
            "train loss:0.005390397362346818\n",
            "train loss:0.001395830194580055\n",
            "train loss:0.002137745139412224\n",
            "train loss:0.0005578910345607961\n",
            "train loss:0.0031094979355877263\n",
            "train loss:0.0002816795999720205\n",
            "train loss:0.0019346194041025965\n",
            "train loss:0.02016867615666431\n",
            "train loss:0.0005104040500341678\n",
            "train loss:0.001474201954578988\n",
            "train loss:0.002269484494967453\n",
            "train loss:0.0026295617193507915\n",
            "train loss:0.00039355231933991684\n",
            "train loss:0.0007257301337393142\n",
            "train loss:0.0004281290608277717\n",
            "train loss:0.0004442332787154337\n",
            "train loss:0.0005105173652387924\n",
            "train loss:0.0031200469342998927\n",
            "train loss:0.03474060137293586\n",
            "train loss:0.00012929267873698297\n",
            "train loss:0.0007894779596697755\n",
            "train loss:0.0019880316787153447\n",
            "train loss:0.0009511568563536411\n",
            "train loss:0.0009721817044567945\n",
            "train loss:0.0010861962446838131\n",
            "train loss:0.0003945983260309513\n",
            "train loss:0.0016575387243212899\n",
            "train loss:0.0007501833614601902\n",
            "train loss:0.001974414099942812\n",
            "train loss:0.002258589473206309\n",
            "train loss:0.0021241555592148094\n",
            "train loss:0.011544271817524224\n",
            "train loss:0.0015427681033319636\n",
            "train loss:0.0008245839875932218\n",
            "train loss:0.00022776963979631085\n",
            "train loss:0.0002108688525255147\n",
            "train loss:0.0002839293267876935\n",
            "train loss:0.009417237387853746\n",
            "train loss:0.000950652738179935\n",
            "train loss:0.00023846195574451067\n",
            "train loss:0.0008495206509807549\n",
            "train loss:0.0002461635536992777\n",
            "train loss:5.5297297289090897e-05\n",
            "train loss:0.00017109058136671583\n",
            "train loss:0.001238582843158514\n",
            "train loss:0.0014117889415577274\n",
            "train loss:0.001379345557938361\n",
            "train loss:0.0015057659269775798\n",
            "train loss:0.005292705923768332\n",
            "train loss:9.262381121973445e-05\n",
            "train loss:0.000643649160812387\n",
            "train loss:0.0013786434262193213\n",
            "train loss:0.0017035648637750422\n",
            "train loss:9.150505429050579e-06\n",
            "train loss:0.0012418842325666118\n",
            "train loss:0.0003420893394291249\n",
            "train loss:0.003043702794185893\n",
            "train loss:5.548143489411981e-05\n",
            "train loss:0.00014922031430494845\n",
            "train loss:0.0009222378547164413\n",
            "train loss:0.000869672619012817\n",
            "train loss:0.0005389161450568071\n",
            "train loss:0.0001157375784901168\n",
            "train loss:0.0019914719771330206\n",
            "train loss:0.0008293670397252566\n",
            "train loss:0.00021250887050139293\n",
            "train loss:0.001545460960670808\n",
            "train loss:0.0004620875053994864\n",
            "train loss:0.001912892655770481\n",
            "train loss:0.015516837830133794\n",
            "train loss:0.0012357195083073098\n",
            "train loss:0.001320390589616265\n",
            "train loss:0.003202358891589555\n",
            "train loss:0.004755709869610453\n",
            "train loss:0.02976412428206938\n",
            "train loss:0.0009930688541553287\n",
            "train loss:0.0006066407496511414\n",
            "train loss:0.0019107437917912215\n",
            "train loss:0.00015325189308991767\n",
            "train loss:0.003798848123855853\n",
            "train loss:0.0002323491915902751\n",
            "train loss:0.001419745675100328\n",
            "train loss:0.0009693276510416522\n",
            "train loss:0.0018414380132831156\n",
            "train loss:0.004824443493360897\n",
            "train loss:0.0024715003692562447\n",
            "train loss:4.241340954894328e-05\n",
            "train loss:0.002308681480258523\n",
            "train loss:0.00014191701283508144\n",
            "train loss:0.0020341619873134147\n",
            "train loss:0.0007678225144388072\n",
            "train loss:0.00035585538334201484\n",
            "train loss:0.0010400508096689973\n",
            "train loss:0.0016700000022020476\n",
            "train loss:0.00525369150069486\n",
            "train loss:5.523999375699494e-05\n",
            "train loss:0.0008415583000368677\n",
            "train loss:0.0002121275897563717\n",
            "train loss:0.005137530919174477\n",
            "train loss:0.0017793253022194975\n",
            "train loss:0.0003299675288873778\n",
            "train loss:0.0009256679425122638\n",
            "train loss:0.0025037983964463057\n",
            "train loss:0.003259910029690893\n",
            "train loss:0.000611830356950736\n",
            "train loss:0.0009486074975729017\n",
            "train loss:0.00010417927618976955\n",
            "train loss:0.0003163098187196261\n",
            "train loss:0.000684952821787929\n",
            "train loss:0.0007821364136796696\n",
            "train loss:0.0016586771913474554\n",
            "train loss:0.0015960013544280142\n",
            "train loss:0.0018640175985952952\n",
            "train loss:0.0008841994081569469\n",
            "train loss:0.000384494028031569\n",
            "train loss:0.0016747816848830589\n",
            "train loss:0.0010794308274946706\n",
            "train loss:0.00246499781838972\n",
            "train loss:0.06276260755132097\n",
            "train loss:0.0002222377124368186\n",
            "train loss:0.0021105984648930372\n",
            "train loss:0.0005583070470639706\n",
            "train loss:0.0003843444299737126\n",
            "train loss:0.0008901279909143109\n",
            "train loss:0.0004892182160896476\n",
            "train loss:0.0007140471881031362\n",
            "train loss:0.0015133190849402187\n",
            "train loss:0.0006234614841930545\n",
            "train loss:0.002759752153608775\n",
            "train loss:0.0004294002577837233\n",
            "train loss:0.0018999031666415316\n",
            "train loss:0.008102467742420152\n",
            "train loss:0.00014264557816793714\n",
            "train loss:0.0005890279870448558\n",
            "train loss:0.0005889915523825003\n",
            "train loss:0.002045832336761154\n",
            "train loss:0.00027027574281730506\n",
            "train loss:0.0010520156594930104\n",
            "train loss:0.0007559762229791654\n",
            "train loss:0.0011698052914942898\n",
            "train loss:0.00026842270711201004\n",
            "train loss:0.0025752865288244547\n",
            "train loss:0.0009509613073136693\n",
            "train loss:0.0007293767880943504\n",
            "train loss:0.007668377432210259\n",
            "train loss:0.0026164008592264604\n",
            "train loss:0.0016017904777262928\n",
            "train loss:0.00044848497499317964\n",
            "train loss:0.006045745841276485\n",
            "train loss:0.0051003332245959125\n",
            "train loss:0.00012746441108851998\n",
            "train loss:0.001100883769098378\n",
            "train loss:0.00024420840537215066\n",
            "train loss:0.001468298528388424\n",
            "train loss:6.240561166311359e-05\n",
            "train loss:0.0012730653070694136\n",
            "train loss:0.0016175918946519593\n",
            "train loss:0.001099493533337887\n",
            "train loss:0.0013533314143754896\n",
            "train loss:0.004514087569313522\n",
            "train loss:0.006467710676894374\n",
            "train loss:0.0013723782556080918\n",
            "train loss:0.00020925073033333956\n",
            "train loss:0.0016006393309743504\n",
            "train loss:0.00013362691570212423\n",
            "train loss:0.00014906222139451128\n",
            "train loss:0.0001219574260457933\n",
            "train loss:0.0007272534081112171\n",
            "train loss:0.001504187168653498\n",
            "train loss:0.0003295280086931633\n",
            "train loss:0.003888820958404154\n",
            "train loss:0.018307329491007773\n",
            "train loss:0.0012243307935108161\n",
            "train loss:0.00039134890963727666\n",
            "train loss:0.0005427848703504075\n",
            "train loss:8.552298791581212e-05\n",
            "train loss:0.0007362198863372608\n",
            "train loss:0.0005714503716517905\n",
            "train loss:0.0027151575478738953\n",
            "train loss:0.00021187587905500266\n",
            "train loss:0.001104480950976988\n",
            "train loss:0.0015937334552701033\n",
            "train loss:0.0004126350872236757\n",
            "train loss:0.0018525278621290382\n",
            "train loss:0.0011837326789736354\n",
            "train loss:0.0009702189099379119\n",
            "train loss:0.005043422593933723\n",
            "train loss:0.0037030925461220333\n",
            "train loss:0.0005338044108084482\n",
            "train loss:0.0008574151711637059\n",
            "train loss:0.0003317563538574131\n",
            "train loss:0.00230474812042013\n",
            "train loss:0.0020948229692640493\n",
            "train loss:0.0009438252215243075\n",
            "train loss:0.0009327675442853005\n",
            "train loss:0.0002666216532184127\n",
            "train loss:0.0018449519219906343\n",
            "train loss:0.00017462772604413883\n",
            "train loss:0.00038900924323785645\n",
            "train loss:0.0009294014093467964\n",
            "train loss:0.0006709858287660363\n",
            "train loss:0.003464721556123299\n",
            "train loss:0.029786482802248004\n",
            "train loss:0.0029021644910430574\n",
            "train loss:0.0001775035350077631\n",
            "train loss:0.000943197513280129\n",
            "train loss:0.0017301578784807905\n",
            "train loss:8.13850811715752e-05\n",
            "train loss:0.0010873921922510013\n",
            "train loss:0.0028017263134739014\n",
            "train loss:0.0019064327464784264\n",
            "train loss:0.0017808699922061875\n",
            "train loss:0.0005615320890433839\n",
            "train loss:0.000124239272043444\n",
            "train loss:0.000129007285679616\n",
            "train loss:1.6717741770840812e-05\n",
            "train loss:7.439935184351204e-05\n",
            "train loss:0.007292435215023656\n",
            "train loss:0.0012028055539247968\n",
            "train loss:0.00041487135907353185\n",
            "train loss:0.0013729345035169052\n",
            "train loss:0.0004147601389116899\n",
            "train loss:0.005430135100119741\n",
            "train loss:4.8630495060073976e-05\n",
            "train loss:0.013320762846860052\n",
            "train loss:0.0005559086965267104\n",
            "train loss:0.02177806966895901\n",
            "train loss:0.004046336946081375\n",
            "train loss:0.0006666255229770769\n",
            "train loss:0.0033768115632494974\n",
            "train loss:0.00012023834538744417\n",
            "train loss:0.0012589229003926756\n",
            "train loss:0.0005599922172788054\n",
            "train loss:0.0002879667529318704\n",
            "train loss:0.0003423085884915259\n",
            "train loss:0.0012443784617787645\n",
            "train loss:0.007141198373921635\n",
            "train loss:0.08242454976454662\n",
            "train loss:0.0002372398021138361\n",
            "train loss:0.00034666821137731627\n",
            "train loss:0.0013606907452502642\n",
            "train loss:6.5409908235067e-05\n",
            "train loss:0.0007484137849333786\n",
            "train loss:0.0006616910824656279\n",
            "train loss:6.056275378131858e-05\n",
            "train loss:0.001387718971694685\n",
            "train loss:0.0007952919997954226\n",
            "train loss:0.00066345325666846\n",
            "train loss:0.0006860447288957178\n",
            "train loss:0.016770496447168038\n",
            "train loss:0.0006882886960404293\n",
            "train loss:0.0006152336408195257\n",
            "train loss:0.0007098345039777019\n",
            "train loss:0.0029399036114190545\n",
            "train loss:0.0004060614374750339\n",
            "train loss:0.00021636649230421802\n",
            "train loss:0.0033623890724380724\n",
            "train loss:0.0004563515382307768\n",
            "train loss:0.0004355228857524718\n",
            "train loss:0.00258409218529614\n",
            "train loss:0.0006761505514992429\n",
            "train loss:0.00010906405283004031\n",
            "train loss:0.0011569589979930804\n",
            "train loss:0.001193183501602778\n",
            "train loss:0.0033553563264731305\n",
            "train loss:0.0004662961185234386\n",
            "train loss:0.0373879211629326\n",
            "train loss:0.0035627162896120956\n",
            "train loss:9.5677554473312e-05\n",
            "train loss:0.0019085061934189082\n",
            "train loss:0.0004107842467723282\n",
            "train loss:0.00026548351127134926\n",
            "train loss:0.0029642840512731916\n",
            "train loss:0.0009730640562754418\n",
            "train loss:0.00025315493865153715\n",
            "train loss:0.00025361989564631234\n",
            "train loss:0.0004859977594010736\n",
            "train loss:0.001725096034640411\n",
            "train loss:0.0002060919407749314\n",
            "train loss:0.0007037775196545326\n",
            "train loss:0.0001718224713690336\n",
            "train loss:0.005507349551247844\n",
            "train loss:0.001697856544041804\n",
            "train loss:0.0039061240336828815\n",
            "train loss:0.0024755701923642633\n",
            "train loss:0.01602143619127017\n",
            "train loss:0.00047882190106996234\n",
            "train loss:0.0017876844170409203\n",
            "train loss:0.001676838830219532\n",
            "train loss:0.0004854547083383805\n",
            "train loss:0.000480002416648712\n",
            "train loss:0.0014241492573831718\n",
            "train loss:0.002244907096736489\n",
            "train loss:0.0005885060284703384\n",
            "train loss:0.0007188137673121857\n",
            "train loss:0.0014599838669512144\n",
            "train loss:0.00020972932346953861\n",
            "train loss:0.005842208566153438\n",
            "train loss:0.0011231651867305327\n",
            "train loss:0.009971447548389533\n",
            "train loss:0.0011842188873249857\n",
            "train loss:0.00648306202392192\n",
            "train loss:0.00034182541017207147\n",
            "train loss:0.0008923471306731324\n",
            "train loss:0.0011860316545480635\n",
            "train loss:0.0037516245588650327\n",
            "train loss:0.00035782199027426735\n",
            "train loss:0.0007151656888802036\n",
            "train loss:0.0011074983066916035\n",
            "train loss:0.0010562106858779518\n",
            "train loss:0.002525699802923425\n",
            "train loss:0.0002703922216244282\n",
            "train loss:0.0007812008405127398\n",
            "train loss:0.0003027335596600762\n",
            "train loss:0.0037700060549835253\n",
            "train loss:0.0037730786178830035\n",
            "train loss:0.0007270108023386332\n",
            "train loss:0.00041887714644504165\n",
            "train loss:0.000492113070744451\n",
            "train loss:0.00015662496945521124\n",
            "train loss:0.00011921842527592992\n",
            "train loss:0.0034472106106845056\n",
            "train loss:0.0054920735750474785\n",
            "train loss:0.0005283944384847786\n",
            "train loss:0.0003900527482959449\n",
            "train loss:0.0008970181882862941\n",
            "train loss:0.00085904307308111\n",
            "train loss:0.0004995801050934486\n",
            "train loss:0.0002325335164494133\n",
            "train loss:0.0011529399186752352\n",
            "train loss:0.0004259366194598302\n",
            "train loss:0.0018691454339187883\n",
            "train loss:0.0004983330627422171\n",
            "train loss:4.097787940495556e-05\n",
            "train loss:3.146486550857969e-05\n",
            "train loss:0.0003634100047404269\n",
            "train loss:0.010082123324043357\n",
            "train loss:0.00046609645918880915\n",
            "train loss:0.002192221363726035\n",
            "train loss:0.0009903325947108494\n",
            "train loss:0.004401712421458905\n",
            "train loss:0.0027516446424770647\n",
            "train loss:0.0038649407982733665\n",
            "train loss:0.010629966556042066\n",
            "train loss:0.0007345860103407658\n",
            "train loss:0.002863334163682617\n",
            "train loss:0.0016294872338689357\n",
            "train loss:2.2069763280132453e-05\n",
            "train loss:0.0007931290566044048\n",
            "train loss:0.0038585097357286885\n",
            "train loss:0.0016500401638849377\n",
            "train loss:0.0006842021229648505\n",
            "train loss:0.004780832616003702\n",
            "train loss:8.909465342500436e-05\n",
            "train loss:0.00013782477339521242\n",
            "train loss:0.00019972837695443344\n",
            "train loss:0.00016801540893475253\n",
            "train loss:0.0005911256356543464\n",
            "train loss:0.0022775405256601254\n",
            "train loss:0.002379167515984751\n",
            "train loss:0.0012933058288045291\n",
            "train loss:0.0003027000078816353\n",
            "train loss:7.500951412387344e-05\n",
            "train loss:0.003334772225481883\n",
            "train loss:0.000525839052806531\n",
            "train loss:0.00020099389485225025\n",
            "train loss:0.002688140110267225\n",
            "train loss:0.001369803305942633\n",
            "train loss:0.0013580687280461976\n",
            "train loss:0.0011044186734673871\n",
            "train loss:0.0015945848388497666\n",
            "train loss:0.0003017402042932106\n",
            "train loss:0.0008192129146224114\n",
            "train loss:0.0003948602510002619\n",
            "train loss:0.002552801597399437\n",
            "train loss:0.005291947499842132\n",
            "train loss:0.003079577772636183\n",
            "train loss:0.0003791522966352424\n",
            "train loss:6.188866775863556e-05\n",
            "train loss:0.0002308708276936735\n",
            "train loss:0.00475261926404763\n",
            "train loss:0.0014353795204044575\n",
            "train loss:0.0009585309568738446\n",
            "train loss:0.0015419151697956682\n",
            "train loss:9.236890838297062e-05\n",
            "train loss:0.0023164097914615224\n",
            "train loss:0.0018100052356073706\n",
            "train loss:0.0005235208533606876\n",
            "train loss:0.0005468230243158771\n",
            "train loss:0.03132611543807236\n",
            "train loss:0.0004444161390339195\n",
            "train loss:8.673745332618234e-05\n",
            "train loss:0.0014903579733238656\n",
            "train loss:0.001740447169464847\n",
            "train loss:0.00032780908463320873\n",
            "train loss:0.0013883742856499585\n",
            "train loss:0.0001431646773792466\n",
            "train loss:0.000875616009920515\n",
            "train loss:0.0027868922274534623\n",
            "train loss:0.00023783046040718277\n",
            "train loss:0.0009967732705127774\n",
            "train loss:8.222607288435312e-05\n",
            "train loss:0.000578427418906592\n",
            "train loss:0.0007356750055786349\n",
            "train loss:0.002365859004733871\n",
            "train loss:4.2273795350350176e-05\n",
            "train loss:0.003893795654279709\n",
            "train loss:0.0012070059216400747\n",
            "train loss:0.00043022434614158403\n",
            "train loss:0.0007392039293208818\n",
            "train loss:0.00013689506558937993\n",
            "train loss:0.00106933357237876\n",
            "train loss:0.0002952645230537745\n",
            "train loss:0.0002842470832372114\n",
            "train loss:0.0026611304291362606\n",
            "train loss:0.0001342301077324146\n",
            "train loss:0.010657509361169984\n",
            "train loss:0.00033078922026120186\n",
            "train loss:0.002986536674780242\n",
            "train loss:0.00191491171358023\n",
            "train loss:0.0076606103608905475\n",
            "train loss:0.0010562633390196205\n",
            "train loss:0.0030328552584621142\n",
            "train loss:0.0030115223978459775\n",
            "train loss:0.00012484749527723104\n",
            "train loss:0.002257103039258854\n",
            "train loss:0.0011176965582415702\n",
            "train loss:0.0018706722258643497\n",
            "train loss:0.01564351821866293\n",
            "train loss:0.0007264073687407093\n",
            "train loss:0.013945551457806566\n",
            "train loss:0.002847384464156021\n",
            "train loss:0.0016196195909285716\n",
            "train loss:0.0007286522331861119\n",
            "train loss:0.0015192770077562686\n",
            "train loss:0.0017517744655209846\n",
            "train loss:0.0012014170225288295\n",
            "train loss:6.205319930277194e-05\n",
            "train loss:0.00087863073892489\n",
            "train loss:0.00021755165086988733\n",
            "train loss:0.00012349833561746672\n",
            "train loss:0.013015957637530974\n",
            "train loss:0.0005518812730539222\n",
            "train loss:0.00021555401290155118\n",
            "train loss:0.0008597009876702366\n",
            "train loss:0.012082164677037837\n",
            "train loss:0.015995712649121686\n",
            "train loss:0.0024030402612885515\n",
            "train loss:0.00034721419208464386\n",
            "train loss:0.0031086288582607346\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2df31d5bdb7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# 매개변수 보존\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"params.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved Network Parameters!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-332214735778>\u001b[0m in \u001b[0;36msave_params\u001b[0;34m(self, file_name)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"params.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXCDz0olu91X",
        "outputId": "0e32eed3-fcd2-4cef-8fd9-160f977bbd0f"
      },
      "source": [
        "cd ch07"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch/ch07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpBrVRdjvCsB",
        "outputId": "aa2694de-07d3-4128-94ea-4dc276569fae"
      },
      "source": [
        "ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "apply_filter.py    params.pkl  simple_convnet.py  visualize_filter.py\n",
            "gradient_check.py  README.md   train_convnet.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "mdIo77-Ku64N",
        "outputId": "345f1f8a-2c4d-42b4-d3aa-10ead57ad1eb"
      },
      "source": [
        "# 매개변수 보존\n",
        "import pickle\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved Network Parameters!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cWT2mBWgnar",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "d593c04d-0dbc-4f77-f848-5ae61fb27b25"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    \"\"\"\n",
        "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
        "    \"\"\"\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "network = SimpleConvNet()\n",
        "# 무작위(랜덤) 초기화 후의 가중치\n",
        "filter_show(network.params['W1'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 30 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2vN1fS0v0al",
        "outputId": "4d998f13-c541-42e8-af15-43e2ee04f1b8"
      },
      "source": [
        "cd ch07"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch/ch07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmz-K9YFhREO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "1b1a8088-4ee7-46c2-bd92-ef2b6b19c067"
      },
      "source": [
        "import pickle\n",
        "# 학습된 가중치\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 30 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQbJhld1hUzr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}